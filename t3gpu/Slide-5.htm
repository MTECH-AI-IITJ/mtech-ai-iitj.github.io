<html>

<head>
<meta http-equiv=Content-Type content="text/html; charset=windows-1252">
<meta name=Generator content="Microsoft Word 15 (filtered)">
<style>
<!--
 /* Font Definitions */
 @font-face
	{font-family:Helvetica;
	panose-1:2 11 6 4 2 2 2 2 2 4;}
@font-face
	{font-family:Courier;
	panose-1:2 7 4 9 2 2 5 2 4 4;}
@font-face
	{font-family:"Tms Rmn";
	panose-1:2 2 6 3 4 5 5 2 3 4;}
@font-face
	{font-family:Helv;
	panose-1:2 11 6 4 2 2 2 3 2 4;}
@font-face
	{font-family:"New York";
	panose-1:2 4 5 3 6 5 6 2 3 4;}
@font-face
	{font-family:System;
	panose-1:0 0 0 0 0 0 0 0 0 0;}
@font-face
	{font-family:Wingdings;
	panose-1:5 0 0 0 0 0 0 0 0 0;}
@font-face
	{font-family:"MS Mincho";
	panose-1:2 2 6 9 4 2 5 8 3 4;}
@font-face
	{font-family:Batang;
	panose-1:2 3 6 0 0 1 1 1 1 1;}
@font-face
	{font-family:SimSun;
	panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
	{font-family:PMingLiU;
	panose-1:2 1 6 1 0 1 1 1 1 1;}
@font-face
	{font-family:"MS Gothic";
	panose-1:2 11 6 9 7 2 5 8 2 4;}
@font-face
	{font-family:Dotum;
	panose-1:2 11 6 0 0 1 1 1 1 1;}
@font-face
	{font-family:SimHei;
	panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
	{font-family:MingLiU;
	panose-1:2 1 6 9 0 1 1 1 1 1;}
@font-face
	{font-family:Mincho;
	panose-1:2 2 6 9 4 3 5 8 3 5;}
@font-face
	{font-family:Gulim;
	panose-1:2 11 6 0 0 1 1 1 1 1;}
@font-face
	{font-family:Century;
	panose-1:2 4 6 4 5 5 5 2 3 4;}
@font-face
	{font-family:"Angsana New";
	panose-1:2 2 6 3 5 4 5 2 3 4;}
@font-face
	{font-family:"Cordia New";
	panose-1:2 11 3 4 2 2 2 2 2 4;}
@font-face
	{font-family:Mangal;
	panose-1:0 0 4 0 0 0 0 0 0 0;}
@font-face
	{font-family:Latha;
	panose-1:2 0 4 0 0 0 0 0 0 0;}
@font-face
	{font-family:Sylfaen;
	panose-1:1 10 5 2 5 3 6 3 3 3;}
@font-face
	{font-family:Vrinda;
	panose-1:0 0 4 0 0 0 0 0 0 0;}
@font-face
	{font-family:Raavi;
	panose-1:2 0 5 0 0 0 0 0 0 0;}
@font-face
	{font-family:Shruti;
	panose-1:2 0 5 0 0 0 0 0 0 0;}
@font-face
	{font-family:Sendnya;
	panose-1:0 0 4 0 0 0 0 0 0 0;}
@font-face
	{font-family:Gautami;
	panose-1:2 0 5 0 0 0 0 0 0 0;}
@font-face
	{font-family:Tunga;
	panose-1:0 0 4 0 0 0 0 0 0 0;}
@font-face
	{font-family:"Estrangelo Edessa";
	panose-1:0 0 0 0 0 0 0 0 0 0;}
@font-face
	{font-family:"Cambria Math";
	panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
	{font-family:"Yu Gothic";
	panose-1:2 11 4 0 0 0 0 0 0 0;}
@font-face
	{font-family:DengXian;
	panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
	{font-family:Calibri;
	panose-1:2 15 5 2 2 2 4 3 2 4;}
@font-face
	{font-family:"Calibri Light";
	panose-1:2 15 3 2 2 2 4 3 2 4;}
@font-face
	{font-family:"Palatino Linotype";
	panose-1:2 4 5 2 5 5 5 3 3 4;}
@font-face
	{font-family:Verdana;
	panose-1:2 11 6 4 3 5 4 4 2 4;}
@font-face
	{font-family:"Arial Unicode MS";
	panose-1:2 11 6 4 2 2 2 2 2 4;}
@font-face
	{font-family:Aptos;}
@font-face
	{font-family:"Aptos Display";}
@font-face
	{font-family:"Segoe UI Emoji";
	panose-1:2 11 5 2 4 2 4 2 2 3;}
@font-face
	{font-family:Consolas;
	panose-1:2 11 6 9 2 2 4 3 2 4;}
@font-face
	{font-family:"Segoe UI";
	panose-1:2 11 5 2 4 2 4 2 2 3;}
@font-face
	{font-family:"Segoe UI Symbol";
	panose-1:2 11 5 2 4 2 4 2 2 3;}
 /* Style Definitions */
 p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:8.0pt;
	margin-left:0in;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;}
h1
	{mso-style-link:"Heading 1 Char";
	margin-top:.25in;
	margin-right:0in;
	margin-bottom:4.0pt;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:20.0pt;
	font-family:"Aptos Display",sans-serif;
	color:#0F4761;
	font-weight:normal;}
h2
	{mso-style-link:"Heading 2 Char";
	margin-top:8.0pt;
	margin-right:0in;
	margin-bottom:4.0pt;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:16.0pt;
	font-family:"Aptos Display",sans-serif;
	color:#0F4761;
	font-weight:normal;}
h3
	{mso-style-link:"Heading 3 Char";
	margin-top:8.0pt;
	margin-right:0in;
	margin-bottom:4.0pt;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:14.0pt;
	font-family:"Aptos",sans-serif;
	color:#0F4761;
	font-weight:normal;}
h4
	{mso-style-link:"Heading 4 Char";
	margin-top:4.0pt;
	margin-right:0in;
	margin-bottom:2.0pt;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#0F4761;
	font-weight:normal;
	font-style:italic;}
h5
	{mso-style-link:"Heading 5 Char";
	margin-top:4.0pt;
	margin-right:0in;
	margin-bottom:2.0pt;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#0F4761;
	font-weight:normal;}
h6
	{mso-style-link:"Heading 6 Char";
	margin-top:2.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#595959;
	font-weight:normal;
	font-style:italic;}
p.MsoHeading7, li.MsoHeading7, div.MsoHeading7
	{mso-style-link:"Heading 7 Char";
	margin-top:2.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#595959;}
p.MsoHeading8, li.MsoHeading8, div.MsoHeading8
	{mso-style-link:"Heading 8 Char";
	margin:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#272727;
	font-style:italic;}
p.MsoHeading9, li.MsoHeading9, div.MsoHeading9
	{mso-style-link:"Heading 9 Char";
	margin:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#272727;}
p.MsoTitle, li.MsoTitle, div.MsoTitle
	{mso-style-link:"Title Char";
	margin-top:0in;
	margin-right:0in;
	margin-bottom:4.0pt;
	margin-left:0in;
	font-size:28.0pt;
	font-family:"Aptos Display",sans-serif;
	letter-spacing:-.5pt;}
p.MsoTitleCxSpFirst, li.MsoTitleCxSpFirst, div.MsoTitleCxSpFirst
	{mso-style-link:"Title Char";
	margin:0in;
	font-size:28.0pt;
	font-family:"Aptos Display",sans-serif;
	letter-spacing:-.5pt;}
p.MsoTitleCxSpMiddle, li.MsoTitleCxSpMiddle, div.MsoTitleCxSpMiddle
	{mso-style-link:"Title Char";
	margin:0in;
	font-size:28.0pt;
	font-family:"Aptos Display",sans-serif;
	letter-spacing:-.5pt;}
p.MsoTitleCxSpLast, li.MsoTitleCxSpLast, div.MsoTitleCxSpLast
	{mso-style-link:"Title Char";
	margin-top:0in;
	margin-right:0in;
	margin-bottom:4.0pt;
	margin-left:0in;
	font-size:28.0pt;
	font-family:"Aptos Display",sans-serif;
	letter-spacing:-.5pt;}
p.MsoSubtitle, li.MsoSubtitle, div.MsoSubtitle
	{mso-style-link:"Subtitle Char";
	margin-top:0in;
	margin-right:0in;
	margin-bottom:8.0pt;
	margin-left:0in;
	line-height:115%;
	font-size:14.0pt;
	font-family:"Aptos",sans-serif;
	color:#595959;
	letter-spacing:.75pt;}
p
	{margin-right:0in;
	margin-left:0in;
	font-size:12.0pt;
	font-family:"Times New Roman",serif;}
code
	{font-family:"Courier New";}
p.MsoListParagraph, li.MsoListParagraph, div.MsoListParagraph
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:8.0pt;
	margin-left:.5in;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;}
p.MsoListParagraphCxSpFirst, li.MsoListParagraphCxSpFirst, div.MsoListParagraphCxSpFirst
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:.5in;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;}
p.MsoListParagraphCxSpMiddle, li.MsoListParagraphCxSpMiddle, div.MsoListParagraphCxSpMiddle
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:.5in;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;}
p.MsoListParagraphCxSpLast, li.MsoListParagraphCxSpLast, div.MsoListParagraphCxSpLast
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:8.0pt;
	margin-left:.5in;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;}
p.MsoQuote, li.MsoQuote, div.MsoQuote
	{mso-style-link:"Quote Char";
	margin-top:8.0pt;
	margin-right:0in;
	margin-bottom:8.0pt;
	margin-left:0in;
	text-align:center;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#404040;
	font-style:italic;}
p.MsoIntenseQuote, li.MsoIntenseQuote, div.MsoIntenseQuote
	{mso-style-link:"Intense Quote Char";
	margin-top:.25in;
	margin-right:.6in;
	margin-bottom:.25in;
	margin-left:.6in;
	text-align:center;
	line-height:115%;
	border:none;
	padding:0in;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#0F4761;
	font-style:italic;}
span.MsoIntenseEmphasis
	{color:#0F4761;
	font-style:italic;}
span.MsoIntenseReference
	{font-variant:small-caps;
	color:#0F4761;
	letter-spacing:.25pt;
	font-weight:bold;}
span.Heading1Char
	{mso-style-name:"Heading 1 Char";
	mso-style-link:"Heading 1";
	font-family:"Aptos Display",sans-serif;
	color:#0F4761;}
span.Heading2Char
	{mso-style-name:"Heading 2 Char";
	mso-style-link:"Heading 2";
	font-family:"Aptos Display",sans-serif;
	color:#0F4761;}
span.Heading3Char
	{mso-style-name:"Heading 3 Char";
	mso-style-link:"Heading 3";
	font-family:"Times New Roman",serif;
	color:#0F4761;}
span.Heading4Char
	{mso-style-name:"Heading 4 Char";
	mso-style-link:"Heading 4";
	font-family:"Times New Roman",serif;
	color:#0F4761;
	font-style:italic;}
span.Heading5Char
	{mso-style-name:"Heading 5 Char";
	mso-style-link:"Heading 5";
	font-family:"Times New Roman",serif;
	color:#0F4761;}
span.Heading6Char
	{mso-style-name:"Heading 6 Char";
	mso-style-link:"Heading 6";
	font-family:"Times New Roman",serif;
	color:#595959;
	font-style:italic;}
span.Heading7Char
	{mso-style-name:"Heading 7 Char";
	mso-style-link:"Heading 7";
	font-family:"Times New Roman",serif;
	color:#595959;}
span.Heading8Char
	{mso-style-name:"Heading 8 Char";
	mso-style-link:"Heading 8";
	font-family:"Times New Roman",serif;
	color:#272727;
	font-style:italic;}
span.Heading9Char
	{mso-style-name:"Heading 9 Char";
	mso-style-link:"Heading 9";
	font-family:"Times New Roman",serif;
	color:#272727;}
span.TitleChar
	{mso-style-name:"Title Char";
	mso-style-link:Title;
	font-family:"Aptos Display",sans-serif;
	letter-spacing:-.5pt;}
span.SubtitleChar
	{mso-style-name:"Subtitle Char";
	mso-style-link:Subtitle;
	font-family:"Times New Roman",serif;
	color:#595959;
	letter-spacing:.75pt;}
span.QuoteChar
	{mso-style-name:"Quote Char";
	mso-style-link:Quote;
	color:#404040;
	font-style:italic;}
span.IntenseQuoteChar
	{mso-style-name:"Intense Quote Char";
	mso-style-link:"Intense Quote";
	color:#0F4761;
	font-style:italic;}
p.msonormal0, li.msonormal0, div.msonormal0
	{mso-style-name:msonormal;
	margin-right:0in;
	margin-left:0in;
	font-size:12.0pt;
	font-family:"Times New Roman",serif;}
span.citation-709
	{mso-style-name:citation-709;}
span.math-inline
	{mso-style-name:math-inline;}
span.citation-708
	{mso-style-name:citation-708;}
span.citation-707
	{mso-style-name:citation-707;}
span.citation-699
	{mso-style-name:citation-699;}
span.citation-698
	{mso-style-name:citation-698;}
span.citation-691
	{mso-style-name:citation-691;}
span.citation-690
	{mso-style-name:citation-690;}
span.citation-689
	{mso-style-name:citation-689;}
span.citation-684
	{mso-style-name:citation-684;}
span.citation-683
	{mso-style-name:citation-683;}
span.citation-682
	{mso-style-name:citation-682;}
span.citation-681
	{mso-style-name:citation-681;}
span.citation-680
	{mso-style-name:citation-680;}
span.citation-679
	{mso-style-name:citation-679;}
span.citation-676
	{mso-style-name:citation-676;}
span.citation-675
	{mso-style-name:citation-675;}
span.citation-674
	{mso-style-name:citation-674;}
span.citation-673
	{mso-style-name:citation-673;}
span.citation-672
	{mso-style-name:citation-672;}
span.citation-671
	{mso-style-name:citation-671;}
span.citation-670
	{mso-style-name:citation-670;}
span.citation-669
	{mso-style-name:citation-669;}
span.citation-668
	{mso-style-name:citation-668;}
span.citation-666
	{mso-style-name:citation-666;}
span.citation-665
	{mso-style-name:citation-665;}
span.citation-662
	{mso-style-name:citation-662;}
span.citation-661
	{mso-style-name:citation-661;}
span.citation-660
	{mso-style-name:citation-660;}
span.citation-659
	{mso-style-name:citation-659;}
span.citation-658
	{mso-style-name:citation-658;}
span.citation-657
	{mso-style-name:citation-657;}
span.citation-656
	{mso-style-name:citation-656;}
span.citation-655
	{mso-style-name:citation-655;}
span.citation-654
	{mso-style-name:citation-654;}
span.citation-653
	{mso-style-name:citation-653;}
span.citation-652
	{mso-style-name:citation-652;}
span.citation-651
	{mso-style-name:citation-651;}
span.citation-650
	{mso-style-name:citation-650;}
span.citation-649
	{mso-style-name:citation-649;}
span.citation-648
	{mso-style-name:citation-648;}
span.citation-647
	{mso-style-name:citation-647;}
span.citation-646
	{mso-style-name:citation-646;}
span.citation-645
	{mso-style-name:citation-645;}
span.citation-644
	{mso-style-name:citation-644;}
span.citation-643
	{mso-style-name:citation-643;}
span.citation-642
	{mso-style-name:citation-642;}
span.citation-641
	{mso-style-name:citation-641;}
span.citation-640
	{mso-style-name:citation-640;}
span.citation-639
	{mso-style-name:citation-639;}
span.citation-638
	{mso-style-name:citation-638;}
span.citation-637
	{mso-style-name:citation-637;}
span.citation-634
	{mso-style-name:citation-634;}
span.citation-632
	{mso-style-name:citation-632;}
span.citation-631
	{mso-style-name:citation-631;}
span.citation-630
	{mso-style-name:citation-630;}
span.citation-629
	{mso-style-name:citation-629;}
span.citation-628
	{mso-style-name:citation-628;}
span.citation-627
	{mso-style-name:citation-627;}
span.citation-626
	{mso-style-name:citation-626;}
span.citation-625
	{mso-style-name:citation-625;}
span.citation-623
	{mso-style-name:citation-623;}
span.citation-622
	{mso-style-name:citation-622;}
span.citation-621
	{mso-style-name:citation-621;}
span.citation-620
	{mso-style-name:citation-620;}
span.citation-619
	{mso-style-name:citation-619;}
span.citation-618
	{mso-style-name:citation-618;}
span.citation-617
	{mso-style-name:citation-617;}
span.citation-616
	{mso-style-name:citation-616;}
span.citation-615
	{mso-style-name:citation-615;}
span.citation-613
	{mso-style-name:citation-613;}
span.citation-612
	{mso-style-name:citation-612;}
span.citation-611
	{mso-style-name:citation-611;}
span.citation-609
	{mso-style-name:citation-609;}
span.citation-608
	{mso-style-name:citation-608;}
span.citation-607
	{mso-style-name:citation-607;}
span.citation-606
	{mso-style-name:citation-606;}
span.citation-605
	{mso-style-name:citation-605;}
span.citation-604
	{mso-style-name:citation-604;}
span.citation-603
	{mso-style-name:citation-603;}
span.citation-602
	{mso-style-name:citation-602;}
span.citation-601
	{mso-style-name:citation-601;}
span.citation-598
	{mso-style-name:citation-598;}
span.citation-597
	{mso-style-name:citation-597;}
span.citation-596
	{mso-style-name:citation-596;}
span.citation-595
	{mso-style-name:citation-595;}
span.citation-594
	{mso-style-name:citation-594;}
span.citation-593
	{mso-style-name:citation-593;}
span.citation-591
	{mso-style-name:citation-591;}
span.citation-590
	{mso-style-name:citation-590;}
span.citation-589
	{mso-style-name:citation-589;}
span.citation-588
	{mso-style-name:citation-588;}
span.citation-586
	{mso-style-name:citation-586;}
span.citation-585
	{mso-style-name:citation-585;}
span.citation-584
	{mso-style-name:citation-584;}
span.citation-583
	{mso-style-name:citation-583;}
span.citation-582
	{mso-style-name:citation-582;}
span.citation-581
	{mso-style-name:citation-581;}
span.citation-579
	{mso-style-name:citation-579;}
span.citation-578
	{mso-style-name:citation-578;}
span.citation-577
	{mso-style-name:citation-577;}
span.citation-576
	{mso-style-name:citation-576;}
span.msoIns
	{mso-style-name:"";
	text-decoration:underline;
	color:teal;}
span.msoDel
	{mso-style-name:"";
	text-decoration:line-through;
	color:red;}
.MsoPapDefault
	{margin-bottom:8.0pt;
	line-height:115%;}
@page WordSection1
	{size:8.5in 11.0in;
	margin:1.0in 1.0in 1.0in 1.0in;}
div.WordSection1
	{page:WordSection1;}
 /* List Definitions */
 ol
	{margin-bottom:0in;}
ul
	{margin-bottom:0in;}
-->
</style>

</head>

<body lang=EN-US style='word-wrap:break-word'>

<div class=WordSection1>

<p class=MsoNormal><b>Slide 1: Unit of CPU Performance Measurement: FLOPs</b></p>

<p class=MsoNormal><b>Explanation (easy language):</b><br>
FLOPs means <i>Floating Point Operations per Second</i>.<br>
It measures how many math operations (like +, -, ×, ÷) a processor can perform
every second. It’s a key measure of CPU or GPU speed.</p>

<p class=MsoNormal><b>MCQs:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>FLOPs stand for:<br>
     a) Floating Point Operations per Second <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     b) File Operations per Second<br>
     c) Frame Operations per Second<br>
     d) Fast Logic Operations</li>
 <li class=MsoNormal>What does a higher FLOP value mean?<br>
     a) Slower performance<br>
     b) Faster performance <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span></li>
</ol>

<p class=MsoNormal><b>Use Case:</b><br>
Used in benchmarking CPUs and GPUs in scientific computing, simulations, and AI
models.</p>

<p class=MsoNormal><b>Numerical Example:</b><br>
If a CPU performs <b>2 × 10&#8313; operations in 1 second</b>, it has <b>2
GFLOPs</b> (2 Giga FLOPs).</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 2–3: CPU Performance Model (FLOPs)</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
The CPU performance can be calculated by counting how many floating-point
operations (like multiply and add) are done in a second.<br>
In the given code, each loop does one multiply (*) and one add (+) — i.e., 2
FLOPs per loop.</p>

<p class=MsoNormal><b>MCQs:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>In the code x = a * x + b;, how many FLOPs are there?<br>
     a) 1<br>
     b) 2 <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     c) 3<br>
     d) 4</li>
 <li class=MsoNormal>If the loop runs 50 times, total FLOPs = ?<br>
     a) 25<br>
     b) 50<br>
     c) 100 <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     d) 200</li>
</ol>

<p class=MsoNormal><b>Use Case:</b><br>
Used in measuring CPU efficiency in numerical computation, e.g., physics
simulation or weather prediction.</p>

<p class=MsoNormal><b>Numerical:</b><br>
If function mul_add() takes <b>2 seconds</b> to finish <b>100 FLOPs</b>, then
performance = 100 / 2 = <b>50 FLOPs/sec</b>.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 4–5: FLOPs and Arithmetic Intensity (AI)</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
Even if a CPU/GPU is fast, reading data from memory is slow.<br>
<b>Arithmetic Intensity (AI)</b> = number of computations per byte of data
read/written.<br>
Higher AI = better performance (more work per memory load).</p>

<p class=MsoNormal><b>MCQs:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Arithmetic Intensity relates:<br>
     a) FLOPs and bandwidth <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     b) FLOPs and power<br>
     c) FLOPs and voltage<br>
     d) FLOPs and heat</li>
 <li class=MsoNormal>Memory bandwidth is measured in:<br>
     a) FLOPs<br>
     b) GB/s <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span></li>
</ol>

<p class=MsoNormal><b>Use Case:</b><br>
Used in performance modeling to check if an algorithm is <i>compute-bound</i>
or <i>memory-bound.</i></p>

<p class=MsoNormal><b>Numerical:</b><br>
If a program reads/writes <b>8 GB</b> in <b>2 seconds</b>,<br>
&#8594; Bandwidth = (8 / 2) = <b>4 GB/s</b></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 6–7: Calculation of Arithmetic Intensity</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
Each iteration performs 2 memory loads (x[i], y[i]) and 1 store (y[i]), and 2
operations (multiply and add).<br>
So, data traffic = 24 bytes × n.<br>
AI = computations / data transfer = 2 / 24 = <b>1/12 FLOPs/byte</b>.</p>

<p class=MsoNormal><b>MCQs:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>For y[i] = a * x[i] + y[i];, how many memory operations?<br>
     a) 1<br>
     b) 2<br>
     c) 3 <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span></li>
 <li class=MsoNormal>Arithmetic Intensity =<br>
     a) Bytes / FLOPs<br>
     b) FLOPs / Bytes <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span></li>
</ol>

<p class=MsoNormal><b>Use Case:</b><br>
Used to analyze code efficiency in data-heavy applications like image
processing or matrix operations.</p>

<p class=MsoNormal><b>Numerical:</b><br>
If n = 1000 &#8594; Data transfer = 24 × 1000 = <b>24,000 bytes</b>,<br>
Computations = 2 × 1000 = 2000,<br>
AI = 2000 / 24,000 = <b>0.083 FLOPs/byte</b></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 8: Homework – DGEMM</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
DGEMM performs matrix multiplication of three matrices (A × B = C).<br>
AI = n/16 means for large n, AI grows, showing good computation-to-memory
ratio.</p>

<p class=MsoNormal><b>MCQs:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>DGEMM mainly performs:<br>
     a) Vector addition<br>
     b) Matrix multiplication <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span></li>
 <li class=MsoNormal>Arithmetic Intensity of DGEMM =<br>
     a) n/8<br>
     b) n/16 <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     c) n/4</li>
</ol>

<p class=MsoNormal><b>Use Case:</b><br>
Used in linear algebra libraries (like BLAS, LAPACK), neural networks, and
simulations.</p>

<p class=MsoNormal><b>Numerical:</b><br>
If n = 64, then AI = 64 / 16 = <b>4 FLOPs/byte</b></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 9–10: Amdahl’s Law</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
Amdahl’s law shows that even if you parallelize a program, the speedup is
limited by the portion that remains serial (cannot be parallelized).</p>

<p class=MsoNormal><b>Formula:</b><br>
Speedup = 1 / [(1 - P) + (P / N)]<br>
Where P = parallel fraction, N = number of cores.</p>

<p class=MsoNormal><b>MCQs:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Amdahl’s law is used to estimate:<br>
     a) Power efficiency<br>
     b) Theoretical speedup <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     c) Memory usage<br>
     d) FLOPs</li>
 <li class=MsoNormal>If 100% code parallel &#8594; speedup = ?<br>
     a) Infinite <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     b) 1<br>
     c) 0</li>
</ol>

<p class=MsoNormal><b>Use Case:</b><br>
Used to analyze the benefit of adding more cores in CPUs/GPUs.</p>

<p class=MsoNormal><b>Numerical:</b><br>
If 1% serial, P = 0.99, N = 61,<br>
Speedup = 1 / (0.01 + 0.99 / 61) = <b>&#8776; 38.1</b></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 12–22: Matrix Multiplication on GPU</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
On GPUs, matrix multiplication is parallelized — each thread calculates one
element.<br>
Data is split into blocks, loaded into <i>shared memory</i> for reuse.<br>
Goal: reduce memory access and increase parallel computation.</p>

<p class=MsoNormal><b>MCQs:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Shared memory is:<br>
     a) Slow global memory<br>
     b) Fast on-chip memory <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span></li>
 <li class=MsoNormal>Each thread in GPU computes:<br>
     a) Whole matrix<br>
     b) One element <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span></li>
</ol>

<p class=MsoNormal><b>Use Case:</b><br>
Used in deep learning (e.g., training neural networks), 3D graphics, and
physics simulations.</p>

<p class=MsoNormal><b>Numerical:</b><br>
If each block handles a <b>16×16</b> tile and grid = <b>8×8 blocks</b>,<br>
&#8594; Total matrix size = (16×8) × (16×8) = <b>128×128</b></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 23: CPU/GPU Synchronization</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
Synchronization ensures CPU and GPU work in order — CPU waits for GPU to finish
tasks before using results. It avoids data conflicts.</p>

<p class=MsoNormal><b>MCQs:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Synchronization ensures:<br>
     a) Random execution<br>
     b) Ordered execution <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span></li>
 <li class=MsoNormal>GPU synchronization is needed when:<br>
     a) Using shared data <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     b) Using registers only</li>
</ol>

<p class=MsoNormal><b>Use Case:</b><br>
Used in CUDA programming to coordinate between kernels and host CPU.</p>

<p class=MsoNormal><b>Numerical:</b><br>
If CPU launches kernel taking <b>5 ms</b>, and synchronization adds <b>1 ms
overhead</b>,<br>
&#8594; Total execution = <b>6 ms</b></p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<div style='border:none;border-bottom:double windowtext 2.25pt;padding:0in 0in 1.0pt 0in'>

<p class=MsoNormal style='border:none;padding:0in'>&nbsp;</p>

</div>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal><b>Slide 1: Title Slide</b></p>

<p class=MsoNormal>Explanation:</p>

<p class=MsoNormal>This is the title slide, simply indicating that it's Lecture
05 of a course on GPU Programming1.</p>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is the topic of this lecture?</li>
</ol>

<p class=MsoNormal>a) CPU Architecture</p>

<p class=MsoNormal>b) Memory Management</p>

<p class=MsoNormal>c) GPU Programming</p>

<p class=MsoNormal>d) Web Development</p>

<p class=MsoNormal>Answer: c)</p>

<p class=MsoNormal><b>Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Introductory Context:</b> This slide's main purpose is
     to establish the subject and number of the lecture within an academic or
     training course.</li>
</ul>

<p class=MsoNormal><b>Numerical Example:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>(Not applicable for a title slide.)</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 2: Unit of CPU Performance Measurement: FLOPS and
Floating-Point Representation</b></p>

<p class=MsoNormal>Explanation:</p>

<p class=MsoNormal>This slide introduces FLOPS (Floating-point operations per
second) as a unit to measure CPU performance2. It then details the structure of
single-precision floating-point numbers: 1 bit for the Sign, 8 bits for the
Exponent, and 23 bits for the Mantissa3. It also mentions that double-precision
uses 1, 11, and 52 bits for these fields, respectively4. The latter half of the
slide provides an example of converting a binary floating-point number to its
decimal value (e.g., $1.00101_2 = 1.5625$) 5and converting a decimal number to
its binary representation ($34.625_{10} = 100010.101_2$)6.</p>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What are the field lengths (Sign, Exponent, Mantissa) for
     double-precision floating-point numbers?</li>
</ol>

<p class=MsoNormal>a) 1, 8, and 23 bits</p>

<p class=MsoNormal>b) 1, 10, and 53 bits</p>

<p class=MsoNormal>c) 1, 11, and 52 bits</p>

<p class=MsoNormal>d) 2, 16, and 48 bits</p>

<p class=MsoNormal>Answer: c) <sup>7</sup></p>

<p class=MsoNormal><b>Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Performance Benchmarking:</b> FLOPS is a key metric for
     comparing the computational power of different processors (CPUs or GPUs).</li>
 <li class=MsoNormal><b>Numerical Accuracy:</b> Understanding the bit
     allocation helps in choosing between single- and double-precision
     depending on the required numerical accuracy and memory usage in
     scientific simulations or machine learning.</li>
</ul>

<p class=MsoNormal><b>Numerical Example:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Conversion:</b> What is the decimal value of $1.00101_2$?</li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal><b>Calculation:</b> $1 \times 2^0 + 0 \times 2^{-1} + 0
      \times 2^{-2} + 1 \times 2^{-3} + 0 \times 2^{-4} + 1 \times 2^{-5}$<sup>8</sup>.</li>
  <li class=MsoNormal>$1 + 0 + 0 + 1/8 + 0 + 1/32 = 1 + 0.125 + 0.03125 =
      \mathbf{1.15625}$9.</li>
 </ul>
</ul>

<p class=MsoNormal>(Note: The slide's example calculation leads to $1.5625$ 10,
which is the correct decimal for $1.01010_2$, but the slide presents
$1.00101_2$11, which is $1.15625$. Sticking to the slide's presented result of
$1.5625$ based on the provided calculations.)</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 3: FLOPS Calculation Example</b></p>

<p class=MsoNormal>Explanation:</p>

<p class=MsoNormal>This slide elaborates on FLOPS as a simple measurement of
floating-point operations per second, essentially counting the number of
mathematical operations (like addition, multiplication, etc.) a computer
performs on an algorithm per second12. It provides a C-like code example,
mul_add, which executes the line x = a * x + b; (line (a)) 50 times in a
loop131313131313. Line (a) involves two floating-point operations (*
multiplication and + addition)14. Therefore, a single call to mul_add performs
$50 \times 2 = \mathbf{100~FLOP}$15. If this single call takes 1 second on a
CPU, the CPU's performance is 100 FLOPs16.</p>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>In the provided mul_add function, how many floating-point
     operations (FLOP) are executed in a single call?</li>
</ol>

<p class=MsoNormal>a) 2</p>

<p class=MsoNormal>b) 50</p>

<p class=MsoNormal>c) 51</p>

<p class=MsoNormal>d) 100</p>

<p class=MsoNormal>Answer: d) <sup>171717171717171717</sup></p>

<p class=MsoNormal><b>Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Algorithm Analysis:</b> Used by developers to manually
     calculate the computational complexity (total number of floating-point
     operations) of a small, core part of a numerical algorithm.</li>
 <li class=MsoNormal><b>Performance Estimation:</b> Provides a basic metric for
     estimating the execution time if the processor's FLOPs rating is known.</li>
</ul>

<p class=MsoNormal><b>Numerical Example:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Problem:</b> If the loop in mul_add ran for $i &lt;
     100$ instead of $i &lt; 50$, how many total FLOP would one call execute?</li>
 <li class=MsoNormal><b>Calculation:</b> The loop runs 100 times. Each
     iteration has 2 FLOP (* and +)<sup>18</sup>.</li>
 <li class=MsoNormal>Total FLOP = $100 \times 2 = \mathbf{200~FLOP}$.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 4: Computation &amp; Communication (Memory
Hierarchy)</b></p>

<p class=MsoNormal>Explanation:</p>

<p class=MsoNormal>This slide illustrates the memory hierarchy in a CPU system,
emphasizing that performance involves both Computation (FLOPs) and
Communication (data movement)19. The hierarchy shows 4 CPU cores 20accessing
data through multiple levels of cache (L1, L2, Shared L3) before reaching the
Main Memory (DRAM)21212121212121212121212121212121. The accompanying table
highlights the significant difference in access time, showing that accessing
Main Memory is about 200 CPU Cycles, while accessing a Register is only 1 Cycle22.
Since memory operations are so slow, it's crucial to perform as many operations
as possible with loaded data before fetching the next data23. The metric used
to measure this is Arithmetic Intensity (AI)24.</p>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Which memory location has the highest access latency
     (takes the most CPU cycles to access)?</li>
</ol>

<p class=MsoNormal>a) Register (~1)</p>

<p class=MsoNormal>b) L1 Cache (~4)</p>

<p class=MsoNormal>c) L3 Cache (~75)</p>

<p class=MsoNormal>d) Main Memory (~200)</p>

<p class=MsoNormal>Answer: d) <sup>25</sup></p>

<p class=MsoNormal><b>Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Code Optimization:</b> Programmers design algorithms
     (e.g., using data blocking/tiling) to maximize data reuse and keep data in
     faster memory (like L1/L2 caches or registers) to reduce the penalty of
     slow memory access (DRAM)<sup>26</sup>.</li>
 <li class=MsoNormal><b>Architecture Design:</b> Helps engineers understand the
     bottlenecks in a system to balance computation power (cores) with memory
     bandwidth (speed of data transfer).</li>
</ul>

<p class=MsoNormal><b>Numerical Example:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Latency Comparison:</b> An L1 Cache access takes about
     4 cycles, while a Main Memory access takes about 200 cycles<sup>27</sup>.
     The main memory access is $\frac{200}{4} = \mathbf{50}$ times slower than
     L1 cache access.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 5: Arithmetic Intensity and Memory Bandwidth</b></p>

<p class=MsoNormal>Explanation:</p>

<p class=MsoNormal>This slide defines Memory Bandwidth as the rate at which
data is transferred (typically in $\mathbf{GB/s}$)28. It is calculated as
$\mathbf{\text{Bandwidth} = [(\text{bytes read} + \text{bytes written}) / 10^9]
/ \text{execution time}}$29. The slide notes that the Memory access bandwidth
of GPUs is limited compared to their peak compute throughput30. Consequently,
algorithms with High Arithmetic Intensity (many arithmetic operations per
memory access) perform well on GPU architectures31.</p>

<p class=MsoNormal>The example highlights the imbalance:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Fermi GPU peak single-precision (SP) throughput: <b>1
     TFLOP/s</b><sup>32</sup>.</li>
 <li class=MsoNormal>Global memory bandwidth: <b>144 GB/s</b><sup>33</sup>.</li>
 <li class=MsoNormal>To reach the peak 1 TFLOP/s, the system would require $\frac{1000
     \text{ GFLOP/s}}{36 \text{ GB/s/operand}} \approx \mathbf{28}$ SP
     arithmetic operations for every 4-byte operand transferred<sup>34343434</sup>.</li>
</ul>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Why do high arithmetic intensity algorithms perform well
     on GPU architectures?</li>
</ol>

<p class=MsoNormal>a) GPUs have unlimited memory bandwidth.</p>

<p class=MsoNormal>b) High AI means fewer computations overall.</p>

<p class=MsoNormal>c) GPU memory access bandwidth is limited compared to peak
compute throughput.</p>

<p class=MsoNormal>d) GPUs only support single-precision operations.</p>

<p class=MsoNormal>Answer: c) <sup>35</sup></p>

<p class=MsoNormal><b>Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>GPU Kernel Development:</b> Programmers aim to
     structure GPU kernels (programs) to have high Arithmetic Intensity, often
     by reusing data already loaded into faster, on-chip memories like shared
     memory<sup>36</sup>.</li>
 <li class=MsoNormal><b>Roofline Model:</b> Arithmetic intensity is the x-axis
     for the Roofline performance model, which bounds an application's
     performance by both compute throughput and memory bandwidth (discussed in
     Slide 9)<sup>37</sup>.</li>
</ul>

<p class=MsoNormal><b>Numerical Example:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Problem:</b> If an application has a memory bandwidth
     of $50 \text{ GB/s}$ and transfers a total of $200 \text{ GB}$ of data,
     what is the execution time?</li>
 <li class=MsoNormal><b>Formula:</b> $\text{Execution Time} = \frac{\text{Total
     Data Transfer (bytes)}}{\text{Bandwidth (bytes/s)}}$.</li>
 <li class=MsoNormal><b>Calculation:</b> $\text{Execution Time} = \frac{200
     \text{ GB}}{50 \text{ GB/s}} = \mathbf{4 \text{ seconds}}$.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 6 &amp; 7: Calculation of Arithmetic Intensity
(CPUs)</b></p>

<p class=MsoNormal>Explanation:</p>

<p class=MsoNormal>These slides define and calculate Arithmetic Intensity
(AI)383838383838383838. AI is the measure of how many floating-point operations
are done per byte loaded or stored from memory39.</p>

<p class=MsoNormal>The formula is:</p>

<p class=MsoNormal>$$\mathbf{AI = \frac{\text{Total number of computations
(FLOP)}}{\text{Total amount of data transfer (bytes)}} =
\frac{FLOP/s}{Bytes/s}}$$</p>

<p class=MsoNormal><sup>40404040</sup>.</p>

<p class=MsoNormal>A code example arraymul_add for $n$ elements is analyzed<sup>41414141</sup>.
The loop body is $y[i] = a * x[i] + y[i]$<sup>42</sup>:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Total FLOP:</b> Each iteration has <b>2 FLOP</b> (* and
     +). Over $n$ iterations, the total FLOP is $\mathbf{2n}$<sup>43</sup>.</li>
 <li class=MsoNormal><b>Total Data Transfer (Traffic):</b> Assuming <b>double-precision</b>
     data types (which are <b>8 bytes</b> per element)<sup>44</sup>:</li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Load $a$: <b>1</b> load (constant $a$ is loaded once).</li>
  <li class=MsoNormal>Load $x[i]$: <b>$n$</b> loads.</li>
  <li class=MsoNormal>Load $y[i]$: <b>$n$</b> loads.</li>
  <li class=MsoNormal>Store $y[i]$: <b>$n$</b> stores.</li>
  <li class=MsoNormal>The slide's calculation is based on the per-iteration
      traffic for the arrays $x$ and $y$: 2 loads ($x[i], y[i]$) and 1 store ($y[i]$),
      plus the initial load of $a$. The provided traffic formula simplifies
      this to focus on array access over $n$ iterations: $\mathbf{[(2 \text{
      loads})(8 \text{ bytes/load}) + (1 \text{ store})(8 \text{ bytes/store})]
      \times n} = (16+8)n = \mathbf{24n}$ bytes (ignoring the single load of $a$)<sup>45</sup>.</li>
 </ul>
 <li class=MsoNormal><b>Arithmetic Intensity (AI):</b> $\mathbf{AI =
     \frac{2n}{24n} = \frac{1}{12}}$<sup>46</sup>.</li>
</ol>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is the correct formula for Arithmetic Intensity (AI)?</li>
</ol>

<p class=MsoNormal>a) $\text{FLOPs} \times \text{Bytes}$</p>

<p class=MsoNormal>b) $\text{Bytes/second} / \text{FLOPs/second}$</p>

<p class=MsoNormal>c) $\text{FLOPs/second} / \text{Bytes/second}$</p>

<p class=MsoNormal>d) $\text{Total Data Transfer} / \text{Total Computations}$</p>

<p class=MsoNormal>Answer: c) <sup>47</sup></p>

<p class=MsoNormal><b>Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Performance Bottleneck Identification:</b> An AI of $1/12$
     is very low, suggesting the application is <b>memory-bound</b>
     (communication/data transfer is the bottleneck) rather than compute-bound.
     This indicates the need for optimization techniques like caching or data
     reuse.</li>
</ul>

<p class=MsoNormal><b>Numerical Example:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Problem:</b> If the arraymul_add function was modified
     to use <b>single-precision</b> floating-point numbers (<b>4 bytes</b> per
     element)<sup>48</sup>. What would the new AI be?</li>
 <li class=MsoNormal><b>Total FLOP:</b> Remains <b>$2n$</b> (2 ops per
     iteration).</li>
 <li class=MsoNormal><b>Total Traffic:</b> $[(2 \text{ loads})(4 \text{
     bytes/load}) + (1 \text{ store})(4 \text{ bytes/store})] \times n = (8+4)n
     = \mathbf{12n}$ bytes.</li>
 <li class=MsoNormal><b>AI:</b> $\frac{2n}{12n} = \mathbf{\frac{1}{6}}$.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 8: Homework (Matrix Multiplication AI)</b></p>

<p class=MsoNormal>Explanation:</p>

<p class=MsoNormal>This slide presents the homework problem of calculating the
Arithmetic Intensity for a matrix multiplication (dgemm) function, which
computes $C[i][j] = \sum_k A[i][k] \times B[k][j]$ for $n \times n$
matrices494949494949494949.</p>

<p class=MsoNormal>The provided <b>Answer</b> for the Arithmetic Intensity is $\mathbf{n/16}$<sup>50</sup>.
This implies that for a larger matrix size $n$, the Arithmetic Intensity is
higher, which is typical for matrix multiplication algorithms.</p>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>For the matrix multiplication (dgemm) function given on
     this slide, what is the stated Arithmetic Intensity?</li>
</ol>

<p class=MsoNormal>a) $1/12$</p>

<p class=MsoNormal>b) $2n$</p>

<p class=MsoNormal>c) $n/16$</p>

<p class=MsoNormal>d) $16/n$</p>

<p class=MsoNormal>Answer: c) <sup>51</sup></p>

<p class=MsoNormal><b>Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Algorithm Scaling:</b> The $n/16$ result shows that
     matrix multiplication has a higher AI than the vector-add example ($1/12$),
     meaning it reuses data more efficiently, especially as $n$ increases. This
     is a common characteristic of matrix operations that make them well-suited
     for high-performance computing.</li>
</ul>

<p class=MsoNormal><b>Numerical Example:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Problem:</b> Using the provided answer, what is the
     Arithmetic Intensity for an $n=64$ matrix?</li>
 <li class=MsoNormal><b>Calculation:</b> $\text{AI} = n/16 = 64/16 =
     \mathbf{4}$ FLOP/Byte.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 9: CPU Performance Model (Roofline Model)</b></p>

<p class=MsoNormal>Explanation:</p>

<p class=MsoNormal>This slide introduces the CPU Performance Model, often
referred to as the Roofline Model52. This model assumes a perfect overlap of
communication and computation, where the run time is determined by whichever
operation takes longer53. The attainable performance, GFLOP/s (Giga
Floating-point operations per second), is bounded by two factors:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Peak GFLOP/s</b> (Compute limit - the flat
     &quot;roof&quot; line)<sup>54</sup>.</li>
 <li class=MsoNormal><b>Arithmetic Intensity ($\text{AI}$) $\times$ Peak GB/s</b>
     (Memory/Communication limit - the slanted line)<sup>55</sup>.</li>
</ol>

<p class=MsoNormal>The formula is:</p>

<p class=MsoNormal>$$\mathbf{\text{GFLOP/s} = \min(\text{Peak GFLOP/s},
\text{AI} \times \text{Peak GB/s})}$$</p>

<p class=MsoNormal><sup>56</sup>.</p>

<p class=MsoNormal>$\mathbf{AI}$ (Arithmetic Intensity) is defined as $\frac{\text{FLOPs}}{\text{Bytes}}$
(as presented to DRAM)<sup>57</sup>. The accompanying graph visually represents
this performance ceiling<sup>585858585858585858</sup>.</p>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>In the CPU Performance Model, what determines the
     attainable GFLOP/s?</li>
</ol>

<p class=MsoNormal>a) The maximum of Peak GFLOP/s and $\text{AI} \times
\text{Peak GB/s}$.</p>

<p class=MsoNormal>b) $\text{Peak GFLOP/s} - \text{AI} \times \text{Peak
GB/s}$.</p>

<p class=MsoNormal>c) The minimum of Peak GFLOP/s and $\text{AI} \times
\text{Peak GB/s}$.</p>

<p class=MsoNormal>d) Only the $\text{AI}$ value.</p>

<p class=MsoNormal>Answer: c) <sup>59</sup></p>

<p class=MsoNormal><b>Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Performance Analysis:</b> The model helps diagnose
     whether an application is <b>compute-bound</b> (performance is limited by
     the processor's maximum FLOPs) or <b>memory-bound</b> (performance is
     limited by the memory bandwidth) based on where its $\text{AI}$ falls on
     the graph.</li>
</ul>

<p class=MsoNormal><b>Numerical Example:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Problem:</b> A CPU has a <b>Peak GFLOP/s of 100</b> and
     a <b>Peak GB/s of 25</b>.</li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>If an algorithm has $\mathbf{AI=2}$ FLOP/Byte, what is
      the attainable GFLOP/s?</li>
  <li class=MsoNormal><b>Calculation:</b> $\min(100, 2 \times 25) = \min(100,
      50) = \mathbf{50 \text{ GFLOP/s}}$. (Memory-bound)</li>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 10 &amp; 11: Amdahl's Law</b></p>

<p class=MsoNormal>Explanation:</p>

<p class=MsoNormal><b>Amdahl's law</b> describes the <b>theoretical speedup ($S$)</b>
of a program when only a portion of it is parallelized<sup>60</sup>. The core
idea is that the speedup is fundamentally <b>limited by the portion of the
program that must run sequentially</b> (cannot be parallelized)<sup>6161</sup>.</p>

<p class=MsoNormal>The formula for speedup ($S$) with $N$ processors is:</p>

<p class=MsoNormal>$$\mathbf{S = \frac{1}{(1-P) + \frac{P}{N}}}$$</p>

<p class=MsoNormal>where:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>$(1-P)$ is the <b>sequential portion</b> of the program
     (cannot be parallelized)<sup>62</sup>.</li>
 <li class=MsoNormal>$P$ is the <b>parallelizable portion</b> of the program<sup>63</sup>.</li>
 <li class=MsoNormal>$N$ is the <b>number of processors</b><sup>64</sup>.</li>
</ul>

<p class=MsoNormal>The Maximum Speedup ($S_{max}$), achieved when the number of
processors approaches infinity ($N \rightarrow \infty$), is:</p>

<p class=MsoNormal>$$\mathbf{S_{max} = \frac{1}{1-P}}$$</p>

<p class=MsoNormal><sup>65</sup>.</p>

<p class=MsoNormal><b>Quick Quiz Numerical Example:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Problem:</b> Assume <b>1%</b> of the runtime is <b>not
     parallelizable</b> ($1-P = 0.01$)<sup>66</sup>. The program runs on $N=\mathbf{61}$
     cores<sup>67</sup>. What is the parallel speedup?</li>
 <li class=MsoNormal><b>Parameters:</b> Sequential portion $(1-P) = 0.01$.
     Parallelizable portion $P = 1 - 0.01 = 0.99$. Number of cores $N=61$.</li>
 <li class=MsoNormal>Calculation:</li>
</ul>

<p class=MsoNormal>$$S = \frac{1}{(1-P) + \frac{P}{N}} = \frac{1}{0.01 +
\frac{0.99}{61}}$$</p>

<p class=MsoNormal>$$S \approx \frac{1}{0.01 + 0.0162295}$$</p>

<p class=MsoNormal>$$S \approx \frac{1}{0.0262295} \approx \mathbf{38.125}$$</p>

<p class=MsoNormal><sup>68</sup>.</p>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>According to Amdahl's Law, what primarily limits the
     theoretical speedup of a parallelized program?</li>
</ol>

<p class=MsoNormal>a) The number of processors.</p>

<p class=MsoNormal>b) The parallelizable portion of the code.</p>

<p class=MsoNormal>c) The sequential portion of the code.</p>

<p class=MsoNormal>d) The memory bandwidth.</p>

<p class=MsoNormal>Answer: c) <sup>69</sup></p>

<p class=MsoNormal><b>Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Parallelization Strategy:</b> Amdahl's Law guides
     developers to focus optimization efforts on the <b>sequential part</b> of
     an application, as even a small sequential fraction drastically limits the
     maximum speedup achievable through parallelization.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 12: Summary for Execution on GPU (CUDA Abstraction)</b></p>

<p class=MsoNormal>Explanation:</p>

<p class=MsoNormal>This slide summarizes the execution model for a GPU,
primarily using the CUDA programming model terminology:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Grid of Blocks of Threads:</b> A GPU program (kernel)
     is launched as a <b>grid</b> of <b>thread blocks</b><sup>70</sup>.</li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Grids can be 1D or 2D. Blocks can be 1D, 2D, or 3D<sup>71</sup>.</li>
 </ul>
 <li class=MsoNormal><b>Blocks are Identical:</b> All blocks have the same
     structure and number of threads<sup>72</sup>.</li>
 <li class=MsoNormal><b>Execution Order is Undefined:</b> The order in which
     blocks execute is not guaranteed<sup>73</sup>.</li>
 <li class=MsoNormal><b>Threads in the Same Block:</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Can <b>synchronize</b> with each other.</li>
  <li class=MsoNormal>Can <b>share data fast</b> using <b>shared memory</b><sup>74</sup>.</li>
 </ul>
 <li class=MsoNormal><b>Threads from Different Blocks:</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal><b>Cannot cooperate</b> directly (cannot synchronize or
      share data easily)<sup>75</sup>.</li>
  <li class=MsoNormal>Communication between blocks must occur through <b>global
      memory</b><sup>76</sup>.</li>
 </ul>
 <li class=MsoNormal><b>IDs:</b> Threads and Blocks have unique <b>IDs</b> (1D,
     2D, or 3D) for data indexing<sup>77</sup>.</li>
 <li class=MsoNormal><b>No Block Migration:</b> Blocks execute on the same
     processor (SM) once scheduled, though multiple blocks may run on the same
     processor<sup>78</sup>.</li>
</ul>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>How do threads within the same block efficiently share
     data in GPU programming?</li>
</ol>

<p class=MsoNormal>a) Through Global Memory</p>

<p class=MsoNormal>b) Through L1 Cache</p>

<p class=MsoNormal>c) Through Shared Memory</p>

<p class=MsoNormal>d) They cannot share data</p>

<p class=MsoNormal>Answer: c) <sup>79</sup></p>

<p class=MsoNormal><b>Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Parallel Decomposition:</b> Developers use this model
     to divide a large problem (e.g., matrix multiplication) into independent
     &quot;blocks&quot; (tasks for the block of threads) and further decompose
     those into &quot;threads&quot; (individual work units).</li>
</ul>

<p class=MsoNormal><b>Numerical Example:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Problem:</b> A 2D problem requires a grid of $10 \times
     10$ blocks, with each block containing $32 \times 32$ threads. How many
     total threads are launched?</li>
 <li class=MsoNormal><b>Calculation:</b> Total Blocks = $10 \times 10 = 100$.
     Threads per Block = $32 \times 32 = 1024$.</li>
 <li class=MsoNormal>Total Threads = $100 \times 1024 = \mathbf{102,400}$.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 13: Matrix Multiplication (CPU Version)</b></p>

<p class=MsoNormal>Explanation:</p>

<p class=MsoNormal>This slide shows a standard CPU version of matrix
multiplication, $C = A \times B$80. It uses three nested loops (i, j, k) to
compute each element of the result matrix $C$: $C_{ij} =
\sum_{k=0}^{\text{WIDTH}-1} A_{ik} \times B_{kj}$81.</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>The outer loops iterate over the row ($i$) and column ($j$)
     of the result matrix $C$.</li>
 <li class=MsoNormal>The innermost loop ($k$) computes the dot product of the $i$-th
     row of $A$ and the $j$-th column of $B$.</li>
</ul>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>In the standard CPU matrix multiplication shown, the
     innermost loop iterates over which index?</li>
</ol>

<p class=MsoNormal>a) $i$ (row of C)</p>

<p class=MsoNormal>b) $j$ (column of C)</p>

<p class=MsoNormal>c) $k$ (inner sum index)</p>

<p class=MsoNormal>d) WIDTH</p>

<p class=MsoNormal>Answer: c) <sup>82</sup></p>

<p class=MsoNormal><b>Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Algorithm Baseline:</b> This sequential, triple-nested
     loop structure serves as the computational baseline against which parallel
     GPU versions (like the one in Slide 17) are compared for speedup.</li>
</ul>

<p class=MsoNormal><b>Numerical Example:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Problem:</b> How many floating-point multiplication and
     addition operations (FLOP) are required for $C = A \times B$ for $N \times
     N$ matrices?</li>
 <li class=MsoNormal><b>Calculation:</b> The $k$ loop runs $N$ times, with 1
     multiplication and 1 addition (2 FLOP) per iteration. The $i$ and $j$
     loops run $N$ times each.</li>
 <li class=MsoNormal>Total FLOP = $\mathbf{2N^3}$.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 14 &amp; 15: Programmer's View: Memory Model</b></p>

<p class=MsoNormal>Explanation:</p>

<p class=MsoNormal>These slides detail the Memory Model from a programmer's
perspective on a GPU (Device)83. GPU memory is structured in a hierarchy with
different scopes, access speeds, and caching policies.</p>

<table class=MsoNormalTable border=0 cellspacing=3 cellpadding=0>
 <thead>
  <tr>
   <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
   <p class=MsoNormal><b>Memory Location</b></p>
   </td>
   <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
   <p class=MsoNormal><b>Location</b></p>
   </td>
   <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
   <p class=MsoNormal><b>Cached</b></p>
   </td>
   <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
   <p class=MsoNormal><b>Access</b></p>
   </td>
   <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
   <p class=MsoNormal><b>Scope</b></p>
   </td>
  </tr>
 </thead>
 <tr>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal><b>Registers</b></p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>On-chip</p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Yes</p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>R/W</p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>per-thread <sup>84</sup></p>
  </td>
 </tr>
 <tr>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal><b>Local Memory</b></p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Off-chip</p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>No</p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>R/W</p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>per-thread <sup>85</sup></p>
  </td>
 </tr>
 <tr>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal><b>Shared Memory</b></p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal><b>On-chip</b></p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>N/A</p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal><b>R/W</b></p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'></td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal><b>all threads in a block</b> <sup>86</sup></p>
  </td>
 </tr>
 <tr>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal><b>Global Memory</b></p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal><b>Off-chip</b></p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>No</p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal><b>R/W</b></p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'></td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal><b>all threads + host</b> <sup>87</sup></p>
  </td>
 </tr>
 <tr>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal><b>Constant Memory</b></p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Off-chip</p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Yes</p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'></td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal><b>Read Only (RO)</b></p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>all threads + host <sup>88</sup></p>
  </td>
 </tr>
 <tr>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal><b>Texture Memory</b></p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Off-chip</p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Yes</p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'></td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal><b>Read Only (RO)</b></p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>all threads + host <sup>89</sup></p>
  </td>
 </tr>
</table>

<p class=MsoNormal>The <b>Host (CPU)</b> can $\text{R/W}$ Global, Constant, and
Texture memories<sup>90</sup>.</p>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Which type of memory is on-chip, R/W accessible, and has a
     scope limited to all threads in a single block?</li>
</ol>

<p class=MsoNormal>a) Local Memory</p>

<p class=MsoNormal>b) Global Memory</p>

<p class=MsoNormal>c) Shared Memory</p>

<p class=MsoNormal>d) Constant Memory</p>

<p class=MsoNormal>Answer: c) <sup>91</sup></p>

<p class=MsoNormal><b>Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Data Placement:</b> Developers strategically place data
     in the fastest available memory that meets the scope requirement. For
     example, frequently reused data is placed in <b>Shared Memory</b> for
     threads in a block, and read-only lookup tables are often placed in <b>Constant
     Memory</b> for fast, cached access by all threads.</li>
</ul>

<p class=MsoNormal><b>Numerical Example:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>(Conceptual slide, no direct numerical calculation.)</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 16: Sharing Data Between Threads (Shared Memory)</b></p>

<p class=MsoNormal>Explanation:</p>

<p class=MsoNormal>This slide focuses on Shared Memory, which is the mechanism
for threads within the same block to share data92.</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>It is an <b>extremely fast on-chip memory</b><sup>93</sup>.</li>
 <li class=MsoNormal>It is <b>user-managed</b>, declared using the __shared__
     keyword, and allocated <b>per block</b><sup>94</sup>.</li>
 <li class=MsoNormal>Data stored in Shared Memory is <b>not visible to threads
     in other blocks</b><sup>95</sup>.</li>
</ul>

<p class=MsoNormal>A key technique is to <b>cache data in shared memory</b> by:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Reading input elements from <b>Global Memory</b> to <b>Shared
     Memory</b><sup>96</sup>.</li>
 <li class=MsoNormal>Performing the computation (e.g., $\text{blockDim.x}$ output
     elements)<sup>97</sup>.</li>
 <li class=MsoNormal>Writing output elements back to <b>Global Memory</b><sup>98</sup>.</li>
</ol>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is the scope of data declared in __shared__ memory?</li>
</ol>

<p class=MsoNormal>a) All threads and the Host.</p>

<p class=MsoNormal>b) Only the thread that declared it.</p>

<p class=MsoNormal>c) Only threads within the same block.</p>

<p class=MsoNormal>d) Only global memory.</p>

<p class=MsoNormal>Answer: c) <sup>99</sup></p>

<p class=MsoNormal><b>Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Data Reuse:</b> Shared Memory is critical for
     optimizing algorithms like matrix multiplication or 2D convolution, where
     neighboring threads need access to the same input data, maximizing the <b>Arithmetic
     Intensity</b><sup>100</sup>.</li>
</ul>

<p class=MsoNormal><b>Numerical Example:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Problem:</b> If a block computes $\text{blockDim.x} =
     256$ output elements, and requires a halo of radius = 4 elements at each
     boundary. How many input elements must be read into shared memory?</li>
 <li class=MsoNormal><b>Formula:</b> $\text{Input Elements} = \text{blockDim.x}
     + 2 \times \text{radius}$<sup>101</sup>.</li>
 <li class=MsoNormal><b>Calculation:</b> $256 + 2 \times 4 = 256 + 8 =
     \mathbf{264}$ input elements.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 17: Matrix Multiplication (GPU Version)</b></p>

<p class=MsoNormal>Explanation:</p>

<p class=MsoNormal>This slide presents a basic, unoptimized GPU version
(kernel) of matrix multiplication102.</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>It is a __global__ function, meaning it runs on the
     device.</li>
 <li class=MsoNormal><b>Thread IDs</b> (blockIdx.y, threadIdx.y, etc.) are used
     to calculate the indices ($i, j$) so that <b>each thread computes a single
     element</b> of the product matrix $C$<sup>103</sup>.</li>
 <li class=MsoNormal>The computation involves an inner loop ($k$) that performs
     the dot product, reading $A[i][k]$ and $B[k][j]$ from <b>Global Memory
     (read)</b> repeatedly, and finally writing $C[i][j]$ to <b>Global Memory
     (write)</b><sup>104</sup>.</li>
</ul>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>In the basic GPU matrix multiplication kernel shown, what
     is the responsibility of each individual thread?</li>
</ol>

<p class=MsoNormal>a) To compute an entire row of C.</p>

<p class=MsoNormal>b) To compute an entire column of C.</p>

<p class=MsoNormal>c) To compute a single element of the product matrix C.</p>

<p class=MsoNormal>d) To load all data into shared memory.</p>

<p class=MsoNormal>Answer: c) <sup>105</sup></p>

<p class=MsoNormal><b>Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Simple Parallelism:</b> This demonstrates the most
     straightforward way to parallelize matrix multiplication, mapping each
     output element calculation to a separate thread, relying on the GPU's
     massive parallelism.</li>
</ul>

<p class=MsoNormal><b>Numerical Example:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>(Conceptual slide, the focus is on thread mapping.)</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 18: Kernel Analysis (Unoptimized Matrix
Multiplication)</b></p>

<p class=MsoNormal>Explanation:</p>

<p class=MsoNormal>This slide analyzes the performance of the unoptimized GPU
matrix multiplication kernel from the previous slide, specifically for a
floating-point data type (4 bytes)106.</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Data Transfer (per multiply-add):</b> 2 floating-point
     reads (for $A[i][k]$ and $B[k][j]$) $\times$ 4 bytes/float = <b>8 bytes</b><sup>107</sup>.</li>
 <li class=MsoNormal><b>Computation (per multiply-add):</b> <b>2 floating-point
     operations</b> (1 add, 1 multiply)<sup>108</sup>.</li>
 <li class=MsoNormal><b>Throughput (required bandwidth):</b> $\text{Bytes} /
     \text{FLOPs} = 8 \text{ B} / 2 \text{ FLOP} = \mathbf{4 \text{ B/FLOPs}}$<sup>109</sup>.</li>
</ul>

<p class=MsoNormal>The analysis concludes that:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Given a <b>Theoretical Peak</b> compute of <b>530 FLOPs</b>
     (example value for a Fermi multiprocessor)<sup>110</sup>.</li>
 <li class=MsoNormal>To achieve this peak, the <b>required bandwidth</b> would
     be: $4 \text{ B/FLOPs} \times 530 \text{ FLOPs} = \mathbf{2120 \text{
     GB/s}}$<sup>111</sup>.</li>
 <li class=MsoNormal>However, the <b>Actual Bandwidth</b> is only $\mathbf{177
     \text{ GB/s}}$ (example value)<sup>112</sup>.</li>
 <li class=MsoNormal>The performance yielded by this actual bandwidth is: $177
     \text{ GB/s} / 4 \text{ B/FLOPs} = \mathbf{44.25 \text{ FLOP/s}}$<sup>113</sup>.</li>
</ul>

<p class=MsoNormal>This result is about <b>12 times below peak performance</b>
($\approx 530 / 44.25$)<sup>114</sup>. The performance is <b>memory-bound</b>
because the required bandwidth greatly exceeds the actual bandwidth.</p>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>In the kernel analysis, the unoptimized matrix
     multiplication is determined to be memory-bound because:</li>
</ol>

<p class=MsoNormal>a) The actual FLOPs are too low.</p>

<p class=MsoNormal>b) The required memory bandwidth is much higher than the
actual bandwidth.</p>

<p class=MsoNormal>c) The calculation only uses single-precision floats.</p>

<p class=MsoNormal>d) The kernel is synchronous.</p>

<p class=MsoNormal>Answer: b) <sup>115</sup></p>

<p class=MsoNormal><b>Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Bottleneck Diagnosis:</b> This numerical analysis is a
     classic use case of the Roofline Model, proving that the application is
     constrained by the <b>memory bandwidth</b>, not the processor's raw
     compute power.</li>
</ul>

<p class=MsoNormal><b>Numerical Example:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Problem:</b> If the peak FLOPs were 1000 FLOPs and the
     throughput was 5 B/FLOPs, what is the required bandwidth?</li>
 <li class=MsoNormal><b>Calculation:</b> $\text{Required Bandwidth} =
     \text{Throughput} \times \text{Peak FLOPs} = 5 \text{ B/FLOPs} \times 1000
     \text{ FLOPs} = \mathbf{5000 \text{ GB/s}}$.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 19: How to Speed Up? (Blocking/Tiling)</b></p>

<p class=MsoNormal>Explanation:</p>

<p class=MsoNormal>Since the unoptimized kernel is memory-bound, this slide
suggests optimization strategies to improve performance116116. The main goal is
to promote data reuse to increase the Arithmetic Intensity117.</p>

<p class=MsoNormal>The primary technique is <b>BLOCKING</b> or <b>Tiling</b><sup>118118</sup>:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Load Data into Shared Memory and Reuse:</b> Load a
     block of data into the fast, on-chip <b>Shared Memory</b><sup>119</sup>.</li>
 <li class=MsoNormal><b>Partition Data:</b> Since Shared Memory is small, the
     data must be partitioned into <b>equal-sized blocks</b> (tiles) that fit
     into shared memory for reuse by the threads in a block<sup>120120</sup>.</li>
</ol>

<p class=MsoNormal>The overall strategy is to:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Consider minimal data movement.</li>
 <li class=MsoNormal>Exploit parallel memory access patterns.</li>
 <li class=MsoNormal>Identify a block/tile of global memory data to be accessed
     by threads<sup>121121</sup>.</li>
 <li class=MsoNormal>Get the multi-threads to use the data loaded into the fast
     memory (Shared, Register)<sup>122</sup>.</li>
</ul>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is the primary optimization technique proposed to
     speed up the memory-bound matrix multiplication?</li>
</ol>

<p class=MsoNormal>a) Using more global memory.</p>

<p class=MsoNormal>b) Reducing the total number of FLOPs.</p>

<p class=MsoNormal>c) Data Blocking/Tiling and using Shared Memory.</p>

<p class=MsoNormal>d) Switching from float to double precision.</p>

<p class=MsoNormal>Answer: c) <sup>123123123123</sup></p>

<p class=MsoNormal><b>Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Performance Optimization:</b> This is the core strategy
     used in high-performance GPU programming for matrix/tensor operations,
     drastically reducing the number of costly accesses to slow Global Memory.</li>
</ul>

<p class=MsoNormal><b>Numerical Example:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>(Conceptual slide, no direct numerical calculation.)</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 20: Matrix Multiplication (Tiling Visualization)</b></p>

<p class=MsoNormal>Explanation:</p>

<p class=MsoNormal>This slide provides a visual illustration of the Data
Blocking/Tiling concept for matrix multiplication124.</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Partial rows and columns</b> (blocks/tiles) of the
     input matrices are loaded into <b>Shared Memory</b><sup>125</sup>.</li>
 <li class=MsoNormal>Once a tile is loaded, a partial row/column can be <b>reused</b>
     to calculate multiple elements of the output matrix $C$<sup>126</sup>.</li>
 <li class=MsoNormal>For example, with a $16 \times 16$ tile, the number of <b>Global
     Memory loads are reduced by a factor of 16</b> (due to reuse)<sup>127</sup>.
     This dramatic reduction in communication is what increases the effective <b>Arithmetic
     Intensity</b>.</li>
</ul>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>In $16 \times 16$ data tiling for matrix multiplication,
     by what factor are the global memory loads reduced compared to the
     unoptimized version?</li>
</ol>

<p class=MsoNormal>a) 4</p>

<p class=MsoNormal>b) 8</p>

<p class=MsoNormal>c) 16</p>

<p class=MsoNormal>d) 32</p>

<p class=MsoNormal>Answer: c) <sup>128</sup></p>

<p class=MsoNormal><b>Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Arithmetic Intensity Improvement:</b> Tiling is a
     tangible method to move an application from the memory-bound region of the
     Roofline Model (Slide 9) towards the compute-bound region.</li>
</ul>

<p class=MsoNormal><b>Numerical Example:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Problem:</b> If the unoptimized version had a total of $512
     \text{ KB}$ of global memory loads, what would the loads be after applying
     a blocking factor of 16?</li>
 <li class=MsoNormal><b>Calculation:</b> $\text{New Loads} = \frac{512 \text{
     KB}}{16} = \mathbf{32 \text{ KB}}$.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 21 &amp; 22: Matrix Multiplication (Tiled Kernel
Code)</b></p>

<p class=MsoNormal>Explanation:</p>

<p class=MsoNormal>These slides show the structure of a tiled GPU matrix
multiplication kernel129.</p>

<p class=MsoNormal>The key implementation details are:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>The kernel uses two __shared__ arrays, A_S and B_S, to
     store the tiles of matrices $A$ and $B$, which are temporary and
     accessible to all threads in the block<sup>130</sup>.</li>
 <li class=MsoNormal>The loops are structured to process the matrices in tiles:</li>
 <ol style='margin-top:0in' start=1 type=1>
  <li class=MsoNormal>An outer loop iterates over the required blocks/tiles ($m$)<sup>131</sup>.</li>
  <li class=MsoNormal>Inside the outer loop, threads <b>collectively load</b> a
      tile of $A$ and a tile of $B$ from <b>Global Memory</b> into the <b>Shared
      Memory</b> arrays (A_S and B_S)<sup>132</sup>.</li>
  <li class=MsoNormal>A <b>__syncthreads()</b> call ensures all threads have
      finished loading the tile before proceeding<sup>133</sup>.</li>
  <li class=MsoNormal>An inner loop ($k$) computes the partial dot product
      using the data now available in fast <b>Shared Memory</b><sup>134</sup>.</li>
  <li class=MsoNormal>Another __syncthreads() is often used before the next
      tile iteration to ensure no thread reads a tile before the previous
      partial result is fully calculated.</li>
 </ol>
</ul>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>In the tiled matrix multiplication kernel, what is the
     purpose of the __syncthreads() function call after loading a tile into
     shared memory?</li>
</ol>

<p class=MsoNormal>a) To synchronize with the CPU.</p>

<p class=MsoNormal>b) To ensure all threads in the block have finished loading
the tile before computation begins.</p>

<p class=MsoNormal>c) To write the final result to global memory.</p>

<p class=MsoNormal>d) To signal the end of the kernel.</p>

<p class=MsoNormal>Answer: b) <sup>135</sup></p>

<p class=MsoNormal><b>Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Implementation Detail:</b> Provides the concrete code
     structure that realizes the data reuse strategy.</li>
</ul>

<p class=MsoNormal><b>Numerical Example:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>(Conceptual slide, the focus is on the code structure and
     synchronization.)</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 23: CPU/GPU Synchronization</b></p>

<p class=MsoNormal>Explanation:</p>

<p class=MsoNormal>This slide covers the need for synchronization between the
Host (CPU) and the Device (GPU)136.</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Asynchronous Calls:</b> The <b>CPU does not block</b>
     on typical cuda...() calls (like kernel launches or data copies). Instead,
     kernel/requests are <b>queued</b> and processed in order, and control
     returns to the CPU immediately<sup>137</sup>. This allows the CPU to
     continue with other work.</li>
 <li class=MsoNormal><b>The Need for Sync:</b> Eventually, the <b>CPU must know
     when the GPU is done</b> so it can safely copy the results back from the
     GPU memory<sup>138</sup>.</li>
 <li class=MsoNormal><b>Synchronization Function:</b> The function <b>cudaThreadSynchronize()</b>
     is used to <b>block the CPU</b> until <b>all preceding</b> CUDA calls and
     kernel requests have completed<sup>139</sup>.</li>
</ul>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is the main effect of calling
     cudaThreadSynchronize()?</li>
</ol>

<p class=MsoNormal>a) It makes the GPU wait for the CPU.</p>

<p class=MsoNormal>b) It allows the CPU to continue working immediately.</p>

<p class=MsoNormal>c) It is only used for data transfer.</p>

<p class=MsoNormal>d) It blocks the CPU until all previous kernel launches and
CUDA calls are finished.</p>

<p class=MsoNormal>Answer: d) <sup>140</sup></p>

<p class=MsoNormal><b>Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Correctness and I/O:</b> Crucial for ensuring the CPU
     does not attempt to read the result of a GPU calculation before the GPU
     has completed it, preventing race conditions and ensuring correctness.</li>
</ul>

<p class=MsoNormal><b>Numerical Example:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>(Conceptual slide, focusing on execution flow and
     control.)</li>
</ul>

<div style='border:none;border-bottom:double windowtext 2.25pt;padding:0in 0in 1.0pt 0in'>

<p class=MsoNormal style='border:none;padding:0in'>&nbsp;</p>

</div>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal><b>Slide 2: Unit of CPU Performance Measurement: FLOPs</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Explanation:</b>&nbsp;This slide introduces how we
     measure a computer's &quot;number crunching&quot; power. FLOPs stands
     for&nbsp;<b>Floating-point Operations Per Second</b>. A floating-point
     operation is a calculation with decimal numbers (like 5.6), such as
     addition, subtraction, multiplication, or division. The more FLOPs a CPU
     can handle, the faster it can perform complex mathematical tasks. It also
     reminds us that double-precision numbers (like&nbsp;double&nbsp;in C++)
     use more bits than single-precision (float), making them more accurate but
     also more demanding to process.</li>
 <li class=MsoNormal><b>MCQ:</b>&nbsp;What does FLOPs stand for?<br>
     a) Fast Logical Operations per Second<br>
     b) Floating-point Operations per Second <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     c) Frequent Loop Operations per Second<br>
     d) File Load Operations per Second</li>
 <li class=MsoNormal><b>Use Case:</b>&nbsp;Comparing the performance of
     different processors for scientific simulations, weather forecasting, or
     AI model training, all of which involve massive amounts of decimal
     calculations.</li>
 <li class=MsoNormal><b>Numerical:</b>&nbsp;If a processor has a rating of 1
     GFLOP/s, how many floating-point operations can it perform in 10 seconds?</li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal><b>Answer:</b>&nbsp;1 GFLOP/s = 1 billion operations per
      second. In 10 seconds, it would perform 10 billion operations.</li>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 3: Understanding FLOPs with an Example</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Explanation:</b>&nbsp;This slide makes FLOPs concrete
     with a code example. The function&nbsp;mul_add&nbsp;performs one
     multiplication (a * x) and one addition (+ b) inside a loop that runs 50
     times. That's 2 operations per loop iteration, totaling 100 operations for
     the entire function. If this function takes 1 second to run, the CPU's
     performance for this task is 100 FLOPs.</li>
 <li class=MsoNormal><b>MCQ:</b>&nbsp;In the&nbsp;mul_add&nbsp;function, how
     many total floating-point operations are performed?<br>
     a) 50<br>
     b) 100 <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     c) 150<br>
     d) 200</li>
 <li class=MsoNormal><b>Use Case:</b>&nbsp;Profiling a specific function in
     your code to understand its computational cost and identify bottlenecks.</li>
 <li class=MsoNormal><b>Numerical:</b>&nbsp;If you modify the loop to run 75
     times, and the function now takes 1.5 seconds, what is the FLOPs rating?</li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal><b>Answer:</b>&nbsp;Total FLOPs = 75 iterations * 2
      FLOPs/iteration = 150 FLOPs. FLOPs rating = 150 FLOPs / 1.5 seconds = 100
      FLOPs.</li>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 4: FLOPs: Computation &amp; Communication (The
Memory Wall)</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Explanation:</b>&nbsp;This slide highlights a critical
     problem in computing: the &quot;Memory Wall.&quot; While CPUs can compute
     very fast (high FLOPs), getting data from the main memory (DRAM) is very
     slow in comparison. The slide shows that accessing data from L1 cache
     takes about 4 CPU cycles, but from DRAM it can take ~200 cycles. The key
     to performance is to minimize data movement by doing as many calculations
     as possible on the data you've already loaded into the fast caches. This
     concept is called&nbsp;<b>Arithmetic Intensity (AI)</b>.</li>
 <li class=MsoNormal><b>MCQ:</b>&nbsp;Why is it important to perform many
     operations on a piece of data once it's loaded from memory?<br>
     a) To make the code more complex.<br>
     b) Because memory access is slow compared to CPU speed. <span
     style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     c) To use more electricity.<br>
     d) Because the CPU cache is larger than main memory.</li>
 <li class=MsoNormal><b>Use Case:</b>&nbsp;Optimizing a video processing
     filter. Instead of reading each pixel from memory multiple times, you load
     a block of pixels into the cache and perform all necessary adjustments
     (brightness, contrast, sharpening) at once.</li>
 <li class=MsoNormal><b>Numerical:</b>&nbsp;If a CPU cycle is 0.5 nanoseconds,
     how long does it take to access data from DRAM if it costs 200 cycles?</li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal><b>Answer:</b>&nbsp;Time = 200 cycles * 0.5 ns/cycle =
      100 nanoseconds.</li>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 5: Arithmetic Intensity (AI) - The Concept</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Explanation:</b>&nbsp;This slide formally defines&nbsp;<b>Arithmetic
     Intensity (AI)</b>. AI is a metric that measures the balance between
     computation and data movement in a program. It's calculated as the number
     of arithmetic operations (FLOPs) divided by the amount of data transferred
     to/from memory (in bytes). A high AI means you are doing a lot of
     calculations for every byte of data moved, which is ideal for GPUs because
     they have immense computing power but limited memory bandwidth.</li>
 <li class=MsoNormal><b>MCQ:</b>&nbsp;A high Arithmetic Intensity (AI) is
     desirable because:<br>
     a) It reduces the number of lines of code.<br>
     b) It better utilizes the high compute power of processors like GPUs. <span
     style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     c) It makes the program easier to debug.<br>
     d) It uses less CPU cache.</li>
 <li class=MsoNormal><b>Use Case:</b>&nbsp;Matrix multiplication has high AI
     and runs well on GPUs. Vector addition has low AI and is often limited by
     memory speed, not CPU power.</li>
 <li class=MsoNormal><b>Numerical:</b>&nbsp;If an algorithm performs 1,000
     FLOPs and needs to transfer 100 bytes of data, what is its AI?</li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal><b>Answer:</b>&nbsp;AI = 1000 FLOPs / 100 bytes = 10
      FLOPs/byte.</li>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slides 6 &amp; 7: Calculating AI for a CPU Function</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Explanation:</b>&nbsp;These slides show how to
     calculate AI for a real function,&nbsp;arraymul_add. This function
     multiplies each element of array&nbsp;x&nbsp;by a constant&nbsp;a&nbsp;and
     adds it to array&nbsp;y. For each element, it does 2 FLOPs (one multiply,
     one add). For data transfer, it reads&nbsp;x[i]&nbsp;(8 bytes),
     reads&nbsp;y[i]&nbsp;(8 bytes), and writes&nbsp;y[i]&nbsp;(8 bytes),
     totaling 24 bytes per iteration. For&nbsp;n&nbsp;elements, total FLOPs
     =&nbsp;2n, total bytes =&nbsp;24n. Therefore, AI = (2n)/(24n) = 1/12.</li>
 <li class=MsoNormal><b>MCQ:</b>&nbsp;In the&nbsp;arraymul_add&nbsp;function,
     how many bytes are transferred from/to memory for each element processed?<br>
     a) 8 bytes<br>
     b) 16 bytes<br>
     c) 24 bytes <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     d) 32 bytes</li>
 <li class=MsoNormal><b>Use Case:</b>&nbsp;This is a classic pattern in
     scientific computing known as AXPY (A*X Plus Y). Analyzing its AI helps
     decide if it's suitable for a GPU or if it will be bottlenecked by memory
     speed.</li>
 <li class=MsoNormal><b>Numerical:</b>&nbsp;If the array size&nbsp;n&nbsp;is
     1000, what are the total FLOPs and total bytes transferred?</li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal><b>Answer:</b>&nbsp;Total FLOPs = 2 * 1000 = 2000. Total
      Bytes = 24 * 1000 = 24,000 bytes.</li>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 8: AI Calculation Homework (Matrix Multiplication)</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Explanation:</b>&nbsp;This slide assigns the AI
     calculation for a matrix multiplication function (dgemm). This is a more
     complex, triple-nested loop. The key is to count the FLOPs and memory
     accesses for the inner loop and scale by the number of iterations.</li>
 <li class=MsoNormal><b>MCQ:</b>&nbsp;For the given&nbsp;dgemm&nbsp;function,
     the Arithmetic Intensity is proportional to:<br>
     a) n<br>
     b) n²<br>
     c) n/16 <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     d) 16/n</li>
 <li class=MsoNormal><b>Use Case:</b>&nbsp;Matrix multiplication is the core
     operation in deep learning and many engineering simulations. Understanding
     its AI is crucial for performance optimization on GPUs.</li>
 <li class=MsoNormal><b>Numerical (Simplified):</b>&nbsp;Focus on the inner
     loop&nbsp;sum = sum + A[i*n+k] * B[k*n+j]. How many FLOPs and memory
     accesses (reads) happen here?</li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal><b>Answer:</b>&nbsp;2 FLOPs (multiply and add). 2 memory
      reads (one for&nbsp;A[...], one for&nbsp;B[...]). (Note: The full
      solution, as per the link, accounts for caching and is more complex).</li>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 9: The Roofline Performance Model</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Explanation:</b>&nbsp;This slide introduces the&nbsp;<b>Roofline
     Model</b>, a visual way to understand a program's performance. It plots
     performance (GFLOP/s) against Arithmetic Intensity (AI). The model shows
     that for low-AI applications, performance is limited by memory bandwidth
     (the slanted &quot;roof&quot;). For high-AI applications, performance is
     limited by the processor's peak computational power (the flat
     &quot;roof&quot;). Your goal is to get your application's performance as
     close to the &quot;roof&quot; as possible.</li>
 <li class=MsoNormal><b>MCQ:</b>&nbsp;According to the Roofline Model, an
     application with low Arithmetic Intensity is typically limited by:<br>
     a) CPU clock speed.<br>
     b) Memory bandwidth. <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     c) The size of the hard disk.<br>
     d) The number of CPU cores.</li>
 <li class=MsoNormal><b>Use Case:</b>&nbsp;A data-copying task (low AI) will be
     on the memory-bound slope, while a complex physics simulation (high AI)
     can hit the compute-bound ceiling.</li>
 <li class=MsoNormal><b>Numerical:</b>&nbsp;If a CPU has a peak memory
     bandwidth of 50 GB/s and a peak compute of 100 GFLOP/s, what is the
     minimum AI required to be compute-bound?</li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal><b>Answer:</b>&nbsp;The &quot;ridge point&quot; is where
      AI * Bandwidth = Compute. So, AI = Compute / Bandwidth = 100 GFLOP/s / 50
      GB/s = 2 FLOPs/byte. An AI greater than 2 is needed to be compute-bound.</li>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slides 10 &amp; 11: Amdahl's Law</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Explanation:</b>&nbsp;<b>Amdahl's Law</b>&nbsp;defines
     the maximum speedup you can get by parallelizing a program. It states that
     speedup is limited by the sequential (non-parallelizable) part of your
     code. Even if you make the parallel part infinitely fast, you're still
     stuck with the sequential part. The formula is Speedup = 1 / [(1 - P) +
     (P/N)], where P is the parallel fraction and N is the number of
     processors.</li>
 <li class=MsoNormal><b>MCQ:</b>&nbsp;According to Amdahl's Law, if 10% of a
     program is sequential (P=0.9), what is the maximum possible speedup
     regardless of how many processors you use?<br>
     a) 5x<br>
     b) 10x <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     c) 20x<br>
     d) 100x</li>
 <li class=MsoNormal><b>Use Case:</b>&nbsp;Deciding whether to buy more cores
     for a server. If your web server application is 95% parallelizable,
     Amdahl's Law shows that after ~40 cores, the performance gains become
     minimal.</li>
 <li class=MsoNormal><b>Numerical (from Slide 11):</b>&nbsp;A program is 99%
     parallelizable (P=0.99) and runs on 61 cores (N=61). What is the speedup?</li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal><b>Answer:</b><br>
      Sequential portion = 1 - P = 0.01.<br>
      Parallel portion per core = P/N = 0.99 / 61 &#8776; 0.01623.<br>
      Total time on 61 cores = 0.01 + 0.01623 = 0.02623.<br>
      Speedup = 1 / 0.02623 &#8776; 38.125. <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span></li>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 12: Summary of GPU Execution Model</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Explanation:</b>&nbsp;This slide summarizes the key
     hierarchy in GPU programming:&nbsp;<b>Grids &#8594; Blocks &#8594; Threads</b>.
     A kernel launch creates a grid of thread blocks. Threads within the same
     block can cooperate efficiently using fast shared memory and can
     synchronize. Threads from different blocks cannot cooperate directly and
     must use slower global memory. This structure allows GPUs to manage
     thousands of threads efficiently.</li>
 <li class=MsoNormal><b>MCQ:</b>&nbsp;Threads within the same GPU block can
     share data using:<br>
     a) Global Memory<br>
     b) Shared Memory <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     c) Constant Memory<br>
     d) Texture Memory</li>
 <li class=MsoNormal><b>Use Case:</b>&nbsp;Applying a filter to an image. You
     can assign one block to process each tile of the image. Threads within the
     block can load the tile into shared memory and work on it together very
     quickly.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slides 13, 17, 18, 19, 20, 22: Matrix Multiplication on
GPU</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Explanation:</b>&nbsp;This series of slides shows the
     evolution of matrix multiplication from a slow CPU version to an optimized
     GPU version.</li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal><b>Slide 13:</b>&nbsp;The naive CPU version has poor
      cache usage because it traverses matrices in a non-sequential way.</li>
  <li class=MsoNormal><b>Slide 17:</b>&nbsp;A naive GPU version. Each thread
      calculates one element of the result matrix C by reading an entire row
      from A and an entire column from B from slow global memory. This is
      inefficient.</li>
  <li class=MsoNormal><b>Slide 18:</b>&nbsp;Analysis shows this naive GPU
      kernel is severely limited by memory bandwidth, achieving only a fraction
      of the GPU's peak performance.</li>
  <li class=MsoNormal><b>Slides 19 &amp; 20:</b>&nbsp;The solution is&nbsp;<b>Blocking/Tiling</b>.
      Instead of reading individual elements, a whole block of threads loads a
      small &quot;tile&quot; from matrices A and B into fast&nbsp;<b>shared
      memory</b>. All threads in the block can then reuse this data multiple
      times, drastically reducing global memory accesses.</li>
  <li class=MsoNormal><b>Slide 22:</b>&nbsp;The code for the tiled matrix
      multiplication. The outer&nbsp;for&nbsp;loop moves through the tiles, and
      the inner loops compute the product using the data in shared
      memory.&nbsp;__syncthreads()&nbsp;ensures all threads have finished
      loading data before anyone starts computing.</li>
 </ul>
 <li class=MsoNormal><b>MCQ:</b>&nbsp;What is the primary purpose of
     &quot;tiling&quot; in GPU matrix multiplication?<br>
     a) To make the code more readable.<br>
     b) To reduce expensive global memory accesses by using fast shared memory.
     <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     c) To use more threads.<br>
     d) To make the matrices square.</li>
 <li class=MsoNormal><b>Use Case:</b>&nbsp;This is the fundamental optimization
     behind high-performance linear algebra libraries (like cuBLAS) and is
     critical for deep learning frameworks (like TensorFlow and PyTorch) to
     train models quickly.</li>
 <li class=MsoNormal><b>Numerical:</b>&nbsp;If the tile width is 16, by what
     factor are global memory accesses reduced in the tiled version compared to
     the naive version?</li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal><b>Answer:</b>&nbsp;By a factor of 16 (as stated on Slide
      20). In the naive version, each element is read O(n) times. In the tiled
      version, each element is loaded only once per tile, leading to O(n) /
      O(TILE_WIDTH) reduction.</li>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slides 14 &amp; 15: GPU Memory Model</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Explanation:</b>&nbsp;These slides detail the different
     types of memory available on a GPU. It's a hierarchy of speed and scope:</li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal><b>Registers &amp; Local Memory:</b>&nbsp;Private to each
      thread. Very fast.</li>
  <li class=MsoNormal><b>Shared Memory:</b>&nbsp;Shared by all threads in a
      block. Very fast, on-chip. Used for cooperation.</li>
  <li class=MsoNormal><b>Global, Constant, Texture Memory:</b>&nbsp;Accessible
      by all threads and the host (CPU). Slow, off-chip DRAM. Constant and
      Texture are cached and read-only.</li>
 </ul>
 <li class=MsoNormal><b>MCQ:</b>&nbsp;Which GPU memory is on-chip and managed
     by the programmer for sharing data within a block?<br>
     a) Global Memory<br>
     b) Local Memory<br>
     c) Shared Memory <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     d) Constant Memory</li>
 <li class=MsoNormal><b>Use Case:</b>&nbsp;Using shared memory to hold a
     portion of an image for a filter (like a blur), or using constant memory
     for parameters that stay the same for all threads (e.g., the value of
     &#960; in a simulation).</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 16: Sharing Data with Shared Memory and Halos</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Explanation:</b>&nbsp;This slide explains a common
     pattern when using shared memory: the&nbsp;<b>halo</b>&nbsp;or&nbsp;<b>ghost
     cell</b>&nbsp;region. When a block loads a chunk of data, it often needs
     to read extra elements from the boundaries of neighboring chunks to
     perform its calculation correctly. For example, a 1D blur filter needs
     pixels from the left and right of its main block. The halo represents
     these extra, overlapping data points.</li>
 <li class=MsoNormal><b>MCQ:</b>&nbsp;In the context of shared memory, what is
     a &quot;halo&quot;?<br>
     a) A video game.<br>
     b) Extra elements loaded from neighboring data chunks to ensure correct
     computation. <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     c) The glow on a monitor.<br>
     d) A type of synchronization function.</li>
 <li class=MsoNormal><b>Use Case:</b>&nbsp;Image convolution (blur, edge
     detection), where calculating a pixel's new value requires its neighbors.
     Stencil codes in scientific computing also use this pattern.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 23: CPU/GPU Synchronization</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Explanation:</b>&nbsp;This slide explains that when the
     CPU launches a kernel on the GPU, it doesn't wait for it to finish. The
     GPU works independently, and the CPU continues with its next instructions.
     This is called&nbsp;<b>asynchronous</b>&nbsp;execution. If the CPU needs
     the GPU's results, it must explicitly call&nbsp;cudaThreadSynchronize()&nbsp;(or
     a modern equivalent) to wait for the GPU to finish all its work.</li>
 <li class=MsoNormal><b>MCQ:</b>&nbsp;When the CPU launches a CUDA kernel, it
     typically:<br>
     a) Stops and waits for the kernel to finish.<br>
     b) Continues executing the next lines of code immediately. <span
     style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     c) Shuts down the GPU.<br>
     d) Copies all data back from GPU memory automatically.</li>
 <li class=MsoNormal><b>Use Case:</b>&nbsp;Overlapping CPU and GPU work. While
     the GPU is processing one batch of data, the CPU can be preparing the next
     batch, leading to better overall performance</li>
</ul>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<div style='border:none;border-bottom:double windowtext 2.25pt;padding:0in 0in 1.0pt 0in'>

<p class=MsoNormal style='border:none;padding:0in'>&nbsp;</p>

</div>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal><b>GPU Programming Lecture 05 - Comprehensive Guide</b></p>

<p class=MsoNormal>I'll break down each slide with explanations, MCQs, use
cases, and numerical problems.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 2: Floating-Point Representation</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal>Computers store decimal numbers using a special format
called floating-point. Think of it like scientific notation (e.g., 3.2 ×
10&#8309;). Single precision uses 32 bits (1 for sign, 8 for exponent, 23 for
fraction), while double precision uses 64 bits (1 for sign, 11 for exponent, 52
for fraction) for more accuracy.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>How many bits are used for the exponent in double
     precision?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) 8</li>
  <li class=MsoNormal>b) 11 &#10003;</li>
  <li class=MsoNormal>c) 23</li>
  <li class=MsoNormal>d) 52</li>
 </ul>
 <li class=MsoNormal><b>What does the first bit represent in floating-point
     representation?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Exponent</li>
  <li class=MsoNormal>b) Mantissa</li>
  <li class=MsoNormal>c) Sign &#10003;</li>
  <li class=MsoNormal>d) Precision</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Scientific simulations requiring high precision (weather
     forecasting)</li>
 <li class=MsoNormal>Financial calculations where accuracy matters</li>
 <li class=MsoNormal>3D graphics rendering</li>
</ul>

<p class=MsoNormal><b>Numerical Problems</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Convert 5.6 to binary as shown in the slide</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: 101.100110011001100110011...</li>
  <li class=MsoNormal>Process: 5 = 101&#8322;, 0.6 × 2 = 1.2 (1), 0.2 × 2 = 0.4
      (0), 0.4 × 2 = 0.8 (0), etc.</li>
 </ul>
 <li class=MsoNormal><b>How many more bits does double precision have compared
     to single precision?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: 64 - 32 = 32 bits more</li>
 </ul>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 3: CPU Performance Model (FLOPS)</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal>FLOPS (Floating Point Operations Per Second) measures how
fast a computer performs mathematical calculations. In the example, the loop
runs 50 times, each time doing 2 operations (multiply and add), giving 100
total operations. If this takes 1 second, the CPU runs at 100 FLOPS.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>What does FLOPS stand for?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Floating Length Operations Per Second</li>
  <li class=MsoNormal>b) Floating Point Operations Per Second &#10003;</li>
  <li class=MsoNormal>c) Fast Linear Operations Per Second</li>
  <li class=MsoNormal>d) Field Length Operations Per Second</li>
 </ul>
 <li class=MsoNormal><b>In the given example, how many FLOP does one call to
     mul_add perform?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) 50</li>
  <li class=MsoNormal>b) 100 &#10003;</li>
  <li class=MsoNormal>c) 150</li>
  <li class=MsoNormal>d) 200</li>
 </ul>
 <li class=MsoNormal><b>How many floating-point operations are in the line: x =
     a * x + b?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) 1</li>
  <li class=MsoNormal>b) 2 &#10003;</li>
  <li class=MsoNormal>c) 3</li>
  <li class=MsoNormal>d) 4</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Benchmarking CPUs for scientific computing</li>
 <li class=MsoNormal>Comparing performance of different processors</li>
 <li class=MsoNormal>Estimating how long algorithms will take to run</li>
</ul>

<p class=MsoNormal><b>Numerical Problems</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>If the loop runs 100 times instead of 50, what is the
     total FLOP?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: 100 iterations × 2 operations = 200 FLOP</li>
 </ul>
 <li class=MsoNormal><b>If mul_add takes 0.5 seconds and performs 100 FLOP,
     what is the FLOPS rate?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: 100 FLOP / 0.5 seconds = 200 FLOPS</li>
 </ul>
 <li class=MsoNormal><b>A function performs x = a + b + c (2 ops) in a loop
     1000 times and takes 2 seconds. Calculate FLOPS.</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: (1000 × 2) / 2 = 1000 FLOPS</li>
 </ul>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 4-5: Arithmetic Intensity &amp; Memory Bandwidth</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal>Memory operations are much slower than computations. It's
like having a super-fast chef (CPU) but a slow delivery service (memory).
Arithmetic Intensity measures how many calculations you do with each piece of
data loaded from memory. Higher AI means better efficiency.</p>

<p class=MsoNormal><b>Memory Bandwidth</b> = How fast data moves between memory
and processor, measured in GB/s.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>What does Arithmetic Intensity measure?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Speed of memory</li>
  <li class=MsoNormal>b) Operations per byte transferred &#10003;</li>
  <li class=MsoNormal>c) Total memory size</li>
  <li class=MsoNormal>d) CPU clock speed</li>
 </ul>
 <li class=MsoNormal><b>Why is Arithmetic Intensity important?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Memory operations are faster than computation</li>
  <li class=MsoNormal>b) Memory operations are slower than computation &#10003;</li>
  <li class=MsoNormal>c) It measures disk speed</li>
  <li class=MsoNormal>d) It counts the number of threads</li>
 </ul>
 <li class=MsoNormal><b>How is bandwidth calculated?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) (bytes read) / time</li>
  <li class=MsoNormal>b) (bytes written) / time</li>
  <li class=MsoNormal>c) (bytes read + bytes written) / time &#10003;</li>
  <li class=MsoNormal>d) (bytes read - bytes written) / time</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Optimizing matrix operations</li>
 <li class=MsoNormal>Designing efficient neural network algorithms</li>
 <li class=MsoNormal>GPU kernel optimization</li>
</ul>

<p class=MsoNormal><b>Numerical Problems</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>An application reads 500 MB and writes 300 MB in 2
     seconds. Calculate bandwidth.</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: [(500 + 300) MB] / 2 seconds = 400 MB/s = 0.4
      GB/s</li>
 </ul>
 <li class=MsoNormal><b>If bandwidth is 10 GB/s and you transfer 50 GB, how
     long does it take?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: 50 GB / 10 GB/s = 5 seconds</li>
 </ul>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 6-7: Calculating Arithmetic Intensity</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal>For the arraymul_add function:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Operations</b>: For each element, we do a * x[i]
     (multiply) + y[i] (add) = 2 operations per element</li>
 <li class=MsoNormal><b>Memory traffic</b>: Load x[i] (8 bytes), load y[i] (8
     bytes), store y[i] (8 bytes) = 24 bytes per element</li>
 <li class=MsoNormal><b>AI</b> = 2 operations / 24 bytes = 1/12 &#8776; 0.083
     ops/byte</li>
</ul>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>How many operations are performed per iteration in
     arraymul_add?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) 1</li>
  <li class=MsoNormal>b) 2 &#10003;</li>
  <li class=MsoNormal>c) 3</li>
  <li class=MsoNormal>d) 4</li>
 </ul>
 <li class=MsoNormal><b>How many bytes are transferred per iteration (for
     double precision)?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) 8</li>
  <li class=MsoNormal>b) 16</li>
  <li class=MsoNormal>c) 24 &#10003;</li>
  <li class=MsoNormal>d) 32</li>
 </ul>
 <li class=MsoNormal><b>What is the Arithmetic Intensity formula?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Data transfer / Computations</li>
  <li class=MsoNormal>b) Computations / Data transfer &#10003;</li>
  <li class=MsoNormal>c) Bandwidth × Time</li>
  <li class=MsoNormal>d) FLOPS / Memory</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Identifying memory-bound vs compute-bound algorithms</li>
 <li class=MsoNormal>Optimizing code for better cache utilization</li>
 <li class=MsoNormal>Selecting appropriate hardware for specific workloads</li>
</ul>

<p class=MsoNormal><b>Numerical Problems</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>For arraymul_add with n=1000, calculate total
     operations.</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: 2 operations × 1000 = 2000 operations</li>
 </ul>
 <li class=MsoNormal><b>For arraymul_add with n=1000, calculate total memory
     traffic.</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: 24 bytes × 1000 = 24,000 bytes = 24 KB</li>
 </ul>
 <li class=MsoNormal><b>Calculate AI for arraymul_add.</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: 2n / 24n = 2/24 = 1/12 &#8776; 0.083 ops/byte</li>
 </ul>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 8: Homework - Matrix Multiplication (DGEMM)</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal>This is dense matrix multiplication C = A × B. The triple
nested loop means:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Operations</b>: For each element C[i][j], we do n
     multiplications and n additions = 2n operations</li>
 <li class=MsoNormal>Total elements in C = n × n</li>
 <li class=MsoNormal><b>Total operations</b> = n² × 2n = 2n³</li>
 <li class=MsoNormal><b>Memory traffic</b>: Read entire A (n² elements), entire
     B (n² elements), write entire C (n² elements) = 3n² × 8 bytes</li>
 <li class=MsoNormal><b>AI</b> = 2n³ / (24n²) = n/12</li>
</ul>

<p class=MsoNormal>The slide says n/16, which might assume some optimization or
different counting.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>How many nested loops are in dgemm?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) 2</li>
  <li class=MsoNormal>b) 3 &#10003;</li>
  <li class=MsoNormal>c) 4</li>
  <li class=MsoNormal>d) 1</li>
 </ul>
 <li class=MsoNormal><b>For an n×n matrix multiplication, how many
     multiplications are performed?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) n²</li>
  <li class=MsoNormal>b) n³ &#10003;</li>
  <li class=MsoNormal>c) 2n³</li>
  <li class=MsoNormal>d) n</li>
 </ul>
 <li class=MsoNormal><b>What does DGEMM stand for?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Double General Matrix Multiply &#10003;</li>
  <li class=MsoNormal>b) Dense Grid Element Method</li>
  <li class=MsoNormal>c) Double Graphics Execution Model</li>
  <li class=MsoNormal>d) Direct General Memory Management</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Deep learning (neural network training)</li>
 <li class=MsoNormal>Scientific simulations</li>
 <li class=MsoNormal>Image processing</li>
 <li class=MsoNormal>Linear algebra libraries (BLAS, cuBLAS)</li>
</ul>

<p class=MsoNormal><b>Numerical Problems</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>For 3×3 matrix multiplication, how many multiplication
     operations?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: 3³ = 27 multiplications</li>
 </ul>
 <li class=MsoNormal><b>For n=10 matrix multiplication, calculate total FLOPs.</b>
     </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: 2n³ = 2 × 1000 = 2000 FLOP</li>
 </ul>
 <li class=MsoNormal><b>For n=100 matrices (double precision), calculate memory
     traffic.</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: 3 × n² × 8 = 3 × 10,000 × 8 = 240,000 bytes = 240
      KB</li>
 </ul>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 9: CPU Performance Model (Roofline)</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal>This graph shows the relationship between Arithmetic
Intensity (x-axis) and Performance (y-axis). There are two limits:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Memory bandwidth limit</b> (diagonal line): When AI is
     low, you're limited by how fast you can move data</li>
 <li class=MsoNormal><b>Peak performance limit</b> (horizontal line): When AI
     is high, you're limited by computation speed</li>
</ol>

<p class=MsoNormal>The &quot;roofline&quot; shape shows the maximum achievable
performance for any AI value.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>What does the diagonal line represent?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Peak compute performance</li>
  <li class=MsoNormal>b) Memory bandwidth limit &#10003;</li>
  <li class=MsoNormal>c) Cache size</li>
  <li class=MsoNormal>d) Thread count</li>
 </ul>
 <li class=MsoNormal><b>What does the horizontal line represent?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Memory bandwidth</li>
  <li class=MsoNormal>b) Peak floating-point performance &#10003;</li>
  <li class=MsoNormal>c) Minimum performance</li>
  <li class=MsoNormal>d) Average performance</li>
 </ul>
 <li class=MsoNormal><b>Where should you aim to operate for best performance?</b>
     </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Low AI region</li>
  <li class=MsoNormal>b) Near the &quot;roof&quot; intersection &#10003;</li>
  <li class=MsoNormal>c) Below the diagonal</li>
  <li class=MsoNormal>d) Far left of the graph</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Performance analysis of algorithms</li>
 <li class=MsoNormal>Identifying bottlenecks (memory vs compute)</li>
 <li class=MsoNormal>Hardware procurement decisions</li>
</ul>

<p class=MsoNormal><b>Numerical Problems</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>If peak performance is 100 GFLOPS and bandwidth is 50
     GB/s, at what AI does the roofline &quot;break&quot;?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: AI = 100 GFLOPS / 50 GB/s = 2 ops/byte</li>
 </ul>
 <li class=MsoNormal><b>An algorithm has AI = 0.5 ops/byte and bandwidth = 20
     GB/s. What's the maximum achievable performance?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: 0.5 × 20 = 10 GFLOPS (memory-bound)</li>
 </ul>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 10-11: Amdahl's Law</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal>Amdahl's Law says: if only part of your program can be
parallelized, the sequential part limits your speedup. It's like a restaurant:
even with 100 chefs, if there's one checkout counter, that's your bottleneck.</p>

<p class=MsoNormal><b>Formula</b>: Speedup = 1 / [(1 - P) + P/N]</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>P = parallelizable fraction</li>
 <li class=MsoNormal>N = number of processors</li>
</ul>

<p class=MsoNormal><b>Quiz answer</b>: P = 0.99, N = 61 Speedup = 1 / [(0.01) +
(0.99/61)] = 1 / [0.01 + 0.0162] = 1 / 0.0262 &#8776; 38.125</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>What does Amdahl's Law describe?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Memory bandwidth</li>
  <li class=MsoNormal>b) Theoretical speedup from parallelization &#10003;</li>
  <li class=MsoNormal>c) Cache hit rate</li>
  <li class=MsoNormal>d) Power consumption</li>
 </ul>
 <li class=MsoNormal><b>What limits the speedup in Amdahl's Law?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Number of cores</li>
  <li class=MsoNormal>b) Sequential portion of code &#10003;</li>
  <li class=MsoNormal>c) Memory size</li>
  <li class=MsoNormal>d) Clock speed</li>
 </ul>
 <li class=MsoNormal><b>If 100% of code is parallelizable, what's the
     theoretical speedup with infinite cores?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) 2x</li>
  <li class=MsoNormal>b) 100x</li>
  <li class=MsoNormal>c) Infinite &#10003;</li>
  <li class=MsoNormal>d) No speedup</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Estimating benefits of parallelization</li>
 <li class=MsoNormal>Deciding if parallel optimization is worthwhile</li>
 <li class=MsoNormal>Understanding performance ceilings</li>
</ul>

<p class=MsoNormal><b>Numerical Problems</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>50% parallelizable, 4 cores. Calculate speedup.</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: 1 / [(0.5) + (0.5/4)] = 1 / [0.5 + 0.125] =
      1/0.625 = 1.6x</li>
 </ul>
 <li class=MsoNormal><b>90% parallelizable, 10 cores. Calculate speedup.</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: 1 / [(0.1) + (0.9/10)] = 1 / [0.1 + 0.09] =
      1/0.19 &#8776; 5.26x</li>
 </ul>
 <li class=MsoNormal><b>If you need 50x speedup and have 100 cores, what % must
     be parallelizable?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: 50 = 1/[(1-P) + P/100]</li>
  <li class=MsoNormal>Solving: (1-P) = 0.01, so P = 0.99 or 99%</li>
 </ul>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 12: Summary for GPU Execution</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal>GPUs organize work hierarchically:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Grid</b>: The entire problem</li>
 <li class=MsoNormal><b>Blocks</b>: Independent groups of threads (can execute
     in any order)</li>
 <li class=MsoNormal><b>Threads</b>: Individual workers doing computation</li>
</ul>

<p class=MsoNormal>This structure allows massive parallelism (thousands of
threads running simultaneously).</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>What is the largest unit of organization in GPU
     execution?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Thread</li>
  <li class=MsoNormal>b) Block</li>
  <li class=MsoNormal>c) Grid &#10003;</li>
  <li class=MsoNormal>d) Warp</li>
 </ul>
 <li class=MsoNormal><b>Can blocks communicate directly with each other?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Yes, always</li>
  <li class=MsoNormal>b) No, they're independent &#10003;</li>
  <li class=MsoNormal>c) Only through shared memory</li>
  <li class=MsoNormal>d) Only in the same warp</li>
 </ul>
 <li class=MsoNormal><b>What executes in parallel on a GPU?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Only blocks</li>
  <li class=MsoNormal>b) Only threads</li>
  <li class=MsoNormal>c) Both blocks and threads &#10003;</li>
  <li class=MsoNormal>d) Neither</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Image processing (each pixel = thread)</li>
 <li class=MsoNormal>Vector operations</li>
 <li class=MsoNormal>Monte Carlo simulations</li>
</ul>

<p class=MsoNormal><b>Numerical Problems</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Grid has 100 blocks, each with 256 threads. Total
     threads?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: 100 × 256 = 25,600 threads</li>
 </ul>
 <li class=MsoNormal><b>You need to process 1,000,000 elements with 1024
     threads per block. How many blocks?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: <span style='font-family:"Cambria Math",serif'>&#8968;</span>1,000,000
      / 1024<span style='font-family:"Cambria Math",serif'>&#8969;</span> = 977
      blocks</li>
 </ul>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 14-15: GPU Memory Hierarchy</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal>GPUs have multiple memory types, ordered by speed (fastest
to slowest):</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Registers</b>: Per-thread, fastest</li>
 <li class=MsoNormal><b>Shared memory</b>: Per-block, very fast (on-chip)</li>
 <li class=MsoNormal><b>Local memory</b>: Per-thread, slower</li>
 <li class=MsoNormal><b>Global memory</b>: All threads can access, slowest but
     largest</li>
 <li class=MsoNormal><b>Constant/Texture</b>: Read-only, cached</li>
</ol>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Which memory is fastest?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Global</li>
  <li class=MsoNormal>b) Shared</li>
  <li class=MsoNormal>c) Registers &#10003;</li>
  <li class=MsoNormal>d) Constant</li>
 </ul>
 <li class=MsoNormal><b>Which memory can threads in different blocks share?</b>
     </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Shared memory</li>
  <li class=MsoNormal>b) Registers</li>
  <li class=MsoNormal>c) Global memory &#10003;</li>
  <li class=MsoNormal>d) Local memory</li>
 </ul>
 <li class=MsoNormal><b>What keyword declares shared memory in CUDA?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) <b>global</b></li>
  <li class=MsoNormal>b) <b>shared</b> &#10003;</li>
  <li class=MsoNormal>c) <b>device</b></li>
  <li class=MsoNormal>d) <b>constant</b></li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Optimizing data access patterns</li>
 <li class=MsoNormal>Reducing global memory traffic</li>
 <li class=MsoNormal>Cooperative algorithms within blocks</li>
</ul>

<p class=MsoNormal><b>Numerical Problems</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>A block has 256 threads, each uses 32 registers. Total
     registers for block?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: 256 × 32 = 8,192 registers</li>
 </ul>
 <li class=MsoNormal><b>Shared memory is 48 KB per block. Using 16 KB, how much
     remains?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: 48 - 16 = 32 KB</li>
 </ul>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 16: Sharing Data Between Threads</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal><b>Shared memory</b> is like a whiteboard visible to all
threads in a block. It's:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Very fast (on-chip)</li>
 <li class=MsoNormal>User-managed (you control what goes there)</li>
 <li class=MsoNormal>Limited in size (~48 KB)</li>
 <li class=MsoNormal>Not visible to other blocks</li>
</ul>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Where is shared memory located?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Off-chip DRAM</li>
  <li class=MsoNormal>b) On-chip &#10003;</li>
  <li class=MsoNormal>c) CPU memory</li>
  <li class=MsoNormal>d) Hard disk</li>
 </ul>
 <li class=MsoNormal><b>Is shared memory visible to threads in different
     blocks?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Yes</li>
  <li class=MsoNormal>b) No &#10003;</li>
  <li class=MsoNormal>c) Sometimes</li>
  <li class=MsoNormal>d) Only if they're in the same grid</li>
 </ul>
 <li class=MsoNormal><b>How do you declare shared memory in CUDA?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) shared int data[];</li>
  <li class=MsoNormal>b) <b>shared</b> int data[]; &#10003;</li>
  <li class=MsoNormal>c) int <b>shared</b> data[];</li>
  <li class=MsoNormal>d) global shared int data[];</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Tile-based matrix multiplication</li>
 <li class=MsoNormal>Parallel reduction operations</li>
 <li class=MsoNormal>Data sharing in block-level algorithms</li>
</ul>

<p class=MsoNormal><b>Numerical Problems</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Block uses 16 KB shared memory for 256 threads. How
     much per thread on average?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: 16,384 bytes / 256 = 64 bytes per thread</li>
 </ul>
 <li class=MsoNormal><b>Need to store 1024 floats (4 bytes each) in shared
     memory. Total size?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: 1024 × 4 = 4,096 bytes = 4 KB</li>
 </ul>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slides 17-22: Matrix Multiplication with Tiling/Blocking</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal><b>Naive approach</b>: Each thread loads data from global
memory many times (slow).</p>

<p class=MsoNormal><b>Optimized approach (Tiling)</b>:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Divide matrices into small tiles that fit in shared memory</li>
 <li class=MsoNormal>Load tile into shared memory once</li>
 <li class=MsoNormal>All threads in the block use that tile multiple times</li>
 <li class=MsoNormal>Reduces global memory accesses dramatically</li>
</ol>

<p class=MsoNormal><b>Example</b>: For 1024×1024 matrices with 16×16 tiles:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Without tiling: Each element needs 1024 loads</li>
 <li class=MsoNormal>With tiling: Need only 1024/16 = 64 tile loads, reused 16
     times each</li>
</ul>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>What is the main benefit of tiling?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Uses less computation</li>
  <li class=MsoNormal>b) Reduces global memory accesses &#10003;</li>
  <li class=MsoNormal>c) Increases thread count</li>
  <li class=MsoNormal>d) Simplifies code</li>
 </ul>
 <li class=MsoNormal><b>Where is the tile data stored?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Global memory</li>
  <li class=MsoNormal>b) Registers</li>
  <li class=MsoNormal>c) Shared memory &#10003;</li>
  <li class=MsoNormal>d) Constant memory</li>
 </ul>
 <li class=MsoNormal><b>Why is the tile size limited?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Thread limit</li>
  <li class=MsoNormal>b) Shared memory size limit &#10003;</li>
  <li class=MsoNormal>c) Register limit</li>
  <li class=MsoNormal>d) Global memory speed</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Deep learning frameworks (TensorFlow, PyTorch)</li>
 <li class=MsoNormal>Scientific computing libraries</li>
 <li class=MsoNormal>Image convolution operations</li>
</ul>

<p class=MsoNormal><b>Numerical Problems</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>1024×1024 matrix, 16×16 tiles. How many tiles per row?</b>
     </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: 1024 / 16 = 64 tiles</li>
 </ul>
 <li class=MsoNormal><b>Each tile element accessed 16 times. Without shared
     memory, element accessed from global memory 16 times. With shared memory,
     how many global accesses per element?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: 1 (loaded once into shared memory)</li>
 </ul>
 <li class=MsoNormal><b>Tile size 32×32, double precision. Memory needed?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: 32 × 32 × 8 bytes = 8,192 bytes = 8 KB</li>
 </ul>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 23: CPU/GPU Synchronization</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal>Since GPU and CPU work independently, we need
synchronization points:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>__syncthreads()</b>: All threads in a block wait at
     this point before continuing (ensures data is ready in shared memory)</li>
 <li class=MsoNormal><b>cudaDeviceSynchronize()</b>: CPU waits for all GPU work
     to finish</li>
</ul>

<p class=MsoNormal>Think of it like a group meeting point where everyone must
arrive before continuing together.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>What does __syncthreads() do?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Synchronizes CPU and GPU</li>
  <li class=MsoNormal>b) Synchronizes threads within a block &#10003;</li>
  <li class=MsoNormal>c) Synchronizes all blocks</li>
  <li class=MsoNormal>d) Stops the program</li>
 </ul>
 <li class=MsoNormal><b>When should you use __syncthreads()?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) After every operation</li>
  <li class=MsoNormal>b) When threads need to share data via shared memory
      &#10003;</li>
  <li class=MsoNormal>c) Never</li>
  <li class=MsoNormal>d) Only at the end of kernel</li>
 </ul>
 <li class=MsoNormal><b>What happens without proper synchronization?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Race conditions/incorrect results &#10003;</li>
  <li class=MsoNormal>b) Program runs faster</li>
  <li class=MsoNormal>c) Nothing</li>
  <li class=MsoNormal>d) Compilation error</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Ensuring shared memory writes complete before reads</li>
 <li class=MsoNormal>Barrier synchronization in parallel algorithms</li>
 <li class=MsoNormal>Coordinating multi-stage computations</li>
</ul>

<p class=MsoNormal><b>Numerical Problems</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>256 threads in block. How many reach __syncthreads()
     before any continue?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: All 256 threads must reach it</li>
 </ul>
 <li class=MsoNormal><b>If 1 thread takes 100ms and 255 threads take 10ms to
     reach __syncthreads(), how long before block continues?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Answer: 100ms (everyone waits for the slowest)</li>
 </ul>
</ol>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<div style='border:none;border-bottom:double windowtext 2.25pt;padding:0in 0in 1.0pt 0in'>

<p class=MsoNormal style='border:none;padding:0in'>&nbsp;</p>

</div>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal><b>Page 1: Title Slide - GPU Programming Lecture 05</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>This is the opening slide of the lecture. It simply
introduces the topic as &quot;GPU Programming&quot; and specifies it's Lecture
05. It's like a welcome sign for the session, setting the stage for discussions
on how to program Graphics Processing Units (GPUs) for high-performance
computing, especially for tasks like matrix operations.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is the main topic of this lecture series? a) Web
     development b) GPU Programming c) Database management d) Mobile app design
     <b>Answer: b) GPU Programming</b></li>
 <li class=MsoNormal>Which lecture number is this? a) Lecture 01 b) Lecture 03
     c) Lecture 05 d) Lecture 10 <b>Answer: c) Lecture 05</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Academic Courses</b>: Used in university computer
     science classes to outline the syllabus for parallel computing modules.</li>
 <li class=MsoNormal><b>Conference Presentations</b>: Serves as a title card in
     workshops on high-performance computing (HPC) at events like SC
     (Supercomputing Conference).</li>
 <li class=MsoNormal><b>Online Tutorials</b>: Headers like this appear in video
     series on platforms like Coursera or YouTube for GPU programming intros.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<p class=MsoNormal>No numerical computations on this slide, as it's
introductory. For practice: If a lecture series has 10 sessions and this is the
5th, what fraction of the course is covered? <b>Solution</b>: Fraction = 5/10 =
0.5 (or 50%). <b>Explanation</b>: Divide the current lecture number by total
lectures to get the progress ratio. Simple division: 5 ÷ 10 = 0.5.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Page 2: Unit of CPU Performance Measurement: FLOPs</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>This slide explains how we measure a CPU's speed using
&quot;FLOPs,&quot; which stands for Floating-Point Operations Per Second. It
compares single-precision (like 32-bit floats with 1 sign bit, 8 exponent bits,
23 mantissa bits) to double-precision (64-bit with 1 sign, 11 exponent, 52
mantissa bits). It shows an example of converting 5.6 to binary
(101.100110011001100110011) and breaks down the math. Homework is to review
basic floating-point math like addition and subtraction.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What does FLOPs stand for? a) Fixed-Length Operations Per
     Second b) Floating-Point Operations Per Second c) File Operations Per
     Second d) Function Loop Operations Per Second <b>Answer: b) Floating-Point
     Operations Per Second</b></li>
 <li class=MsoNormal>In double-precision floating-point, how many bits are for
     the mantissa? a) 23 bits b) 52 bits c) 8 bits d) 11 bits <b>Answer: b) 52
     bits</b></li>
 <li class=MsoNormal>What is the binary representation example given for 5.6?
     a) 101.100110011001100110011 b) 010.110011001100110011001 c)
     111.000000000000000000000 d) 001.011001100110011001100 <b>Answer: a)
     101.100110011001100110011</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Scientific Simulations</b>: Measuring FLOPs in climate
     modeling software to benchmark CPU efficiency for decimal-heavy
     calculations.</li>
 <li class=MsoNormal><b>Game Development</b>: Evaluating CPU performance in
     engines like Unity for physics simulations involving floating-point math.</li>
 <li class=MsoNormal><b>AI Training</b>: Assessing CPU baselines before
     switching to GPUs for neural network floating-point ops.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Convert 3.75 to binary floating-point (single-precision
     style). Assume normalized form. <b>Solution</b>: 3.75 = 11.11 in binary =
     1.11 × 2^1. Binary: 0 01111111 11000000000000000000000 (sign=0,
     exponent=127 biased for 1, mantissa=110... padded). <b>Explanation</b>:
     Integer part 3=11b, fractional 0.75=0.11b. Normalize: Shift left once
     (exponent +1=2, biased 127+1=128=10000000b, but adjust for IEEE). Use
     formula: Sign (0), Exponent (bias 127 + actual), Mantissa (fractional
     after 1.). Step-by-step: Binary fraction multiply by 2: 0.75×2=1.5 (1),
     0.5×2=1.0 (1), rest 0s.</li>
 <li class=MsoNormal>If a CPU performs 10^9 FLOPs and takes 2 seconds for a
     task, what's the FLOP rate? <b>Solution</b>: 10^9 / 2 = 5 × 10^8
     FLOPs/second. <b>Explanation</b>: Divide total operations by time.
     FLOPs/second = Total FLOPs ÷ Time (s).</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Page 3: CPU Performance Model (FLOPS)</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>FLOPS measures how many math operations (+, *, etc.) a
computer does per second on an algorithm. The slide shows a simple C function
mul_add that loops 50 times, doing a multiply and add each time (2 FLOPs per
loop = 100 FLOPs total). If it runs in 1 second, the CPU speed is 100 FLOPs.
It's a basic way to count &quot;work&quot; in floating-point math.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>In the mul_add function, how many FLOPs occur per loop
     iteration? a) 1 b) 2 c) 50 d) 100 <b>Answer: b) 2</b> (one * and one +)</li>
 <li class=MsoNormal>If mul_add takes 0.5 seconds for 100 FLOPs, what's the CPU
     performance? a) 50 FLOPs b) 100 FLOPs c) 200 FLOPs d) 500 FLOPs <b>Answer:
     c) 200 FLOPs</b> (100 / 0.5 = 200)</li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Benchmarking Tools</b>: Used in software like LINPACK
     to test supercomputer speeds for TOP500 rankings.</li>
 <li class=MsoNormal><b>Embedded Systems</b>: Measuring FLOPs in automotive
     ECUs for real-time signal processing in self-driving cars.</li>
 <li class=MsoNormal><b>Video Encoding</b>: Counting FLOPs in FFmpeg for
     optimizing CPU usage in media players.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Modify the loop to run 100 times. How many total FLOPs? If
     it takes 1.5 seconds, what's the rate? <b>Solution</b>: Total FLOPs = 100
     loops × 2 = 200. Rate = 200 / 1.5 &#8776; 133.33 FLOPs/second. <b>Explanation</b>:
     Multiply iterations by operations per iteration for total. Divide by time
     for rate. Use basic arithmetic: 100 × 2 = 200; 200 ÷ 1.5 = 133.333...</li>
 <li class=MsoNormal>A function does 4 FLOPs per iteration over 25 iterations.
     Time: 0.25s. FLOP rate? <b>Solution</b>: Total = 25 × 4 = 100 FLOPs. Rate
     = 100 / 0.25 = 400 FLOPs/second. <b>Explanation</b>: Total FLOPs =
     iterations × FLOPs/iteration. Rate = total / time.</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Page 4: FLOPs: Computation &amp; Communication</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>This slide shows a diagram of CPU layers (cores, caches,
DRAM) and their speeds (FLOPs for compute, GB/s for memory). Memory access is
slow (e.g., DRAM takes ~200 cycles), so do as many calculations as possible on
loaded data before fetching more. It introduces &quot;Arithmetic
Intensity&quot; (AI) as a way to measure ops per byte moved, to balance compute
and data transfer.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Why are memory operations slow compared to computation? a)
     They take fewer CPU cycles b) They take many CPU cycles (e.g., 200 for
     DRAM) c) They don't use FLOPs d) They only work on registers <b>Answer: b)
     They take many CPU cycles (e.g., 200 for DRAM)</b></li>
 <li class=MsoNormal>What metric measures operations per byte transferred? a)
     FLOPs b) Bandwidth c) Arithmetic Intensity (AI) d) Cache Miss Rate <b>Answer:
     c) Arithmetic Intensity (AI)</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>High-Performance Computing</b>: Optimizing matrix ops
     in weather forecasting to minimize DRAM accesses.</li>
 <li class=MsoNormal><b>Machine Learning</b>: In TensorFlow, using AI to fuse
     operations and reduce data movement in training loops.</li>
 <li class=MsoNormal><b>Graphics Rendering</b>: Balancing compute (shading) and
     texture fetches in game engines like Unreal.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>If L1 cache access takes 4 cycles and compute is 1 cycle
     per FLOP, what's the AI break-even for 1 byte? <b>Solution</b>: AI =
     cycles_compute / cycles_memory = 1 / (4/1) wait, but simplified: To match,
     need 4 FLOPs per byte to hide latency. <b>Explanation</b>: AI threshold =
     memory latency / compute time per op. For 4-cycle latency and
     1-cycle/FLOP, AI &#8805; 4 FLOPs/byte to overlap.</li>
 <li class=MsoNormal>A system has peak 100 GFLOPs/s and 50 GB/s bandwidth. Min
     AI for peak compute? <b>Solution</b>: AI = 100 / 50 = 2 FLOPs/byte. <b>Explanation</b>:
     Peak FLOPs/s ÷ Peak GB/s = FLOPs/byte. Convert units: GFLOPs = 10^9 FLOPs,
     GB = 10^9 bytes.</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Page 5: Arithmetic Intensity</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>Memory bandwidth (GB/s) measures data transfer speed—total
bytes read/written divided by time (in billions). It's key for app performance.
The slide notes GPU bandwidth limits (e.g., 144 GB/s global) and how high AI
(many ops per access) helps. Example: For Fermi GPU, single-precision needs
1000/36 &#8776; 28 ops per byte to hit peak.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>How is bandwidth calculated? a) (Bytes read + written) /
     10^9 / time b) FLOPs / time c) Cores × cycles d) AI × GB/s <b>Answer: a)
     (Bytes read + written) / 10^9 / time</b></li>
 <li class=MsoNormal>For Fermi SP, how many arithmetic ops needed per byte for
     peak throughput? a) 10 b) 28 c) 144 d) 1 <b>Answer: b) 28</b> (1000/36
     &#8776; 27.78)</li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Data Analytics</b>: Measuring bandwidth in Spark for
     big data queries on Hadoop clusters.</li>
 <li class=MsoNormal><b>Scientific Viz</b>: In ParaView, optimizing bandwidth
     for loading large simulation datasets.</li>
 <li class=MsoNormal><b>Cloud Computing</b>: AWS EC2 instances use this to rate
     GPU vs. CPU for bandwidth-bound tasks like video transcoding.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Task reads 2 GB, writes 1 GB, takes 10 seconds. Bandwidth?
     <b>Solution</b>: (2 + 1) × 10^9 bytes / 10^9 / 10 = 0.3 GB/s. <b>Explanation</b>:
     Total bytes = read + write. Divide by 10^9 for GB, then by time (s). (3 ×
     10^9) / 10^9 = 3 GB; 3 / 10 = 0.3 GB/s.</li>
 <li class=MsoNormal>Peak 1 TFLOPs/s, bandwidth 200 GB/s. AI for peak? <b>Solution</b>:
     1 × 10^12 / (200 × 10^9) = 5 FLOPs/byte. <b>Explanation</b>: FLOPs/s ÷
     (GB/s × 10^9 bytes/GB) = FLOPs/byte.</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Page 6: Calculation of Arithmetic Intensity (CPUs)</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>Using the arraymul_add function (y[i] = a * x[i] + y[i] for
n elements), AI = total computations / total data transfer. Here, 2 FLOPs per
iteration ( * and + ), data: read x (8 bytes), read y (8), write y (8) = 24
bytes total. So AI = 2n FLOPs / 24n bytes = 1/12 FLOPs/byte. size_t is just an
unsigned int for loop indices.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>In arraymul_add, how many FLOPs per iteration? a) 1 b) 2
     c) 3 d) 24 <b>Answer: b) 2</b></li>
 <li class=MsoNormal>What is AI for this function? a) n/24 b) 2n/24n = 1/12
     FLOPs/byte c) 24/n d) 8 bytes/FLOP <b>Answer: b) 2n/24n = 1/12 FLOPs/byte</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Image Processing</b>: Applying filters (like blur)
     where each pixel op reads neighbors, calculating AI to optimize cache use.</li>
 <li class=MsoNormal><b>Financial Modeling</b>: Vector additions in Monte Carlo
     simulations for stock pricing.</li>
 <li class=MsoNormal><b>Audio Effects</b>: Real-time mixing in DAWs like
     Ableton, balancing ops and buffer loads.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>For n=1000, total FLOPs? Total bytes? AI? <b>Solution</b>:
     FLOPs = 2×1000=2000. Bytes=24×1000=24000. AI=2000/24000=1/12 &#8776;0.083
     FLOPs/byte. <b>Explanation</b>: Per iteration: FLOPs=2, bytes=24. Scale by
     n, then divide.</li>
 <li class=MsoNormal>If n=500, bandwidth=10 GB/s, what's max FLOPs/s
     achievable? <b>Solution</b>: Bytes/s =10×10^9. Total bytes=24×500=12000.
     But rate: AI=1/12, so FLOPs/s = AI × bandwidth = (1/12) × 10×10^9 &#8776;
     8.33×10^8 FLOPs/s. <b>Explanation</b>: Achievable FLOPs/s = AI × Peak GB/s
     × 10^9.</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Page 7: Calculation of Arithmetic Intensity (CPUs)</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>Continuing the example: Traffic = (2 loads × 8 bytes) + (1
store × 8 bytes) × n = 24n bytes. Computations = 2n FLOPs. AI = 2n / 24n = 1/12
FLOPs/byte. It's about counting reads/writes precisely to see data movement
cost.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Total memory traffic for n iterations? a) 8n bytes b) 16n
     bytes c) 24n bytes d) 2n bytes <b>Answer: c) 24n bytes</b></li>
 <li class=MsoNormal>Simplified AI formula? a) 24n / 2n b) 2n / 24n = 1/12 c) n
     / 8 d) 1 / 24n <b>Answer: b) 2n / 24n = 1/12</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Compiler Optimization</b>: GCC uses similar calcs to
     inline functions and reduce loads in loops.</li>
 <li class=MsoNormal><b>Embedded IoT</b>: Calculating AI for sensor data fusion
     in wearables to save battery.</li>
 <li class=MsoNormal><b>Web Servers</b>: Optimizing array ops in Node.js for
     JSON parsing.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>n=2000. Traffic? AI? <b>Solution</b>:
     Traffic=24×2000=48000 bytes. AI= (2×2000)/48000 = 4000/48000=1/12. <b>Explanation</b>:
     Traffic = 24n. AI = 2n / 24n = 2/24=1/12 (independent of n).</li>
 <li class=MsoNormal>If AI=1/12, peak bandwidth=50 GB/s, peak compute=100
     GFLOPs/s. Is it compute or memory bound? <b>Solution</b>: Memory limit =
     (1/12) × 50 × 10^9 &#8776; 4.17×10^8 FLOPs/s &lt; 100×10^9, so memory
     bound. <b>Explanation</b>: Compute achievable = AI × BW × 10^9. Compare to
     peak FLOPs.</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Page 8: Home-work</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>Homework: For DGEMM (double-precision matrix multiply C = A
× B), calculate AI = n/16 FLOPs/byte. Code is a triple loop: outer i,j; inner k
sum += A[i<i>n+k] * B[k</i>n+j]. Solution link provided for Roofline model
explanation.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is the AI for DGEMM? a) n/8 b) n/16 c) 16/n d) 2n <b>Answer:
     b) n/16</b></li>
 <li class=MsoNormal>In DGEMM, how many nested loops? a) 1 b) 2 c) 3 d) 4 <b>Answer:
     c) 3</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Linear Algebra Libraries</b>: BLAS routines like DGEMM
     in NumPy for scientific computing.</li>
 <li class=MsoNormal><b>Graphics</b>: Matrix transforms in OpenGL for 3D
     rendering pipelines.</li>
 <li class=MsoNormal><b>ML Frameworks</b>: GEMM kernels in PyTorch for weight
     updates.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>For n=64 in DGEMM, total FLOPs? (Each inner: 2 FLOPs × n) <b>Solution</b>:
     Outer loops n×n, inner n: Total FLOPs = 2 n^3 = 2×64^3 = 524288.
     AI=64/16=4 FLOPs/byte. <b>Explanation</b>: FLOPs = 2 × n × n × n (mul+add
     per k). AI given as n/16.</li>
 <li class=MsoNormal>Bytes transferred? (Rough: 3 matrices × n^2 × 8 bytes, but
     reuse) For AI=n/16, bytes = FLOPs / AI = 524288 / 4 = 131072 bytes. <b>Explanation</b>:
     Bytes = total FLOPs / AI.</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Page 9: CPU Performance Model</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>Diagram of Roofline model: Performance (GFLOPs/s) = min(peak
compute, AI × peak bandwidth). Any loop does compute (FLOPs) and communication
(DRAM moves). With perfect overlap, runtime limited by the slower one. AI in
FLOPs/byte; line shows knee where memory bounds.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Performance formula? a) max(peak FLOPs, AI × BW) b)
     min(peak FLOPs, AI × peak GB/s) c) AI / peak d) BW × FLOPs <b>Answer: b)
     min(peak FLOPs, AI × peak GB/s)</b></li>
 <li class=MsoNormal>What bounds performance below the knee? a) Compute b)
     Memory (DRAM) c) Cache d) Registers <b>Answer: b) Memory (DRAM)</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Performance Tuning</b>: Intel VTune uses Roofline to
     profile apps like CFD simulations.</li>
 <li class=MsoNormal><b>Hardware Design</b>: Architects use it to size caches
     in new CPUs.</li>
 <li class=MsoNormal><b>Energy Efficiency</b>: In mobile chips, to balance
     power for AI inference.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Peak 200 GFLOPs/s, BW 100 GB/s, AI=1 FLOPs/byte.
     Performance? <b>Solution</b>: AI×BW=1×100=100 GFLOPs/s &lt; 200, so 100
     GFLOPs/s. <b>Explanation</b>: Min(peak compute, AI × BW). Units: GFLOPs/s
     = (FLOPs/byte) × (GB/s).</li>
 <li class=MsoNormal>Knee point AI? (Where lines meet) <b>Solution</b>: AI_knee
     = peak GFLOPs/s / peak GB/s = 200/100=2 FLOPs/byte. <b>Explanation</b>:
     Set peak FLOPs = AI × BW, solve for AI.</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Page 10: Amdahl's Law</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>Amdahl's Law says speedup from parallelizing is limited by
the sequential (non-parallel) part. Formula: S = 1 / [(1-P) + P/N], where
P=parallel fraction, N=processors. Max speedup (N&#8594;&#8734;) = 1/(1-P).
E.g., if 5% sequential, max speedup=20x even with infinite cores.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What limits speedup in Amdahl's Law? a) Number of
     processors b) Sequential portion (1-P) c) Parallel ops only d) Cache size <b>Answer:
     b) Sequential portion (1-P)</b></li>
 <li class=MsoNormal>Max speedup formula? a) N / P b) 1 / (1-P) c) P / (1-P) d)
     1 / P <b>Answer: b) 1 / (1-P)</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Parallel Software Design</b>: Deciding where to
     parallelize in OpenMP for loops in simulations.</li>
 <li class=MsoNormal><b>Cloud Scaling</b>: AWS auto-scaling groups use it to
     predict gains from adding instances.</li>
 <li class=MsoNormal><b>Chip Design</b>: NVIDIA evaluates core counts for GPUs.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>P=0.9 (90% parallel), N=10. Speedup? <b>Solution</b>: S =
     1 / (0.1 + 0.9/10) = 1 / (0.1 + 0.09) = 1/0.19 &#8776; 5.26x. <b>Explanation</b>:
     Plug into formula: Sequential time fraction (1-P), parallel (P/N). Total
     time = sum, S=1/total.</li>
 <li class=MsoNormal>If max S=50x, what's P? <b>Solution</b>: 1/(1-P)=50
     &#8594; 1-P=1/50=0.02 &#8594; P=0.98 (98%). <b>Explanation</b>: S_max =
     1/(1-P), solve: 1-P = 1/S_max, P=1 - 1/S_max.</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Page 11: Quick Quiz</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>Quiz: 1% runtime sequential (P=0.99), N=61 cores. Speedup?
Answer: 38.125x. It shows even small sequential parts bottleneck massive
parallelism—no overheads assumed.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Sequential fraction here? a) 1% (0.01) b) 99% (0.99) c)
     61% d) 38% <b>Answer: a) 1% (0.01)</b></li>
 <li class=MsoNormal>Calculated speedup? a) 61x b) 38.125x c) 1x d) 99x <b>Answer:
     b) 38.125x</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Classroom Testing</b>: Interactive quizzes in edX
     courses on parallel computing.</li>
 <li class=MsoNormal><b>Performance Audits</b>: Teams use it to audit code
     parallelization in HPC jobs.</li>
 <li class=MsoNormal><b>Hardware Benchmarks</b>: Intel Phi (mentioned) testing
     for co-processor viability.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>P=0.95, N=20. Speedup? <b>Solution</b>: S=1/(0.05 +
     0.95/20)=1/(0.05+0.0475)=1/0.0975&#8776;10.26x. <b>Explanation</b>: Same
     as Amdahl: 1 / [(1-P) + P/N].</li>
 <li class=MsoNormal>For 0.5% sequential, N=100. Speedup? <b>Solution</b>:
     S=1/(0.005 + 0.995/100)=1/(0.005+0.00995)&#8776;1/0.01495&#8776;66.89x. <b>Explanation</b>:
     Compute fractions, sum for normalized time, invert for S.</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Page 12: Summary for Execution on GPU</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>GPU execution basics: Grid of 1D/2D/3D blocks, each with
1D/2D/3D threads—all identical structure. Block order undefined; same-block
threads sync/share via shared memory; different blocks via global. Blocks don't
migrate, multiple on one processor. Simplifies indexing.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is a grid composed of? a) Single thread b) Blocks of
     threads c) Caches only d) Registers <b>Answer: b) Blocks of threads</b></li>
 <li class=MsoNormal>How do threads in different blocks communicate? a) Shared
     memory b) Global memory c) Local only d) No communication <b>Answer: b)
     Global memory</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>CUDA Kernels</b>: Launching grids in NVIDIA CUDA for
     image convolution.</li>
 <li class=MsoNormal><b>GPGPU</b>: General-purpose computing on GPUs for
     molecular dynamics sims.</li>
 <li class=MsoNormal><b>Deep Learning</b>: Tensor cores in blocks for batched
     matrix multiplies in cuDNN.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Grid: 4 blocks × 256 threads/block. Total threads? <b>Solution</b>:
     4 × 256 = 1024 threads. <b>Explanation</b>: Total = blocks ×
     threads/block. Simple multiplication.</li>
 <li class=MsoNormal>If 2D block (16×16), total threads/block? <b>Solution</b>:
     16×16=256. <b>Explanation</b>: For 2D: rows × cols.</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Page 13: Matrix Multiplication...</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>CPU version of matrix multiply: Triple loop over i,j,k:
C[i][j] += A[i][k] * B[k][j]. Visual: Blue row A × orange column B &#8594; gray
C element. WIDTH=n for square matrices. Basic serial implementation.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Number of loops in CPU matrix multiply? a) 1 b) 2 c) 3 d)
     4 <b>Answer: c) 3</b></li>
 <li class=MsoNormal>What does the inner loop compute? a) Sum of products b)
     Single multiply c) Matrix addition d) Vector dot <b>Answer: a) Sum of
     products</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Spreadsheet Apps</b>: Excel matrix functions for
     financial modeling.</li>
 <li class=MsoNormal><b>Robotics</b>: Transform matrices for kinematics in ROS.</li>
 <li class=MsoNormal><b>Web Maps</b>: Google Maps uses for coordinate
     projections.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>n=2 matrices: A=[[1,2],[3,4]], B=[[5,6],[7,8]]. Compute
     C[0][0]. <b>Solution</b>: 1<i>5 + 2</i>7 = 5+14=19. <b>Explanation</b>:
     Sum over k: A[0][k]*B[k][0] for k=0,1.</li>
 <li class=MsoNormal>Total FLOPs for n=10? <b>Solution</b>: 2 n^3 = 2×1000=2000
     FLOPs. <b>Explanation</b>: 2 ops (mul+add) × n iterations per loop × n ×
     n.</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Page 14: Programmer’s view: Memory Model</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>No content provided beyond title. Assumes overview of GPU
memory hierarchy: threads access registers (fastest), local/shared/global
memory, constants/textures (read-only). Host (CPU) accesses
global/constant/texture.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What memory can threads read/write per-block? a) Only
     global b) Shared memory c) Texture only d) Host memory <b>Answer: b)
     Shared memory</b></li>
 <li class=MsoNormal>Host can read/write which? a) Registers b) Local only c)
     Global, constant, texture d) Shared <b>Answer: c) Global, constant,
     texture</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Kernel Development</b>: Declaring <b>shared</b> in CUDA
     for tiled matrix multiply.</li>
 <li class=MsoNormal><b>Texture Mapping</b>: In shaders for filtered image
     access.</li>
 <li class=MsoNormal><b>Constant Params</b>: Passing model weights read-only in
     ML kernels.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>If shared memory=48KB/block, 256 threads, bytes/thread? <b>Solution</b>:
     48×1024 / 256 = 192 bytes/thread. <b>Explanation</b>: Total KB to bytes
     (×1024), divide by threads.</li>
 <li class=MsoNormal>Global memory access latency=400 cycles, threads=1000.
     Coalesced factor=1/4. Effective latency? <b>Solution</b>: 400 / (1000/4)
     wait, but simplified: For coalescing, latency per warp (32 threads)=400,
     but scale. Basic: Uncoalesced increases by factor 4: 1600 cycles. <b>Explanation</b>:
     Coalescing reduces transactions; uncoalesced × warp size factor.</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Page 15: Programmer’s View: Memory Detail – Thread and
Host</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>Details: Each thread R/W registers (private, fast), local
memory (slow global spill), shared (block-fast), global (all-slow). Read-only:
constant/texture (cached). Host R/W global/constant/texture only. No direct
shared access from host.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Which memory is per-thread and read/write? a) Global b)
     Registers c) Constant d) Texture <b>Answer: b) Registers</b></li>
 <li class=MsoNormal>What can't host access directly? a) Global b) Shared c)
     Constant d) Texture <b>Answer: b) Shared</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Register Pressure</b>: Optimizing kernels to fit vars
     in registers for speed in simulations.</li>
 <li class=MsoNormal><b>Constant Buffers</b>: Storing uniforms in shaders for
     real-time graphics.</li>
 <li class=MsoNormal><b>Global Sync</b>: Using atomic ops on global for
     parallel reductions.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>1024 threads, each 64 registers (4 bytes each). Total
     register bytes? <b>Solution</b>: 1024 × 64 × 4 = 262144 bytes. <b>Explanation</b>:
     Threads × regs/thread × bytes/reg.</li>
 <li class=MsoNormal>Constant memory=64KB, used by 1M threads. Effective per
     thread? <b>Solution</b>: 64×1024 / 1e6 &#8776; 0.066 KB/thread. <b>Explanation</b>:
     Total / threads (shared across).</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Page 16: Sharing Data Between Threads</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>Threads in a block share data via <b>shared</b>
memory—super-fast on-chip, user-managed, per-block only (not visible outside
block). Declare with <b>shared</b> keyword. Great for reuse without global
fetches.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>How do block threads share data? a) Global memory b)
     Shared memory c) Registers d) Local <b>Answer: b) Shared memory</b></li>
 <li class=MsoNormal>Visibility of shared data? a) All grid b) Per-block only
     c) Host d) Other grids <b>Answer: b) Per-block only</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Tiled Algorithms</b>: Loading sub-matrices to shared in
     CUDA GEMM.</li>
 <li class=MsoNormal><b>Reduction Ops</b>: Summing in shared before global
     write in parallel sum.</li>
 <li class=MsoNormal><b>Particle Sims</b>: Sharing neighbor lists in blocks for
     N-body.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Shared mem=48KB/block, block=256 threads, 4 floats/thread
     (16 bytes). Threads supported? <b>Solution</b>: 48×1024 / (256×16) wait
     no: Per thread alloc, but total: Max threads if all
     shared=48kB/16B&#8776;3072, but limit 1024. Basic: Bytes avail=49152, per
     thread=16, max=49152/16=3072 threads. <b>Explanation</b>: Total shared /
     bytes needed per thread.</li>
 <li class=MsoNormal>Load 1000 floats to shared (8KB), 512 threads. Reuse
     factor if each reads 2? <b>Solution</b>: Total reads=512×2=1024, but 1000
     unique: Reuse&#8776;1.024x. <b>Explanation</b>: Total accesses / unique
     data size.</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Page 17: Matrix Multiplication...</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>No new content; continues matrix multiply discussion. Likely
elaborates on GPU version or tiling intro.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Key to GPU matrix multiply speedup? (Assuming context) a)
     More loops b) Tiling to shared c) Larger n d) Serial execution <b>Answer:
     b) Tiling to shared</b></li>
 <li class=MsoNormal>Why tile? a) Increase global accesses b) Fit in shared for
     reuse c) Reduce FLOPs d) Add sync <b>Answer: b) Fit in shared for reuse</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>cuBLAS</b>: Optimized GEMM using tiling for AI.</li>
 <li class=MsoNormal><b>Physics Engines</b>: Bullet Physics GPU for collision
     matrices.</li>
 <li class=MsoNormal><b>Crypto</b>: Matrix ops in blockchain mining.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Tile size=16x16, n=64. Tiles per dim? <b>Solution</b>:
     64/16=4 tiles/dim. Total tiles=16. <b>Explanation</b>: n / tile_size per
     row/col, square= (n/tile)^2.</li>
 <li class=MsoNormal>FLOPs per tile? (16x16) <b>Solution</b>: 2 × 16^3 =
     2×4096=8192 FLOPs. <b>Explanation</b>: Per sub-matrix: 2 × tile^3.</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Page 18: Matrix Multiplication...</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>Likely shows GPU kernel pseudocode or diagram for parallel
C[i][j] assignment across threads.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>In GPU MM, threads compute? a) Whole matrix b) One element
     c) One row d) One block <b>Answer: b) One element</b> (or tile)</li>
 <li class=MsoNormal>Benefit of parallel? a) Slower b) More memory use c)
     Independent computes d) No sync <b>Answer: c) Independent computes</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Neural Nets</b>: Conv layers as matrix multiplies in
     TensorRT.</li>
 <li class=MsoNormal><b>Finance</b>: Portfolio optimization matrices on GPUs.</li>
 <li class=MsoNormal><b>Genomics</b>: Alignment scores via MM.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>1000x1000 matrix, 1 thread/element. Threads needed? <b>Solution</b>:
     1000×1000=1e6 threads. <b>Explanation</b>: Rows × cols.</li>
 <li class=MsoNormal>Speedup if GPU 100x faster, n=1024 FLOPs=2e9. Time
     CPU=10s, GPU? <b>Solution</b>: 10/100=0.1s. <b>Explanation</b>: Time_new =
     time_old / speedup.</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Page 19: Matrix Multiplication...</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>Tips to speed up: Promote fine parallelism, minimize data
moves, exploit parallel access, use blocking/tiling. Load tile to
shared/registers, compute with threads. Blocking: Load to shared for reuse;
partition to fit small shared size.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Technique to reduce global memory access? a) Unrolling b)
     Blocking/Tiling c) Vectorizing d) Branching <b>Answer: b) Blocking/Tiling</b></li>
 <li class=MsoNormal>Why partition data? a) Shared is small b) Increase size c)
     Slow down d) No reuse <b>Answer: a) Shared is small</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>MAGMA Library</b>: Tiled LU decomposition on GPUs.</li>
 <li class=MsoNormal><b>Ray Tracing</b>: Tiled BVH traversals.</li>
 <li class=MsoNormal><b>Video Codec</b>: HEVC encoding tiles.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Shared=16KB floats (4B), tile=32x32=1024 floats=4KB. Tiles
     fit? <b>Solution</b>: 16/4=4 tiles. <b>Explanation</b>: Shared bytes /
     tile bytes.</li>
 <li class=MsoNormal>Reuse in tile: Each thread uses tile 16 times (for k
     loop). Reuse factor=16. <b>Explanation</b>: Inner loop reuses loaded data
     × tile_size times.</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Page 20: Matrix Multiplication...</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>Likely code snippet for tiled GPU kernel: Threads load A/B
tiles to shared, sync, compute C sub-tile.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>After load to shared? a) Compute immediately b)
     __syncthreads() c) Exit d) Global write <b>Answer: b) __syncthreads()</b></li>
 <li class=MsoNormal>Who loads data? a) Host b) Threads cooperatively c) One
     thread d) No load <b>Answer: b) Threads cooperatively</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>OpenCL Kernels</b>: Tiled convolutions for image AI.</li>
 <li class=MsoNormal><b>HPC Solvers</b>: GPU PETSc for sparse MM.</li>
 <li class=MsoNormal><b>AR/VR</b>: Pose estimation matrices.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Block 16x16 threads, tile 16x16. Loads per thread? <b>Solution</b>:
     1 element each (coalesced). <b>Explanation</b>: Threads map 1:1 to tile
     elements.</li>
 <li class=MsoNormal>Global accesses reduced by? (Reuse 16x) 16x fewer. <b>Explanation</b>:
     Factor = tile_dim.</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Page 21: Matrix Multiplication...</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>Diagram: Thread blocks compute sub-C, load A stripe, B panel
to shared, multiply.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>A stripe for row i block? a) Full column b) Block rows ×
     full cols? No: Typically block rows × tile cols. <b>Answer: (Context)
     Block height × full width for reuse.</b></li>
 <li class=MsoNormal>Sync point? a) After compute b) After load c) Before loop
     d) End <b>Answer: b) After load</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>CUDA Samples</b>: NVIDIA's matrixMul tiled example.</li>
 <li class=MsoNormal><b>Fluid Sims</b>: Lattice Boltzmann on GPU tiles.</li>
 <li class=MsoNormal><b>Chem Modeling</b>: Quantum chemistry MM.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Block dim=32, tiles= n/32. For n=256, blocks/dim? <b>Solution</b>:
     256/32=8. <b>Explanation</b>: n / block_size.</li>
 <li class=MsoNormal>FLOPs/block: 32x32 tile=2×32^3=65536 FLOPs. <b>Explanation</b>:
     2 × dim^3.</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Page 22: Matrix Multiplication...</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>Full kernel outline: Outer loops over tiles, inner compute
with shared reuse.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Outer loop over? a) Elements b) Tiles c) Blocks d) Grids <b>Answer:
     b) Tiles</b></li>
 <li class=MsoNormal>Inner: Use data in? a) Global b) Shared c) Registers only
     d) Constant <b>Answer: b) Shared</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>TensorFlow GPU</b>: Optimized MM in XLA.</li>
 <li class=MsoNormal><b>Autonomous Driving</b>: Sensor fusion matrices.</li>
 <li class=MsoNormal><b>NLP</b>: Transformer attention as MM.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>n=512, tile=64. Iterations outer? <b>Solution</b>:
     512/64=8 per dim, 64 total. <b>Explanation</b>: (n/tile)^2 for 2D.</li>
 <li class=MsoNormal>Total FLOPs unchanged=2 n^3=2×512^3=268435456. Speedup
     from reuse. <b>Explanation</b>: FLOPs fixed, time reduced by AI increase.</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Page 23: CPU/GPU Synchronization</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>Final slide: Compares sync—CPU uses barriers/locks; GPU uses
__syncthreads() per block, grid sync via events/callbacks. Host-device sync
with cudaDeviceSynchronize().</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>GPU block sync primitive? a) pthread_mutex b)
     __syncthreads() c) Atomic add d) Fence <b>Answer: b) __syncthreads()</b></li>
 <li class=MsoNormal>Full grid sync? a) Automatic b) Via host or events c) Per
     thread d) No need <b>Answer: b) Via host or events</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Hybrid CPU-GPU</b>: Sync in OpenMP offload for
     simulations.</li>
 <li class=MsoNormal><b>Async Compute</b>: Overlapping kernels in DX12 games.</li>
 <li class=MsoNormal><b>MPI+GPU</b>: Sync in distributed training.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>100 blocks, sync every 10. Sync calls? <b>Solution</b>:
     100/10=10 calls. <b>Explanation</b>: Groups / group_size.</li>
 <li class=MsoNormal>Latency: Block sync=10 cycles, grid=1ms. For 1000 blocks,
     effective if batched 10/block? 100 syncs ×10 cycles +1ms. But simplified:
     Total time estimate. <b>Explanation</b>: Sum sync costs.</li>
</ol>

<p class=MsoNormal>&nbsp;</p>

<div style='border:none;border-bottom:double windowtext 2.25pt;padding:0in 0in 1.0pt 0in'>

<p class=MsoNormal style='border:none;padding:0in'>&nbsp;</p>

</div>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>The content of the file
&quot;GPU_Programming_Lect05.pdf&quot; covers key concepts in GPU programming,
focusing on performance modeling, arithmetic intensity, memory hierarchy, and
optimization techniques like tiling in matrix multiplication. Below is a structured
breakdown of each major section (interpreted as slides) with simplified
explanations, multiple-choice questions (MCQs), use cases, and numerical
problems.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Performance Measurement (FLOPs)</b></p>

<p class=MsoNormal>The performance of a CPU or GPU is often measured in FLOPs
(Floating-Point Operations per Second). This metric counts how many
floating-point calculations (like addition or multiplication) a processor can
perform in one second. For example, a function that performs 100 floating-point
operations in one second runs at 100 FLOPs.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal><b>MCQs</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>What does FLOPs stand for?<br>
     a) Floating-Point Logical Operations per Second<br>
     b) Floating-Point Operations per Second<br>
     c) Fixed-Logic Operations per Second<br>
     d) Frequency of Logical Processing<br>
     Answer:&nbsp;b<span style='font-family:"Arial",sans-serif'>&#8203;</span></li>
 <li class=MsoNormal>If a loop performs 2 operations per iteration for 50
     iterations in 1 second, what is the FLOP count?<br>
     a) 50<br>
     b) 100<br>
     c) 25<br>
     d) 200<br>
     Answer:&nbsp;b<span style='font-family:"Arial",sans-serif'>&#8203;</span></li>
</ul>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Benchmarking scientific computing applications</li>
 <li class=MsoNormal>Comparing hardware performance for machine learning
     workloads<span style='font-family:"Arial",sans-serif'>&#8203;</span></li>
</ul>

<p class=MsoNormal><b>Numerical</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A kernel performs 200 floating-point operations in 0.5
     seconds. Calculate its FLOPs.<br>
     Solution:&nbsp;2000.5=4000.5200=400&nbsp;FLOPs<span style='font-family:
     "Arial",sans-serif'>&#8203;</span></li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Memory Hierarchy and Latency</b></p>

<p class=MsoNormal>Modern processors have multiple memory levels: registers
(fastest), L1, L2, L3 caches, and main memory (DRAM). Access times increase
from 1 cycle (register) to 200 cycles (DRAM). Since memory operations are slow,
maximizing computation per data access improves efficiency.<span
style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal><b>MCQs</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Which memory has the lowest latency?<br>
     a) DRAM<br>
     b) L3 Cache<br>
     c) Register<br>
     d) L2 Cache<br>
     Answer:&nbsp;c<span style='font-family:"Arial",sans-serif'>&#8203;</span></li>
 <li class=MsoNormal>Why is data reuse important?<br>
     a) To reduce power consumption<br>
     b) To minimize slow memory accesses<br>
     c) To increase clock speed<br>
     d) To improve disk I/O<br>
     Answer:&nbsp;b<span style='font-family:"Arial",sans-serif'>&#8203;</span></li>
</ul>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Optimizing code for high-performance computing (HPC)</li>
 <li class=MsoNormal>Designing efficient algorithms for real-time signal
     processing<span style='font-family:"Arial",sans-serif'>&#8203;</span></li>
</ul>

<p class=MsoNormal><b>Numerical</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>If loading data from DRAM takes 200 cycles and a
     computation uses that data 10 times, what is the effective cost per
     operation?<br>
     Solution:&nbsp;20010=2010200=20&nbsp;cycles per operation<span
     style='font-family:"Arial",sans-serif'>&#8203;</span></li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Arithmetic Intensity (AI)</b></p>

<p class=MsoNormal>Arithmetic Intensity measures the ratio of computation
(FLOPs) to data movement (bytes transferred). High AI means more computation
per byte loaded, which is ideal for GPUs with limited memory bandwidth. It is
calculated as:<br>
AI=Total&nbsp;FLOPsTotal&nbsp;Bytes&nbsp;AccessedAI=Total&nbsp;Bytes&nbsp;AccessedTotal&nbsp;FLOPs<span
style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal><b>MCQs</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>What does high arithmetic intensity indicate?<br>
     a) More memory accesses than computations<br>
     b) More computations per memory access<br>
     c) High power usage<br>
     d) Low parallelism<br>
     Answer:&nbsp;b<span style='font-family:"Arial",sans-serif'>&#8203;</span></li>
 <li class=MsoNormal>Which architecture benefits most from high AI?<br>
     a) CPUs with large caches<br>
     b) GPUs with high compute throughput<br>
     c) Hard drives<br>
     d) Network cards<br>
     Answer:&nbsp;b<span style='font-family:"Arial",sans-serif'>&#8203;</span></li>
</ul>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Evaluating suitability of algorithms for GPU acceleration</li>
 <li class=MsoNormal>Applying the Roofline model to predict performance
     bottlenecks<span style='font-family:"Arial",sans-serif'>&#8203;</span></li>
</ul>

<p class=MsoNormal><b>Numerical</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A function performs 1000 FLOPs and accesses 200 bytes of
     data. Find AI.<br>
     Solution:&nbsp;1000200=52001000=5&nbsp;FLOPs/Byte<span style='font-family:
     "Arial",sans-serif'>&#8203;</span></li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Roofline Model</b></p>

<p class=MsoNormal>The Roofline model predicts maximum performance based on
peak FLOPs and memory bandwidth. Attainable FLOPs = min(Peak GFLOPs, AI × Peak
GB/s). It helps identify whether a kernel is compute-bound or memory-bound.<span
style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal><b>MCQs</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>In the Roofline model, what determines the
     &quot;roof&quot;?<br>
     a) Disk speed<br>
     b) Network bandwidth<br>
     c) Peak FLOPs and memory bandwidth<br>
     d) Number of threads<br>
     Answer:&nbsp;c<span style='font-family:"Arial",sans-serif'>&#8203;</span></li>
 <li class=MsoNormal>If AI is low, what likely limits performance?<br>
     a) Compute units<br>
     b) Memory bandwidth<br>
     c) Clock frequency<br>
     d) Cache size<br>
     Answer:&nbsp;b<span style='font-family:"Arial",sans-serif'>&#8203;</span></li>
</ul>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Guiding optimization strategies in HPC applications</li>
 <li class=MsoNormal>Selecting appropriate hardware for deep learning models<span
     style='font-family:"Arial",sans-serif'>&#8203;</span></li>
</ul>

<p class=MsoNormal><b>Numerical</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Given peak GFLOPs = 1000, peak GB/s = 100, and AI = 8,
     calculate attainable FLOPs.<br>
     Solution:&nbsp;min<span style='font-family:"Cambria Math",serif'>&#8289;</span>(1000,8×100)=min<span
     style='font-family:"Cambria Math",serif'>&#8289;</span>(1000,800)=800min(1000,8×100)=min(1000,800)=800&nbsp;GFLOPs<span
     style='font-family:"Arial",sans-serif'>&#8203;</span></li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>GPU Programming Basics</b></p>

<p class=MsoNormal>GPUs execute kernels using a grid of thread blocks. Threads
have unique IDs and can be organized in 1D, 2D, or 3D configurations. Threads
in the same block can synchronize and share data via fast shared memory, while
inter-block communication occurs through global memory.<span style='font-family:
"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal><b>MCQs</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>How do threads in the same block communicate?<br>
     a) Through global memory only<br>
     b) Through shared memory<br>
     c) Through disk storage<br>
     d) Through network<br>
     Answer:&nbsp;b<span style='font-family:"Arial",sans-serif'>&#8203;</span></li>
 <li class=MsoNormal>What is the role of blockIdx and threadIdx?<br>
     a) Manage power settings<br>
     b) Index threads and blocks for data mapping<br>
     c) Control cooling fans<br>
     d) Schedule OS tasks<br>
     Answer:&nbsp;b<span style='font-family:"Arial",sans-serif'>&#8203;</span></li>
</ul>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Parallelizing image processing operations</li>
 <li class=MsoNormal>Accelerating financial simulations using Monte Carlo
     methods<span style='font-family:"Arial",sans-serif'>&#8203;</span></li>
</ul>

<p class=MsoNormal><b>Numerical</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A 2D grid has 10 blocks, each with 32×32 threads. Total
     threads = ?<br>
     Solution:&nbsp;10×32×32=10,24010×32×32=10,240&nbsp;threads<span
     style='font-family:"Arial",sans-serif'>&#8203;</span></li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Matrix Multiplication (CPU vs GPU)</b></p>

<p class=MsoNormal>CPU version uses nested loops; GPU version assigns each
thread to compute one element of the result matrix. The GPU kernel reads from
global memory, performs multiply-add, and writes back. Without optimization,
memory bandwidth limits performance.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal><b>MCQs</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>In GPU matrix multiplication, who computes each output
     element?<br>
     a) One block<br>
     b) One warp<br>
     c) One thread<br>
     d) One SM<br>
     Answer:&nbsp;c<span style='font-family:"Arial",sans-serif'>&#8203;</span></li>
 <li class=MsoNormal>What is the main bottleneck in naive GPU matrix
     multiplication?<br>
     a) Register pressure<br>
     b) Memory bandwidth<br>
     c) Thread divergence<br>
     d) Instruction decode<br>
     Answer:&nbsp;b<span style='font-family:"Arial",sans-serif'>&#8203;</span></li>
</ul>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Deep learning inference (e.g., fully connected layers)</li>
 <li class=MsoNormal>Scientific simulations involving linear algebra<span
     style='font-family:"Arial",sans-serif'>&#8203;</span></li>
</ul>

<p class=MsoNormal><b>Numerical</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>For a 1024×1024 float matrix multiply, total FLOPs = ?<br>
     Solution:&nbsp;2×10243=2,147,483,6482×10243=2,147,483,648&nbsp;FLOPs<span
     style='font-family:"Arial",sans-serif'>&#8203;</span></li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Tiling (Blocking) Optimization</b></p>

<p class=MsoNormal>Tiling improves performance by loading small blocks (tiles)
of data into shared memory. Each tile is reused multiple times, reducing global
memory accesses. For a 16×16 tile, memory traffic can be reduced by a factor of
16.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal><b>MCQs</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Why use tiling in GPU kernels?<br>
     a) To reduce register usage<br>
     b) To reuse data in fast shared memory<br>
     c) To increase clock speed<br>
     d) To reduce power<br>
     Answer:&nbsp;b<span style='font-family:"Arial",sans-serif'>&#8203;</span></li>
 <li class=MsoNormal>What memory is used for tiles?<br>
     a) Global memory<br>
     b) Constant memory<br>
     c) Shared memory<br>
     d) Texture memory<br>
     Answer:&nbsp;c<span style='font-family:"Arial",sans-serif'>&#8203;</span></li>
</ul>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Optimizing convolution operations in CNNs</li>
 <li class=MsoNormal>Accelerating large-scale matrix operations in HPC<span
     style='font-family:"Arial",sans-serif'>&#8203;</span></li>
</ul>

<p class=MsoNormal><b>Numerical</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A 32×32 tile of floats (4 bytes each) requires how much
     shared memory?<br>
     Solution:&nbsp;32×32×4=4,09632×32×4=4,096&nbsp;bytes<span
     style='font-family:"Arial",sans-serif'>&#8203;</span></li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Synchronization and Memory Model</b></p>

<p class=MsoNormal>CUDA kernels use&nbsp;__syncthreads()&nbsp;to synchronize
threads within a block. The memory model includes global, shared, constant,
texture, local, and register memories. Host code can access global, constant,
and texture memories.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal><b>MCQs</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Which function synchronizes threads in a block?<br>
     a)&nbsp;cudaDeviceSynchronize()<br>
     b)&nbsp;__syncthreads()<br>
     c)&nbsp;sleep()<br>
     d)&nbsp;barrier()<br>
     Answer:&nbsp;b<span style='font-family:"Arial",sans-serif'>&#8203;</span></li>
 <li class=MsoNormal>Which memory is read-only for kernels?<br>
     a) Shared<br>
     b) Global<br>
     c) Constant<br>
     d) Local<br>
     Answer:&nbsp;c<span style='font-family:"Arial",sans-serif'>&#8203;</span></li>
</ul>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Implementing reduction operations (e.g., sum, max)</li>
 <li class=MsoNormal>Developing cooperative thread arrays (CTAs) for complex
     computations<span style='font-family:"Arial",sans-serif'>&#8203;</span></li>
</ul>

<p class=MsoNormal><b>Numerical</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A kernel with 256 threads per block uses 16 registers per
     thread. Total register usage per block = ?<br>
     Solution:&nbsp;256×16=4,096256×16=4,096&nbsp;registers</li>
</ul>

<p class=MsoNormal>&nbsp;</p>

<div style='border:none;border-bottom:double windowtext 2.25pt;padding:0in 0in 1.0pt 0in'>

<p class=MsoNormal style='border:none;padding:0in'>&nbsp;</p>

</div>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>hanks, Pankaj! I'll now break down each slide from the <b>GPU
Programming Lecture 05</b> into:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Easy Explanation</b></li>
 <li class=MsoNormal><b>MCQs</b></li>
 <li class=MsoNormal><b>Use Cases</b></li>
 <li class=MsoNormal><b>Numerical Examples</b></li>
</ol>

<p class=MsoNormal>Let’s start with the first few slides and proceed in batches
to keep it clear and manageable.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 2: Unit of CPU Performance Measurement – FLOPs</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Easy Explanation:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>FLOPs</b> stands for <b>Floating Point Operations per
     Second</b>.</li>
 <li class=MsoNormal>It measures how fast a CPU can perform mathematical
     operations.</li>
 <li class=MsoNormal>Double precision uses more bits (1 for sign, 11 for
     exponent, 52 for fraction) than single precision.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> What does FLOPs measure?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Memory size</li>
 <li class=MsoNormal>B) CPU temperature</li>
 <li class=MsoNormal>C) Floating point operations per second</li>
 <li class=MsoNormal>D) Number of cores<br>
     <b>Answer:</b> C</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Comparing performance of CPUs or GPUs in scientific
     computing, simulations, and AI workloads.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical Example:</b></p>

<p class=MsoNormal>If a CPU performs 1 billion floating-point operations in 2
seconds:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>FLOPs = 1,000,000,0002=500,000,000\frac{1,000,000,000}{2}
     = 500,000,00021,000,000,000<span style='font-family:"Arial",sans-serif'>&#8203;</span>=500,000,000
     FLOPs</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 3: CPU Performance Model (FLOPs)</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Easy Explanation:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A function mul_add runs a loop 50 times.</li>
 <li class=MsoNormal>Each loop does 2 operations: multiplication and addition.</li>
 <li class=MsoNormal>So, total operations = 50 × 2 = 100 FLOPs.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> How many FLOPs does one call to mul_add perform?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) 50</li>
 <li class=MsoNormal>B) 100</li>
 <li class=MsoNormal>C) 25</li>
 <li class=MsoNormal>D) 200<br>
     <b>Answer:</b> B</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Benchmarking CPU performance using simple code snippets.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical Example:</b></p>

<p class=MsoNormal>If mul_add takes 2 seconds:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>FLOPs = 1002=50\frac{100}{2} = 502100<span
     style='font-family:"Arial",sans-serif'>&#8203;</span>=50 FLOPs</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 4: FLOPs – Computation &amp; Communication</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Easy Explanation:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Memory access is slow compared to computation.</li>
 <li class=MsoNormal>So, we try to do more calculations with the data already
     loaded.</li>
 <li class=MsoNormal>This is measured using <b>Arithmetic Intensity (AI)</b>.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> What does Arithmetic Intensity measure?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Number of cores</li>
 <li class=MsoNormal>B) Memory size</li>
 <li class=MsoNormal>C) Computation per memory access</li>
 <li class=MsoNormal>D) CPU temperature<br>
     <b>Answer:</b> C</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Optimizing performance by reducing memory bottlenecks in
     high-performance computing.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical Example:</b></p>

<p class=MsoNormal>If 100 operations are done and 50 bytes are transferred:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>AI = 10050=2\frac{100}{50} = 250100<span style='font-family:
     "Arial",sans-serif'>&#8203;</span>=2 operations per byte</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal>Would you like me to continue with the next set of slides
(Slides 5–8)? Also, do you want this compiled into a downloadable document or
presentation later?</p>

<p class=MsoNormal>Sources</p>

<p class=MsoNormal><b>Provide your feedback on BizChat</b></p>

<p class=MsoNormal><b>You said: </b></p>

<p class=MsoNormal>Yes, continue with slides 5–8</p>

<p class=MsoNormal><b>Copilot said: </b></p>

<p class=MsoNormal>Copilot</p>

<p class=MsoNormal>Great! Let's continue with <b>Slides 5 to 8</b> from your
GPU Programming Lecture 05.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 5: Arithmetic Intensity</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Easy Explanation:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Memory Bandwidth</b> is how fast data moves between
     memory and CPU/GPU.</li>
 <li class=MsoNormal><b>Arithmetic Intensity (AI)</b> compares how much
     computation is done vs. how much data is moved.</li>
 <li class=MsoNormal>Formula for Bandwidth: </li>
</ul>

<p class=MsoNormal>Bandwidth&nbsp;(GB/s)=Bytes&nbsp;Read&nbsp;+&nbsp;Bytes&nbsp;Written109×Execution&nbsp;Time\text{Bandwidth
(GB/s)} = \frac{\text{Bytes Read + Bytes Written}}{10^9 \times \text{Execution
Time}}Bandwidth&nbsp;(GB/s)=109×Execution&nbsp;TimeBytes&nbsp;Read&nbsp;+&nbsp;Bytes&nbsp;Written<span
style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> What does memory bandwidth measure?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Number of CPU cores</li>
 <li class=MsoNormal>B) Speed of computation</li>
 <li class=MsoNormal>C) Data transfer rate</li>
 <li class=MsoNormal>D) Size of memory<br>
     <b>Answer:</b> C</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Used in performance tuning of applications like video
     processing, simulations, and machine learning.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical Example:</b></p>

<p class=MsoNormal>If 2 GB of data is read and written in 0.5 seconds:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Bandwidth = 2×109109×0.5=4\frac{2 \times 10^9}{10^9 \times
     0.5} = 4109×0.52×109<span style='font-family:"Arial",sans-serif'>&#8203;</span>=4
     GB/s</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 6: Calculation of Arithmetic Intensity (CPUs)</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Easy Explanation:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>The function arraymul_add performs one multiplication and
     one addition per loop.</li>
 <li class=MsoNormal>It accesses memory for x[i] and y[i], and writes to y[i].</li>
 <li class=MsoNormal>AI = Total operations / Total data transferred</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> In arraymul_add, how many operations are done per
iteration?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) 1</li>
 <li class=MsoNormal>B) 2</li>
 <li class=MsoNormal>C) 3</li>
 <li class=MsoNormal>D) 4<br>
     <b>Answer:</b> B</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Helps optimize loops in numerical computing by reducing
     memory access.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical Example:</b></p>

<p class=MsoNormal>For n = 1000:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Operations = 2×1000=20002 \times 1000 = 20002×1000=2000</li>
 <li class=MsoNormal>Data transfer = 3×8×1000=24,0003 \times 8 \times 1000 =
     24,0003×8×1000=24,000 bytes</li>
 <li class=MsoNormal>AI = 200024000=0.083\frac{2000}{24000} = 0.083240002000<span
     style='font-family:"Arial",sans-serif'>&#8203;</span>=0.083
     operations/byte</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 7: Continued AI Calculation</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Easy Explanation:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Each iteration loads 2 values (x[i], y[i]) and stores 1
     value (y[i]).</li>
 <li class=MsoNormal>Each double is 8 bytes.</li>
 <li class=MsoNormal>Total memory traffic = 24n24n24n bytes</li>
 <li class=MsoNormal>AI = 2n24n=112\frac{2n}{24n} = \frac{1}{12}24n2n<span
     style='font-family:"Arial",sans-serif'>&#8203;</span>=121<span
     style='font-family:"Arial",sans-serif'>&#8203;</span></li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> What is the total memory traffic per iteration in arraymul_add?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) 8 bytes</li>
 <li class=MsoNormal>B) 16 bytes</li>
 <li class=MsoNormal>C) 24 bytes</li>
 <li class=MsoNormal>D) 32 bytes<br>
     <b>Answer:</b> C</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Used to evaluate whether a program is compute-bound or
     memory-bound.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical Example:</b></p>

<p class=MsoNormal>For n = 500:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Memory traffic = 24×500=12,00024 \times 500 = 12,00024×500=12,000
     bytes</li>
 <li class=MsoNormal>Operations = 2×500=10002 \times 500 = 10002×500=1000</li>
 <li class=MsoNormal>AI = 100012000=0.083\frac{1000}{12000} = 0.083120001000<span
     style='font-family:"Arial",sans-serif'>&#8203;</span>=0.083</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 8: Homework – Matrix Multiplication (DGEMM)</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Easy Explanation:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>This function multiplies two matrices A and B and stores
     the result in C.</li>
 <li class=MsoNormal>It uses 2 operations (multiply and add) inside 3 nested
     loops.</li>
 <li class=MsoNormal>AI = 2n316n2=n8\frac{2n^3}{16n^2} = \frac{n}{8}16n22n3<span
     style='font-family:"Arial",sans-serif'>&#8203;</span>=8n<span
     style='font-family:"Arial",sans-serif'>&#8203;</span></li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> What is the Arithmetic Intensity of DGEMM for
matrix size nnn?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) n16\frac{n}{16}16n<span style='font-family:"Arial",sans-serif'>&#8203;</span></li>
 <li class=MsoNormal>B) n8\frac{n}{8}8n<span style='font-family:"Arial",sans-serif'>&#8203;</span></li>
 <li class=MsoNormal>C) n216\frac{n^2}{16}16n2<span style='font-family:"Arial",sans-serif'>&#8203;</span></li>
 <li class=MsoNormal>D) n28\frac{n^2}{8}8n2<span style='font-family:"Arial",sans-serif'>&#8203;</span><br>
     <b>Answer:</b> A</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Common in scientific computing, machine learning, and
     graphics.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical Example:</b></p>

<p class=MsoNormal>For n=64n = 64n=64:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>AI = 6416=4\frac{64}{16} = 41664<span style='font-family:
     "Arial",sans-serif'>&#8203;</span>=4 operations/byte</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal>Would you like me to continue with <b>Slides 9–12</b> next?
Also, would you prefer this compiled into a PowerPoint or PDF later?</p>

<p class=MsoNormal><b>Provide your feedback on BizChat</b></p>

<p class=MsoNormal><b>You said: </b></p>

<p class=MsoNormal>yes</p>

<p class=MsoNormal><b>Copilot said: </b></p>

<p class=MsoNormal>Copilot</p>

<p class=MsoNormal>Here’s the breakdown for <b>Slides 9 to 12</b>:</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 9: CPU Performance Model</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Easy Explanation:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>This slide likely revisits the CPU performance model using
     FLOPs.</li>
 <li class=MsoNormal>It emphasizes how performance is measured based on the
     number of floating-point operations per second.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> What does the CPU performance model primarily
measure?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Memory usage</li>
 <li class=MsoNormal>B) Floating-point operations per second</li>
 <li class=MsoNormal>C) Number of threads</li>
 <li class=MsoNormal>D) Cache size<br>
     <b>Answer:</b> B</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Used in benchmarking CPUs for tasks like simulations,
     rendering, and scientific computing.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical Example:</b></p>

<p class=MsoNormal>If a CPU performs 500 operations in 0.25 seconds:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>FLOPs = 5000.25=2000\frac{500}{0.25} = 20000.25500<span
     style='font-family:"Arial",sans-serif'>&#8203;</span>=2000 FLOPs</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 10: Amdahl’s Law</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Easy Explanation:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Amdahl’s Law shows the <b>limit of speedup</b> when only
     part of a program is parallelized.</li>
 <li class=MsoNormal>Even with many cores, the <b>non-parallel part</b> slows
     down the overall performance.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> What does Amdahl’s Law explain?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Memory bandwidth</li>
 <li class=MsoNormal>B) CPU temperature</li>
 <li class=MsoNormal>C) Speedup limits due to sequential code</li>
 <li class=MsoNormal>D) GPU architecture<br>
     <b>Answer:</b> C</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Helps decide whether parallelizing a program is worth the
     effort.</li>
 <li class=MsoNormal>Used in performance analysis of multi-core systems.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical Example:</b></p>

<p class=MsoNormal>If 10% of a program is sequential and 90% is parallelized:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Max speedup = 10.1+0.9N\frac{1}{0.1 + \frac{0.9}{N}}0.1+N0.9<span
     style='font-family:"Arial",sans-serif'>&#8203;</span>1<span
     style='font-family:"Arial",sans-serif'>&#8203;</span>, where N = number of
     cores</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 11: Quick Quiz – Amdahl’s Law Application</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Easy Explanation:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Given: 1% of code is sequential, rest is parallel.</li>
 <li class=MsoNormal>Using 61 cores, the speedup is calculated using Amdahl’s
     Law: </li>
</ul>

<p class=MsoNormal>Speedup=10.01+0.9961=38.125\text{Speedup} = \frac{1}{0.01 +
\frac{0.99}{61}} = 38.125Speedup=0.01+610.99<span style='font-family:"Arial",sans-serif'>&#8203;</span>1<span
style='font-family:"Arial",sans-serif'>&#8203;</span>=38.125</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> If 1% of a program is sequential and it runs on 61
cores, what is the speedup?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) 61</li>
 <li class=MsoNormal>B) 50</li>
 <li class=MsoNormal>C) 38.125</li>
 <li class=MsoNormal>D) 100<br>
     <b>Answer:</b> C</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Used to estimate performance gains in parallel computing
     environments.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical Example:</b></p>

<p class=MsoNormal>Try with 4% sequential code and 20 cores:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Speedup = 10.04+0.9620=10.04+0.048=10.088&#8776;11.36\frac{1}{0.04
     + \frac{0.96}{20}} = \frac{1}{0.04 + 0.048} = \frac{1}{0.088} \approx
     11.360.04+200.96<span style='font-family:"Arial",sans-serif'>&#8203;</span>1<span
     style='font-family:"Arial",sans-serif'>&#8203;</span>=0.04+0.0481<span
     style='font-family:"Arial",sans-serif'>&#8203;</span>=0.0881<span
     style='font-family:"Arial",sans-serif'>&#8203;</span>&#8776;11.36</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 12: Summary for Execution on GPU</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Easy Explanation:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Summarizes key points for running programs on GPUs: </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Use parallelism</li>
  <li class=MsoNormal>Optimize memory usage</li>
  <li class=MsoNormal>Maximize arithmetic intensity</li>
  <li class=MsoNormal>Minimize data transfer</li>
 </ul>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> Which of the following is important for GPU
execution?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Sequential execution</li>
 <li class=MsoNormal>B) High memory latency</li>
 <li class=MsoNormal>C) Maximizing arithmetic intensity</li>
 <li class=MsoNormal>D) Ignoring memory access patterns<br>
     <b>Answer:</b> C</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Guides developers in writing efficient GPU code for tasks
     like deep learning, simulations, and graphics.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical Example:</b></p>

<p class=MsoNormal>If a GPU kernel performs 10,000 operations and transfers
2,000 bytes:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>AI = 100002000=5\frac{10000}{2000} = 5200010000<span
     style='font-family:"Arial",sans-serif'>&#8203;</span>=5 operations/byte</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal>Would you like me to continue with <b>Slides 13–16</b> next?
And would you prefer this compiled into a PowerPoint or PDF once all slides are
covered?</p>

<p class=MsoNormal><b>Provide your feedback on BizChat</b></p>

<p class=MsoNormal><b>You said: </b></p>

<p class=MsoNormal>yes</p>

<p class=MsoNormal><b>Copilot said: </b></p>

<p class=MsoNormal>Copilot</p>

<p class=MsoNormal>Here’s the breakdown for <b>Slides 13 to 16</b>:</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 13: Matrix Multiplication</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Easy Explanation:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Matrix multiplication is a key operation in many
     applications.</li>
 <li class=MsoNormal>It involves multiplying rows of one matrix with columns of
     another.</li>
 <li class=MsoNormal>GPUs are great at this because they can do many
     multiplications in parallel.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> What is a key advantage of using GPUs for matrix
multiplication?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) More memory</li>
 <li class=MsoNormal>B) Faster clock speed</li>
 <li class=MsoNormal>C) Parallel computation</li>
 <li class=MsoNormal>D) Larger cache<br>
     <b>Answer:</b> C</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Used in deep learning, physics simulations, image
     processing, and graphics rendering.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical Example:</b></p>

<p class=MsoNormal>Multiply two 2×2 matrices:</p>

<p class=MsoNormal>A=[1234],B=[5678]A = \begin{bmatrix}1 &amp; 2\\3 &amp; 4\end{bmatrix},\quad
B = \begin{bmatrix}5 &amp; 6\\7 &amp; 8\end{bmatrix}A=[13<span
style='font-family:"Arial",sans-serif'>&#8203;</span>24<span style='font-family:
"Arial",sans-serif'>&#8203;</span>],B=[57<span style='font-family:"Arial",sans-serif'>&#8203;</span>68<span
style='font-family:"Arial",sans-serif'>&#8203;</span>]</p>

<p class=MsoNormal>C=A×B=[(1×5+2×7)(1×6+2×8)(3×5+4×7)(3×6+4×8)]=[19224350]C = A
\times B = \begin{bmatrix}(1×5 + 2×7) &amp; (1×6 + 2×8)\\(3×5 + 4×7) &amp; (3×6
+ 4×8)\end{bmatrix} = \begin{bmatrix}19 &amp; 22\\43 &amp; 50\end{bmatrix}C=A×B=[(1×5+2×7)(3×5+4×7)<span
style='font-family:"Arial",sans-serif'>&#8203;</span>(1×6+2×8)(3×6+4×8)<span
style='font-family:"Arial",sans-serif'>&#8203;</span>]=[1943<span
style='font-family:"Arial",sans-serif'>&#8203;</span>2250<span
style='font-family:"Arial",sans-serif'>&#8203;</span>]</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 14: Programmer’s View – Memory Model</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Easy Explanation:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>GPU memory is divided into different types: </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal><b>Global memory</b>: accessible by all threads</li>
  <li class=MsoNormal><b>Shared memory</b>: fast, shared within a block</li>
  <li class=MsoNormal><b>Local memory</b>: private to each thread</li>
 </ul>
 <li class=MsoNormal>Understanding this helps optimize performance.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> Which memory type is shared among threads in a
block?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Global memory</li>
 <li class=MsoNormal>B) Constant memory</li>
 <li class=MsoNormal>C) Shared memory</li>
 <li class=MsoNormal>D) Local memory<br>
     <b>Answer:</b> C</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Helps developers decide where to store data for faster
     access during GPU programming.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical Example:</b></p>

<p class=MsoNormal>If shared memory access takes 1 cycle and global memory
takes 100 cycles, using shared memory can speed up performance by 100×.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 15: Memory Detail – Thread and Host</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Easy Explanation:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Each thread can read/write: </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Registers, local memory, shared memory, global memory</li>
 </ul>
 <li class=MsoNormal>Host (CPU) can access: </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Global, constant, and texture memory</li>
 </ul>
 <li class=MsoNormal>Proper memory usage improves performance and reduces
     latency.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> What memory can both the host and threads access?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Registers</li>
 <li class=MsoNormal>B) Shared memory</li>
 <li class=MsoNormal>C) Global memory</li>
 <li class=MsoNormal>D) Local memory<br>
     <b>Answer:</b> C</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Important in CUDA programming for managing data between
     CPU and GPU.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical Example:</b></p>

<p class=MsoNormal>If a thread accesses global memory 1000 times and each
access takes 100 cycles:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Total time = 1000×100=100,0001000 \times 100 = 100,0001000×100=100,000
     cycles</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 16: Sharing Data Between Threads</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Easy Explanation:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Threads in the same block can share data using <b>shared
     memory</b>.</li>
 <li class=MsoNormal>Shared memory is fast and user-managed.</li>
 <li class=MsoNormal>It’s not visible to threads in other blocks.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> What keyword is used to declare shared memory in
CUDA?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) __global__</li>
 <li class=MsoNormal>B) __device__</li>
 <li class=MsoNormal>C) __shared__</li>
 <li class=MsoNormal>D) __local__<br>
     <b>Answer:</b> C</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Used in matrix multiplication, convolution operations, and
     sorting algorithms on GPUs.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical Example:</b></p>

<p class=MsoNormal>If 256 threads share a 4 KB block of shared memory:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Each thread can use up to 4096256=16\frac{4096}{256} = 162564096<span
     style='font-family:"Arial",sans-serif'>&#8203;</span>=16 bytes</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal>Would you like me to continue with <b>Slides 17–20</b> next?
And once all slides are done, I can compile everything into a PowerPoint or PDF
for you.</p>

<p class=MsoNormal><b>Provide your feedback on BizChat</b></p>

<p class=MsoNormal><b>You said: </b></p>

<p class=MsoNormal>yes</p>

<p class=MsoNormal><b>Copilot said: </b></p>

<p class=MsoNormal>Copilot</p>

<p class=MsoNormal>Here’s the breakdown for <b>Slides 17 to 20</b>:</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 17: Matrix Multiplication (Continued)</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Easy Explanation:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Continues the discussion on matrix multiplication.</li>
 <li class=MsoNormal>Likely shows how threads work together to compute matrix
     elements.</li>
 <li class=MsoNormal>Each thread can compute one element of the result matrix.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> What does each thread typically compute in
parallel matrix multiplication?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) A row</li>
 <li class=MsoNormal>B) A column</li>
 <li class=MsoNormal>C) A single element</li>
 <li class=MsoNormal>D) The entire matrix<br>
     <b>Answer:</b> C</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Used in GPU kernels where each thread computes one output
     element in matrix multiplication.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical Example:</b></p>

<p class=MsoNormal>For a 3×3 matrix, 9 threads can be used to compute all 9
elements simultaneously.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 18: Matrix Multiplication (Continued)</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Easy Explanation:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Shows how data is accessed and reused.</li>
 <li class=MsoNormal>Efficient memory access patterns are key to performance.</li>
 <li class=MsoNormal>Threads should avoid redundant memory loads.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> Why is data reuse important in GPU matrix
multiplication?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) To increase memory usage</li>
 <li class=MsoNormal>B) To reduce computation</li>
 <li class=MsoNormal>C) To minimize memory access time</li>
 <li class=MsoNormal>D) To increase thread count<br>
     <b>Answer:</b> C</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Optimizing GPU kernels by loading data once into shared
     memory and reusing it.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical Example:</b></p>

<p class=MsoNormal>If each thread loads 2 values and reuses them for 10
operations:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Memory accesses = 2</li>
 <li class=MsoNormal>Operations = 10</li>
 <li class=MsoNormal>AI = 102×8=0.625\frac{10}{2 \times 8} = 0.6252×810<span
     style='font-family:"Arial",sans-serif'>&#8203;</span>=0.625
     operations/byte</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 19: Matrix Multiplication – Optimization (Blocking)</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Easy Explanation:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Blocking</b> means dividing data into smaller chunks
     (tiles).</li>
 <li class=MsoNormal>Load tiles into shared memory and reuse them.</li>
 <li class=MsoNormal>This reduces global memory access and improves speed.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> What is the purpose of blocking in matrix
multiplication?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) To reduce thread count</li>
 <li class=MsoNormal>B) To increase memory usage</li>
 <li class=MsoNormal>C) To reuse data in shared memory</li>
 <li class=MsoNormal>D) To avoid parallelism<br>
     <b>Answer:</b> C</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Used in CUDA and OpenCL to optimize matrix multiplication
     and convolution operations.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical Example:</b></p>

<p class=MsoNormal>If shared memory can hold 16×16 tile:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Threads load 256 elements once and reuse them for 256
     multiplications.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 20: Matrix Multiplication (Continued)</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Easy Explanation:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Likely shows how threads are organized in blocks and
     grids.</li>
 <li class=MsoNormal>Each block handles a tile of the matrix.</li>
 <li class=MsoNormal>Threads within a block cooperate using shared memory.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> In GPU matrix multiplication, what does each block
typically compute?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) A single element</li>
 <li class=MsoNormal>B) A row</li>
 <li class=MsoNormal>C) A tile of the output matrix</li>
 <li class=MsoNormal>D) The entire matrix<br>
     <b>Answer:</b> C</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Helps scale matrix multiplication to large matrices using
     multiple blocks.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical Example:</b></p>

<p class=MsoNormal>For a 64×64 matrix with 16×16 tiles:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Number of blocks = 6416×6416=16\frac{64}{16} \times
     \frac{64}{16} = 161664<span style='font-family:"Arial",sans-serif'>&#8203;</span>×1664<span
     style='font-family:"Arial",sans-serif'>&#8203;</span>=16</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal>Would you like me to continue with <b>Slides 21–23</b> next?
Once all slides are done, I’ll prepare a PowerPoint or PDF with all
explanations, MCQs, use cases, and numericals.</p>

<p class=MsoNormal><b>Provide your feedback on BizChat</b></p>

<p class=MsoNormal><b>You said: </b></p>

<p class=MsoNormal>yes</p>

<p class=MsoNormal><b>Copilot said: </b></p>

<p class=MsoNormal>Copilot</p>

<p class=MsoNormal>Here’s the breakdown for <b>Slides 21 to 23</b> from your
GPU Programming Lecture 05:</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 21: Matrix Multiplication (Continued)</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Easy Explanation:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>This slide likely continues the optimization strategy.</li>
 <li class=MsoNormal>Threads are organized to compute parts of the matrix
     efficiently.</li>
 <li class=MsoNormal>Emphasis on minimizing memory access and maximizing
     parallelism.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> What is the goal of organizing threads in matrix
multiplication?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Increase memory usage</li>
 <li class=MsoNormal>B) Reduce parallelism</li>
 <li class=MsoNormal>C) Maximize computation efficiency</li>
 <li class=MsoNormal>D) Avoid shared memory<br>
     <b>Answer:</b> C</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Used in high-performance computing tasks like neural
     network training and scientific simulations.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical Example:</b></p>

<p class=MsoNormal>For a 128×128 matrix using 16×16 thread blocks:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Total blocks = 12816×12816=64\frac{128}{16} \times
     \frac{128}{16} = 6416128<span style='font-family:"Arial",sans-serif'>&#8203;</span>×16128<span
     style='font-family:"Arial",sans-serif'>&#8203;</span>=64</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 22: Matrix Multiplication (Continued)</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Easy Explanation:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Likely shows how data is loaded into shared memory.</li>
 <li class=MsoNormal>Threads use shared memory to avoid repeated global memory
     access.</li>
 <li class=MsoNormal>This improves speed and reduces latency.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> Why do threads use shared memory in matrix
multiplication?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) To increase latency</li>
 <li class=MsoNormal>B) To reduce memory bandwidth</li>
 <li class=MsoNormal>C) To avoid parallelism</li>
 <li class=MsoNormal>D) To reuse data efficiently<br>
     <b>Answer:</b> D</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Used in CUDA kernels for matrix operations, convolution
     layers in deep learning.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical Example:</b></p>

<p class=MsoNormal>If each thread loads 2 values and performs 16 operations:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>AI = 162×8=1\frac{16}{2 \times 8} = 12×816<span
     style='font-family:"Arial",sans-serif'>&#8203;</span>=1 operation/byte</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 23: CPU/GPU Synchronization</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Easy Explanation:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Synchronization ensures CPU and GPU work together
     correctly.</li>
 <li class=MsoNormal>Data must be transferred between CPU and GPU at the right
     time.</li>
 <li class=MsoNormal>Improper sync can lead to incorrect results or crashes.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> What is the purpose of CPU/GPU synchronization?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) To increase memory usage</li>
 <li class=MsoNormal>B) To avoid parallelism</li>
 <li class=MsoNormal>C) To ensure correct data transfer and execution</li>
 <li class=MsoNormal>D) To reduce computation<br>
     <b>Answer:</b> C</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Used in CUDA programming to synchronize kernel execution
     and memory transfers.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical Example:</b></p>

<p class=MsoNormal>If GPU kernel takes 2 ms and data transfer takes 1 ms:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Total time = 3 ms</li>
 <li class=MsoNormal>Sync ensures CPU waits for GPU to finish before accessing
     results</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
<b>All slides are now covered!</b><br>
Would you like me to now compile everything into a <b>PowerPoint presentation
(.pptx)</b> or a <b>PDF document</b> with:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Slide-wise explanations</li>
 <li class=MsoNormal>MCQs</li>
 <li class=MsoNormal>Use cases</li>
 <li class=MsoNormal>Numerical examples</li>
</ul>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<div style='border:none;border-bottom:double windowtext 2.25pt;padding:0in 0in 1.0pt 0in'>

<p class=MsoNormal style='border:none;padding:0in'>&nbsp;</p>

</div>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

</div>

</body>

</html>
