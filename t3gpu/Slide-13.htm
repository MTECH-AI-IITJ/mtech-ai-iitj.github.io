<html>

<head>
<meta http-equiv=Content-Type content="text/html; charset=windows-1252">
<meta name=Generator content="Microsoft Word 15 (filtered)">
<style>
<!--
 /* Font Definitions */
 @font-face
	{font-family:Wingdings;
	panose-1:5 0 0 0 0 0 0 0 0 0;}
@font-face
	{font-family:"Cambria Math";
	panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
	{font-family:Aptos;}
@font-face
	{font-family:"Segoe UI Emoji";
	panose-1:2 11 5 2 4 2 4 2 2 3;}
 /* Style Definitions */
 p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:8.0pt;
	margin-left:0in;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;}
h1
	{mso-style-link:"Heading 1 Char";
	margin-top:.25in;
	margin-right:0in;
	margin-bottom:4.0pt;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:20.0pt;
	font-family:"Aptos Display",sans-serif;
	color:#0F4761;
	font-weight:normal;}
h2
	{mso-style-link:"Heading 2 Char";
	margin-top:8.0pt;
	margin-right:0in;
	margin-bottom:4.0pt;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:16.0pt;
	font-family:"Aptos Display",sans-serif;
	color:#0F4761;
	font-weight:normal;}
h3
	{mso-style-link:"Heading 3 Char";
	margin-top:8.0pt;
	margin-right:0in;
	margin-bottom:4.0pt;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:14.0pt;
	font-family:"Aptos",sans-serif;
	color:#0F4761;
	font-weight:normal;}
h4
	{mso-style-link:"Heading 4 Char";
	margin-top:4.0pt;
	margin-right:0in;
	margin-bottom:2.0pt;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#0F4761;
	font-weight:normal;
	font-style:italic;}
h5
	{mso-style-link:"Heading 5 Char";
	margin-top:4.0pt;
	margin-right:0in;
	margin-bottom:2.0pt;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#0F4761;
	font-weight:normal;}
h6
	{mso-style-link:"Heading 6 Char";
	margin-top:2.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#595959;
	font-weight:normal;
	font-style:italic;}
p.MsoHeading7, li.MsoHeading7, div.MsoHeading7
	{mso-style-link:"Heading 7 Char";
	margin-top:2.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#595959;}
p.MsoHeading8, li.MsoHeading8, div.MsoHeading8
	{mso-style-link:"Heading 8 Char";
	margin:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#272727;
	font-style:italic;}
p.MsoHeading9, li.MsoHeading9, div.MsoHeading9
	{mso-style-link:"Heading 9 Char";
	margin:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#272727;}
p.MsoTitle, li.MsoTitle, div.MsoTitle
	{mso-style-link:"Title Char";
	margin-top:0in;
	margin-right:0in;
	margin-bottom:4.0pt;
	margin-left:0in;
	font-size:28.0pt;
	font-family:"Aptos Display",sans-serif;
	letter-spacing:-.5pt;}
p.MsoTitleCxSpFirst, li.MsoTitleCxSpFirst, div.MsoTitleCxSpFirst
	{mso-style-link:"Title Char";
	margin:0in;
	font-size:28.0pt;
	font-family:"Aptos Display",sans-serif;
	letter-spacing:-.5pt;}
p.MsoTitleCxSpMiddle, li.MsoTitleCxSpMiddle, div.MsoTitleCxSpMiddle
	{mso-style-link:"Title Char";
	margin:0in;
	font-size:28.0pt;
	font-family:"Aptos Display",sans-serif;
	letter-spacing:-.5pt;}
p.MsoTitleCxSpLast, li.MsoTitleCxSpLast, div.MsoTitleCxSpLast
	{mso-style-link:"Title Char";
	margin-top:0in;
	margin-right:0in;
	margin-bottom:4.0pt;
	margin-left:0in;
	font-size:28.0pt;
	font-family:"Aptos Display",sans-serif;
	letter-spacing:-.5pt;}
p.MsoSubtitle, li.MsoSubtitle, div.MsoSubtitle
	{mso-style-link:"Subtitle Char";
	margin-top:0in;
	margin-right:0in;
	margin-bottom:8.0pt;
	margin-left:0in;
	line-height:115%;
	font-size:14.0pt;
	font-family:"Aptos",sans-serif;
	color:#595959;
	letter-spacing:.75pt;}
a:link, span.MsoHyperlink
	{color:#467886;
	text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
	{color:#96607D;
	text-decoration:underline;}
p.MsoListParagraph, li.MsoListParagraph, div.MsoListParagraph
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:8.0pt;
	margin-left:.5in;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;}
p.MsoListParagraphCxSpFirst, li.MsoListParagraphCxSpFirst, div.MsoListParagraphCxSpFirst
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:.5in;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;}
p.MsoListParagraphCxSpMiddle, li.MsoListParagraphCxSpMiddle, div.MsoListParagraphCxSpMiddle
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:.5in;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;}
p.MsoListParagraphCxSpLast, li.MsoListParagraphCxSpLast, div.MsoListParagraphCxSpLast
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:8.0pt;
	margin-left:.5in;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;}
p.MsoQuote, li.MsoQuote, div.MsoQuote
	{mso-style-link:"Quote Char";
	margin-top:8.0pt;
	margin-right:0in;
	margin-bottom:8.0pt;
	margin-left:0in;
	text-align:center;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#404040;
	font-style:italic;}
p.MsoIntenseQuote, li.MsoIntenseQuote, div.MsoIntenseQuote
	{mso-style-link:"Intense Quote Char";
	margin-top:.25in;
	margin-right:.6in;
	margin-bottom:.25in;
	margin-left:.6in;
	text-align:center;
	line-height:115%;
	border:none;
	padding:0in;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#0F4761;
	font-style:italic;}
span.MsoIntenseEmphasis
	{color:#0F4761;
	font-style:italic;}
span.MsoIntenseReference
	{font-variant:small-caps;
	color:#0F4761;
	letter-spacing:.25pt;
	font-weight:bold;}
span.Heading1Char
	{mso-style-name:"Heading 1 Char";
	mso-style-link:"Heading 1";
	font-family:"Aptos Display",sans-serif;
	color:#0F4761;}
span.Heading2Char
	{mso-style-name:"Heading 2 Char";
	mso-style-link:"Heading 2";
	font-family:"Aptos Display",sans-serif;
	color:#0F4761;}
span.Heading3Char
	{mso-style-name:"Heading 3 Char";
	mso-style-link:"Heading 3";
	font-family:"Times New Roman",serif;
	color:#0F4761;}
span.Heading4Char
	{mso-style-name:"Heading 4 Char";
	mso-style-link:"Heading 4";
	font-family:"Times New Roman",serif;
	color:#0F4761;
	font-style:italic;}
span.Heading5Char
	{mso-style-name:"Heading 5 Char";
	mso-style-link:"Heading 5";
	font-family:"Times New Roman",serif;
	color:#0F4761;}
span.Heading6Char
	{mso-style-name:"Heading 6 Char";
	mso-style-link:"Heading 6";
	font-family:"Times New Roman",serif;
	color:#595959;
	font-style:italic;}
span.Heading7Char
	{mso-style-name:"Heading 7 Char";
	mso-style-link:"Heading 7";
	font-family:"Times New Roman",serif;
	color:#595959;}
span.Heading8Char
	{mso-style-name:"Heading 8 Char";
	mso-style-link:"Heading 8";
	font-family:"Times New Roman",serif;
	color:#272727;
	font-style:italic;}
span.Heading9Char
	{mso-style-name:"Heading 9 Char";
	mso-style-link:"Heading 9";
	font-family:"Times New Roman",serif;
	color:#272727;}
span.TitleChar
	{mso-style-name:"Title Char";
	mso-style-link:Title;
	font-family:"Aptos Display",sans-serif;
	letter-spacing:-.5pt;}
span.SubtitleChar
	{mso-style-name:"Subtitle Char";
	mso-style-link:Subtitle;
	font-family:"Times New Roman",serif;
	color:#595959;
	letter-spacing:.75pt;}
span.QuoteChar
	{mso-style-name:"Quote Char";
	mso-style-link:Quote;
	color:#404040;
	font-style:italic;}
span.IntenseQuoteChar
	{mso-style-name:"Intense Quote Char";
	mso-style-link:"Intense Quote";
	color:#0F4761;
	font-style:italic;}
.MsoPapDefault
	{margin-bottom:8.0pt;
	line-height:115%;}
@page WordSection1
	{size:8.5in 11.0in;
	margin:1.0in 1.0in 1.0in 1.0in;}
div.WordSection1
	{page:WordSection1;}
 /* List Definitions */
 ol
	{margin-bottom:0in;}
ul
	{margin-bottom:0in;}
-->
</style>

</head>

<body lang=EN-US link="#467886" vlink="#96607D" style='word-wrap:break-word'>

<div class=WordSection1>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129513;</span>
Slide 1: GPU Programming</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128313;</span>
Explanation</b></p>

<p class=MsoNormal>GPU programming means writing programs that run on the
Graphics Processing Unit instead of (or in addition to) the CPU. GPUs contain
thousands of lightweight threads that execute the same instruction on different
data in parallel — ideal for image processing, scientific simulation, and AI
training.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Deep-learning frameworks (like TensorFlow / PyTorch) use
     GPU kernels to accelerate matrix multiplications.</li>
 <li class=MsoNormal>Weather-model simulation.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129518;</span>
Numerical Example</b></p>

<p class=MsoNormal>If a CPU can process 4 operations at once and a GPU can run
1024 threads, the theoretical speed-up &#8776; 1024 ÷ 4 = 256× (if fully
utilized).</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>GPU programming is mainly used for<br>
     a) Sequential tasks&#8195;b) Parallel tasks&#8195;<span style='font-family:
     "Segoe UI Emoji",sans-serif'>&#9989;</span> <b>Answer:</b> b</li>
 <li class=MsoNormal>The key feature of GPU architecture is<br>
     a) High clock speed&#8195;b) Massive parallelism&#8195;<span
     style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span> <b>Answer:</b>
     b</li>
 <li class=MsoNormal>GPU threads are<br>
     a) Heavyweight&#8195;b) Lightweight&#8195;<span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     <b>Answer:</b> b</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129513;</span>
Slide 2: Memory Hierarchy</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128313;</span>
Explanation</b></p>

<p class=MsoNormal>GPU memory comes in layers — registers, shared memory,
global, constant, and texture memory. Shared memory is much faster than global
memory but limited in size. Global memory is large but slower.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Shared memory &#8594; temporary storage for partial
     results in a matrix multiplication.</li>
 <li class=MsoNormal>Constant memory &#8594; stores coefficients used by all
     threads.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129518;</span>
Numerical Example</b></p>

<p class=MsoNormal>If shared memory latency &#8776; 100× lower than global
memory, using it for inner-loop operations gives &#8776; 100× speed gain.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Which GPU memory is fastest? &#8594; <span
     style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span> Shared
     memory</li>
 <li class=MsoNormal>Which is accessible by both CPU and GPU? &#8594; <span
     style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span> Global
     memory</li>
 <li class=MsoNormal>Shared memory is __________ in size &#8594; <span
     style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span> Small (but
     fast)</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129513;</span>
Slide 3: GPU Memory Hierarchy</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128313;</span>
Explanation</b></p>

<p class=MsoNormal>Each Streaming Multiprocessor (SM) has registers and shared
memory (on-chip). All SMs share global memory (off-chip DRAM). Constant and
texture memory are read-only and cached for efficiency.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Texture memory &#8594; 2-D lookup tables in image
     processing.</li>
 <li class=MsoNormal>Constant memory &#8594; kernel parameters.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129518;</span>
Numerical Example</b></p>

<p class=MsoNormal>If global memory = 6 GB @ 400 GB/s, shared memory = 64 KB @
3000 GB/s &#8594; shared memory bandwidth is &#8776; 7.5× higher.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Constant memory is <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Read-only</li>
 <li class=MsoNormal>Shared memory belongs to <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Each block</li>
 <li class=MsoNormal>Global memory lies <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Off-chip</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129513;</span>
Slide 4: Shared Memory Architecture</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128313;</span>
Explanation</b></p>

<p class=MsoNormal>Shared memory is split into banks (e.g., 32 banks). Each
bank can serve one thread at a time. If two threads in a warp access the same
bank, their accesses are serialized &#8594; bank conflict &#8594; slowdown.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case</b></p>

<p class=MsoNormal>Matrix transpose kernel: careful indexing avoids bank
conflicts.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129518;</span>
Numerical Example</b></p>

<p class=MsoNormal>If 32 threads access 32 different banks &#8594; 1 cycle. If
all access same bank &#8594; 32 cycles (32× slowdown).</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Shared memory is divided into <span style='font-family:
     "Segoe UI Emoji",sans-serif'>&#9989;</span> Banks</li>
 <li class=MsoNormal>Bank conflict means <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Multiple threads access same bank</li>
 <li class=MsoNormal>To avoid conflicts &#8594; <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Use padding</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129513;</span>
Slide 5: Bank Conflicts</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128313;</span>
Explanation</b></p>

<p class=MsoNormal>Bank conflicts arise when threads within a warp access
different addresses that map to the same memory bank. The hardware serializes
these accesses, reducing parallelism.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>1-D array access &#8594; add one padding column to make
     index non-conflicting.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129518;</span>
Numerical Example</b></p>

<p class=MsoNormal>If each bank can handle 1 access per cycle and 4 threads hit
same bank &#8594; 4 cycles instead of 1.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Bank conflicts cause <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Serialization</li>
 <li class=MsoNormal>To avoid them <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Change data layout</li>
 <li class=MsoNormal>Banks are <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Striped across addresses</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129513;</span>
Slide 6: Single-Instruction, Multiple-Thread Warp Execution on Fermi</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128313;</span>
Explanation</b></p>

<p class=MsoNormal>A warp (usually 32 threads) executes the same instruction
simultaneously. The Fermi architecture schedules warps — if one warp waits for
data, another is executed to hide latency.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case</b></p>

<p class=MsoNormal>Matrix multiplication &#8594; warps load different matrix
tiles and operate in parallel.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129518;</span>
Numerical Example</b></p>

<p class=MsoNormal>If each warp has 32 threads and an SM runs 48 warps &#8594;
48 × 32 = 1536 threads active per SM.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Warp size on Fermi <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     32</li>
 <li class=MsoNormal>One instruction executed by <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     All threads in warp</li>
 <li class=MsoNormal>Warp scheduling helps <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Hide latency</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129513;</span>
Slide 7: SIMD Execution of Scalar Threads</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128313;</span>
Explanation</b></p>

<p class=MsoNormal>SIMD (Single Instruction Multiple Data) means one
instruction operates on many data items. GPUs implement this through SIMT
(Single Instruction Multiple Threads), where each thread executes its own
scalar operation but lock-stepped within the warp.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Image filtering &#8594; same operation on each pixel.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129518;</span>
Numerical Example</b></p>

<p class=MsoNormal>1 instruction applied to 32 data &#8594; 1× 32 = 32 parallel
operations per cycle.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>SIMD means <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Same instruction multiple data</li>
 <li class=MsoNormal>GPU uses <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     SIMT model</li>
 <li class=MsoNormal>Scalar threads execute <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Independently but lock-step</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129513;</span>
Slide 8: Single Instruction Multiple Threads (SIMT)</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128313;</span>
Explanation</b></p>

<p class=MsoNormal>SIMT is NVIDIA’s execution model where threads in a warp can
be individually active or inactive. Inactive threads don’t change their data
but the same instruction is issued for all.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case</b></p>

<p class=MsoNormal>Conditional operations (“if” statements) in CUDA kernels.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129518;</span>
Numerical Example</b></p>

<p class=MsoNormal>If 16 threads active, 16 inactive &#8594; only 50%
utilization of SIMD lanes.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>SIMT allows <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Thread masking</li>
 <li class=MsoNormal>Inactive threads <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Don’t modify data</li>
 <li class=MsoNormal>Model used in <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     CUDA</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129513;</span>
Slide 9: Problem with Thread Execution</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128313;</span>
Explanation</b></p>

<p class=MsoNormal>When threads in a warp follow different branches (e.g.,
if/else), the GPU must execute each path separately &#8594; serialized
execution &#8594; warp divergence.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case</b></p>

<p class=MsoNormal>If some pixels require extra processing &#8594; divergence
reduces efficiency.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129518;</span>
Numerical Example</b></p>

<p class=MsoNormal>If half threads idle during each branch &#8594; 50% warp
efficiency.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Divergence occurs due to <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Different branches</li>
 <li class=MsoNormal>GPU executes branches <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Sequentially</li>
 <li class=MsoNormal>Result <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Low utilization</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129513;</span>
Slide 10: Impact of Branch Divergence</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128313;</span>
Explanation</b></p>

<p class=MsoNormal>When threads take different paths, half of the SIMD lanes
may be idle. E.g., if 16-wide SIMD, 8 threads in path A and 8 in path B &#8594;
only 8 active at a time &#8594; 50% efficiency.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case</b></p>

<p class=MsoNormal>Conditional shading in graphics kernels.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129518;</span>
Numerical Example</b></p>

<p class=MsoNormal>Utilization = active threads / warp size = 8 / 16 = 0.5
(50%).</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Warp utilization when half idle <span style='font-family:
     "Segoe UI Emoji",sans-serif'>&#9989;</span> 50%</li>
 <li class=MsoNormal>Divergence reduces <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Parallel efficiency</li>
 <li class=MsoNormal>GPUs handle it <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     By serializing paths</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129513;</span>
Slide 11 – Quick Quiz &amp; Slide 12 – Solution</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128313;</span>
Explanation</b></p>

<p class=MsoNormal>These slides test understanding of divergence and memory
concepts (likely matching questions to “Solution” answers).</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Self-check during training / certification on CUDA.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129518;</span>
Numerical Example</b></p>

<p class=MsoNormal>If profiling shows warp efficiency = 60%, and GPU has 1024
cores &#8594; &#8776; 614 cores active (0.6 × 1024).</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Warp size <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     32</li>
 <li class=MsoNormal>Divergence caused by <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Branching</li>
 <li class=MsoNormal>To reduce divergence <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Reorder data</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129513;</span>
Slide 13: Solutions to Branch Divergence</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128313;</span>
Explanation</b></p>

<p class=MsoNormal>Strategies: profile branches, replace small branches with
branchless code, group similar data, use persistent threads, and warp
intrinsics. Goal &#8594; keep warps executing same path.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case</b></p>

<p class=MsoNormal>Sort particles by type before processing different physics
routines.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129518;</span>
Numerical Example</b></p>

<p class=MsoNormal>Grouping reduces divergence from 50% &#8594; 90% efficiency
= 1.8× speed-up.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Branchless transforms <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Reduce divergence</li>
 <li class=MsoNormal>Reordering data <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Groups similar branches</li>
 <li class=MsoNormal>Warp intrinsics <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Optimize within warp</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129513;</span>
Slide 14: Persistent Threads Idea</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128313;</span>
Explanation</b></p>

<p class=MsoNormal>Instead of launching millions of short threads, launch a few
that stay alive and pull new work from a global queue (using atomic
operations). Improves load balancing.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case</b></p>

<p class=MsoNormal>Ray tracing, task queues in Monte-Carlo simulations.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129518;</span>
Numerical Example</b></p>

<p class=MsoNormal>If 10 SMs × 1 block each, each block fetches work until 1000
tasks done &#8594; balanced utilization &#8776; 100%.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Persistent threads <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Stay alive through computation</li>
 <li class=MsoNormal>They pull work from <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Global queue</li>
 <li class=MsoNormal>Main benefit <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Better load balancing</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129513;</span>
Slide 15: Warp Divergence</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128313;</span>
Explanation</b></p>

<p class=MsoNormal>When threads within the same warp follow different paths,
each path executes separately. Global divergence = different warps take
different paths &#8594; fine; warp divergence = within a warp &#8594; bad for
performance.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case</b></p>

<p class=MsoNormal>Re-organize data so each warp processes similar data &#8594;
fewer divergent branches.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129518;</span>
Numerical Example</b></p>

<p class=MsoNormal>If two warps each split 50–50, effective utilization &#8776;
50%.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Warp divergence <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Different threads in same warp take different paths</li>
 <li class=MsoNormal>Best solution <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Group data per warp</li>
 <li class=MsoNormal>Divergence reduces <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Throughput</li>
</ol>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129513;</span>
Slide 16 – 17: Example Code &amp; Warp-Aligned Branches</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128313;</span>
Explanation</b></p>

<p class=MsoNormal>Code example shows if(data[i] &gt; 0) &#8594; branch. If
threads in same warp face mixed data, warp divergence occurs. To avoid &#8594;
reorder data so each warp handles similar values.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case</b></p>

<p class=MsoNormal>In image segmentation, group bright pixels and dark pixels
so each warp processes a similar region.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#129518;</span>
Numerical Example</b></p>

<p class=MsoNormal>64 threads = 2 warps. If each warp handles one type of
branch (A or B) &#8594; 100% utilization; if mixed &#8594; 50%.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Divergence depends on <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Data distribution</li>
 <li class=MsoNormal>Warp-aligned branches <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     Improve efficiency</li>
 <li class=MsoNormal>Ideal warp <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     All threads take same path</li>
</ol>

<p class=MsoNormal>&nbsp;</p>

<div style='border:none;border-bottom:double windowtext 2.25pt;padding:0in 0in 1.0pt 0in'>

<p class=MsoNormal style='border:none;padding:0in'>&nbsp;</p>

</div>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal><b>Slide 1: GPU Programming - Lecture 13</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
This is just the title slide, introducing the topic for this lecture, which
will focus on GPU Memory Hierarchy and Thread Execution.</p>

<p class=MsoNormal><b>MCQ:</b><br>
What is the main topic of this lecture?<br>
a) CPU Architecture<br>
b) GPU Memory Hierarchy and Thread Execution<br>
c) Python Programming<br>
<b>Answer:</b>&nbsp;b) GPU Memory Hierarchy and Thread Execution</p>

<p class=MsoNormal><b>Use Case:</b><br>
Understanding this topic is crucial for writing efficient code for tasks like
image processing or scientific simulations on a GPU, as it teaches you how to
manage data for maximum speed.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 2: Memory Hierarchy</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
Think of GPU memory like a pyramid. At the top, you have the fastest but
smallest memory (Registers and Shared Memory), and at the bottom, you have the
slowest but largest memory (Global Memory).</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Shared Memory:</b>&nbsp;Very fast memory shared by all
     threads in a block. It's like a small, shared whiteboard for a team.</li>
 <li class=MsoNormal><b>Local Memory:</b>&nbsp;A thread's private memory, but
     it's actually stored in slow global memory. It's like a personal notebook
     that you have to fetch from a faraway library.</li>
 <li class=MsoNormal><b>Global/Constant/Texture Memory:</b>&nbsp;Large, slow
     memory accessible by all threads and the CPU. It's the main library of
     information.</li>
</ul>

<p class=MsoNormal><b>MCQ:</b><br>
Which GPU memory is fastest but only accessible by threads within the same
block?<br>
a) Global Memory<br>
b) Shared Memory<br>
c) Local Memory<br>
<b>Answer:</b>&nbsp;b) Shared Memory</p>

<p class=MsoNormal><b>Use Case:</b><br>
When performing a matrix multiplication, loading a tile of the matrix into
shared memory allows all threads in a block to access it quickly, drastically
speeding up the calculation.</p>

<p class=MsoNormal><b>Numerical:</b><br>
If accessing global memory takes 400 cycles and accessing shared memory takes
10 cycles, what is the speedup when using shared memory?<br>
<b>Answer:</b>&nbsp;Speedup = 400 / 10 = 40x faster.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 3: GPU Memory Hierarchy (Detailed)</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
This slide dives deeper into the memory structure.</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Global Memory:</b>&nbsp;The main, large pool of memory.
     It has an L2 cache shared by all Streaming Multiprocessors (SMs).</li>
 <li class=MsoNormal><b>L1 Cache/Shared Memory:</b>&nbsp;Each SM has its own
     fast L1 cache and a portion of programmable shared memory.</li>
 <li class=MsoNormal><b>Registers:</b>&nbsp;The fastest memory, dedicated to
     each thread for storing its private variables.</li>
</ul>

<p class=MsoNormal><b>MCQ:</b><br>
What is the lifetime of a variable stored in a GPU register?<br>
a) The lifetime of the entire program<br>
b) The lifetime of the thread that owns it<br>
c) The lifetime of the block that owns it<br>
<b>Answer:</b>&nbsp;b) The lifetime of the thread that owns it</p>

<p class=MsoNormal><b>Use Case:</b><br>
In a physics simulation, each thread calculates the position of a single
particle. The thread uses registers to hold the particle's current coordinates
and velocity for ultra-fast computation.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 4: Shared Memory Architecture</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
Shared memory is organized into &quot;banks&quot; (32 of them) to allow many
threads to access data simultaneously without a traffic jam.</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Cache Line:</b>&nbsp;A 128-byte chunk of data.</li>
 <li class=MsoNormal><b>Bank:</b>&nbsp;Each cache line is divided into 32
     banks, each 4 bytes wide.</li>
 <li class=MsoNormal><b>Ideal Access:</b>&nbsp;If 32 threads each read from a
     different bank, all reads happen in one cycle. It's like 32 checkout
     counters, each serving one customer.</li>
</ul>

<p class=MsoNormal><b>MCQ:</b><br>
How many memory banks is a typical shared memory divided into?<br>
a) 16<br>
b) 32<br>
c) 64<br>
<b>Answer:</b>&nbsp;b) 32</p>

<p class=MsoNormal><b>Use Case:</b><br>
When performing a blur filter on an image, threads in a warp can load adjacent
pixels from shared memory in a single cycle if the pixels are stored in
different banks.</p>

<p class=MsoNormal><b>Numerical:</b><br>
A shared memory cache line is 128 bytes. If each bank is 4 bytes, how many
banks are there?<br>
<b>Answer:</b>&nbsp;128 bytes / 4 bytes per bank = 32 banks.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 5: Bank Conflicts</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
A bank conflict is the &quot;traffic jam&quot; we want to avoid. It happens
when two or more threads in the same warp try to read from or write to
the&nbsp;<b>same memory bank</b>&nbsp;at the same time. When this happens,
their accesses have to be done one after the other (serialized), which slows
everything down.</p>

<p class=MsoNormal><b>MCQ:</b><br>
What causes a shared memory bank conflict?<br>
a) Threads accessing global memory<br>
b) Multiple threads in a warp accessing different banks<br>
c) Multiple threads in a warp accessing the same bank simultaneously<br>
<b>Answer:</b>&nbsp;c) Multiple threads in a warp accessing the same bank
simultaneously</p>

<p class=MsoNormal><b>Use Case:</b><br>
In a matrix transpose operation, if threads access data in a column-major order
from a row-major stored matrix, they will likely all hit the same bank, causing
severe bank conflicts and reducing performance.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 6 &amp; 7: SIMD Warp Execution</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
GPUs don't run threads one by one. They group them into &quot;warps&quot;
(e.g., 32 threads). A warp is executed in a lock-step manner, like a Single
Instruction, Multiple Data (SIMD) unit. All threads in a warp execute the same
instruction at the same time, but on different data. The warp scheduler manages
multiple warps to keep the GPU busy.</p>

<p class=MsoNormal><b>MCQ:</b><br>
What is a &quot;warp&quot; in GPU programming?<br>
a) A type of global memory<br>
b) A group of threads executed simultaneously in a SIMD fashion<br>
c) A synchronization primitive<br>
<b>Answer:</b>&nbsp;b) A group of threads executed simultaneously in a SIMD
fashion</p>

<p class=MsoNormal><b>Use Case:</b><br>
Adding two large vectors. Warp 0 might add elements 0-31, Warp 1 adds elements
32-63, and so on. All threads in a warp perform the same &quot;add&quot;
instruction simultaneously.</p>

<p class=MsoNormal><b>Numerical:</b><br>
If a GPU has a warp size of 32 and you launch 1024 threads, how many warps are
created?<br>
<b>Answer:</b>&nbsp;1024 threads / 32 threads/warp = 32 warps.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 8, 9, 10: Branch Divergence</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
This is a major performance problem. When threads within the&nbsp;<b>same warp</b>&nbsp;take
different paths in an&nbsp;if-else&nbsp;statement, branch divergence occurs.
The GPU cannot execute both paths at once. It must execute one path for the
active threads while the others wait, and then execute the other path. This
leads to poor utilization of the GPU's compute power.</p>

<p class=MsoNormal><b>MCQ:</b><br>
What is the primary performance issue caused by branch divergence?<br>
a) Increased global memory usage<br>
b) Serialization of execution within a warp, leaving some threads idle<br>
c) Bank conflicts in shared memory<br>
<b>Answer:</b>&nbsp;b) Serialization of execution within a warp, leaving some
threads idle</p>

<p class=MsoNormal><b>Use Case:</b><br>
A kernel that applies a threshold to an image:&nbsp;if (pixel &gt; 128) pixel =
255; else pixel = 0;&nbsp;If the bright and dark pixels are randomly mixed
within a warp, divergence will occur.</p>

<p class=MsoNormal><b>Numerical:</b><br>
In a warp of 32 threads, if 8 threads take the 'if' path and 24 take the 'else'
path, what is the SIMD lane utilization during each serialized execution?<br>
<b>Answer:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>When executing the 'if' path: 8/32 = 25% utilization.</li>
 <li class=MsoNormal>When executing the 'else' path: 24/32 = 75% utilization.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 11 &amp; 12: Quick Quiz (Thread Utilization)</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
This emphasizes the importance of choosing a thread block size that is a
multiple of the warp size (32) to avoid having partially filled warps, which
waste resources.</p>

<p class=MsoNormal><b>MCQ:</b><br>
For best GPU utilization, what should the size of a thread block ideally be?<br>
a) A prime number<br>
b) A multiple of the warp size (e.g., 32, 64, 128, etc.)<br>
c) As large as possible<br>
<b>Answer:</b>&nbsp;b) A multiple of the warp size</p>

<p class=MsoNormal><b>Numerical (From Slide):</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Scenario 1:</b>&nbsp;10 blocks of 100 threads. Warp
     size=32.</li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Threads per block: 100.</li>
  <li class=MsoNormal>Warps per block: 3 full warps (96 threads) + 1 warp with
      only 4 active threads.</li>
  <li class=MsoNormal>Utilization = (100 / (4 * 32)) = 100 / 128 =&nbsp;<b>78.125%</b>.</li>
 </ul>
 <li class=MsoNormal><b>Scenario 2:</b>&nbsp;8 blocks of 128 threads.</li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Threads per block: 128.</li>
  <li class=MsoNormal>Warps per block: 4 full warps (128 threads).</li>
  <li class=MsoNormal>Utilization = (128 / (4 * 32)) = 128 / 128 =&nbsp;<b>100%</b>&nbsp;for
      full blocks.</li>
  <li class=MsoNormal>The last block might be incomplete if N=1000, but
      overall, Scenario 2 is better.</li>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 13: Solutions to Branch Divergence</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
We can fix or reduce branch divergence by:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Profiling:</b>&nbsp;Finding where it happens.</li>
 <li class=MsoNormal><b>Branchless Code:</b>&nbsp;Using math tricks to
     avoid&nbsp;if&nbsp;statements.</li>
 <li class=MsoNormal><b>Data Reordering:</b>&nbsp;Sorting or grouping data so
     that all threads in a warp take the same branch.</li>
 <li class=MsoNormal><b>Persistent Threads:</b>&nbsp;Using a small set of
     long-running threads that fetch work from a queue.</li>
 <li class=MsoNormal><b>Warp Intrinsics:</b>&nbsp;Using special functions to
     manage threads at the warp level.</li>
</ol>

<p class=MsoNormal><b>MCQ:</b><br>
Which technique can often provide a &quot;large speedup&quot; by eliminating
most branch divergence?<br>
a) Using more registers<br>
b) Reordering the input data<br>
c) Increasing block size<br>
<b>Answer:</b>&nbsp;b) Reordering the input data</p>

<p class=MsoNormal><b>Use Case:</b><br>
In a ray tracer, instead of having each thread check if a ray hits a different
random object, you can sort the rays by the object they are most likely to hit.
This ensures all threads in a warp are checking against the same object,
avoiding divergence.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 14: Persistent Threads</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
Instead of launching a massive number of threads that die quickly, you launch a
small, fixed number of threads that stay alive (&quot;persist&quot;). These
threads continuously grab new pieces of work from a global queue until all work
is done. This is great for balancing workloads where tasks take different
amounts of time.</p>

<p class=MsoNormal><b>MCQ:</b><br>
What is the main advantage of the persistent threads model?<br>
a) It uses less shared memory<br>
b) It dynamically balances workload and keeps SMs busy<br>
c) It completely eliminates branch divergence<br>
<b>Answer:</b>&nbsp;b) It dynamically balances workload and keeps SMs busy</p>

<p class=MsoNormal><b>Use Case:</b><br>
Processing a list of tasks with unpredictable completion times, like checking
different URLs for a web crawler or simulating particles with complex, varying
interactions.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 15, 16, 17: Warp Divergence &amp; Ideal Data Layout</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
This reinforces the concept of divergence and shows the ideal scenario. The key
takeaway is:&nbsp;<b>You can have branching in your code, but you should try to
organize your data and threads so that all threads within a single warp take
the same branch.</b>&nbsp;This is called keeping the branching
&quot;warp-aligned.&quot;</p>

<p class=MsoNormal><b>MCQ:</b><br>
How can you prevent warp divergence in a kernel that has an&nbsp;if-else&nbsp;statement?<br>
a) By using smaller data types<br>
b) By organizing data so that all threads in a warp satisfy the same condition<br>
c) By using more thread blocks<br>
<b>Answer:</b>&nbsp;b) By organizing data so that all threads in a warp satisfy
the same condition</p>

<p class=MsoNormal><b>Use Case:</b><br>
In a database query (WHERE salary &gt; 50000), if you first sort the data
by&nbsp;salary, then threads in a warp will all be processing high-salary or
low-salary records together, avoiding divergence during the conditional check.</p>

<p class=MsoNormal><b>Numerical:</b><br>
You have 64 threads (2 warps) and a condition. In the bad case, each warp is
50% A and 50% B. In the good case, Warp 0 is 100% A and Warp 1 is 100% B. How
many execution steps are needed in each case?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Bad Case:</b>&nbsp;2 steps per warp * 2 warps = 4 total
     execution steps (serialized per warp).</li>
 <li class=MsoNormal><b>Good Case:</b>&nbsp;1 step per warp = 2 total execution
     steps (can be done in parallel). This is a 2x speedup.</li>
</ul>

<p class=MsoNormal>&nbsp;</p>

<div style='border:none;border-bottom:double windowtext 2.25pt;padding:0in 0in 1.0pt 0in'>

<p class=MsoNormal style='border:none;padding:0in'>&nbsp;</p>

</div>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>&nbsp;</li>
</ul>

<p class=MsoNormal>This is an analysis of the key concepts from your GPU
Programming lecture slides (Lecture-13).</p>

<p class=MsoNormal>I have broken down the content into three main
sections:&nbsp;<b>Memory Hierarchy</b>,&nbsp;<b>Shared Memory and Bank
Conflicts</b>, and&nbsp;<b>Warp Divergence</b>, providing the requested
explanation, MCQs, use cases, and a numerical/conceptual problem for each
concept.</p>

<p class=MsoNormal><b>1. GPU Memory Hierarchy (Slides 2 &amp; 3)</b></p>

<p class=MsoNormal><b>Easy Language Explanation</b></p>

<p class=MsoNormal>A GPU has a complex set of storage spaces, like a highly
tiered system, designed to get data to the computing cores as fast as possible.</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Registers:</b>&nbsp;The fastest memory, located right
     next to each core. It is private to a single thread and holds temporary
     variables.</li>
 <li class=MsoNormal><b>Shared Memory (SMEM):</b>&nbsp;Very fast on-chip memory
     that is shared by all threads within one&nbsp;<i>thread block</i>. It’s
     user-managed (programmable) and is the main way for threads to quickly
     share and communicate data.</li>
 <li class=MsoNormal><b>Local Memory:</b>&nbsp;A concept used when a thread
     runs out of Registers; it &quot;spills&quot; data to slower off-chip
     memory.</li>
 <li class=MsoNormal><b>Global Memory:</b>&nbsp;The largest, main memory (like
     the VRAM on your GPU card). It is accessible by&nbsp;<b>all</b>&nbsp;threads
     on the GPU and the CPU, but it is the slowest.</li>
 <li class=MsoNormal><b>Caches (L1/L2):</b>&nbsp;Hardware-managed fast storage
     to temporarily hold frequently used data, which helps reduce the need to
     go all the way to Global Memory.</li>
</ul>

<p class=MsoNormal><b>Multiple-Choice Questions (MCQs)</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Question:</b>&nbsp;Which memory space is programmable
     (like a software-managed cache) and is used for fast communication&nbsp;<i>between
     threads in the same block</i>?</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal>A) Global Memory</li>
  <li class=MsoNormal>B)&nbsp;<b>Shared Memory</b></li>
  <li class=MsoNormal>C) Local Memory</li>
  <li class=MsoNormal>D) Constant Memory</li>
 </ul>
 <li class=MsoNormal><b>Question:</b>&nbsp;Which memory type is accessible by
     all threads across the entire GPU&nbsp;<i>and</i>&nbsp;the CPU?</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal>A) Shared Memory</li>
  <li class=MsoNormal>B) Registers</li>
  <li class=MsoNormal>C) L1 Cache</li>
  <li class=MsoNormal>D)&nbsp;<b>Global Memory</b></li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Matrix Multiplication:</b>&nbsp;Threads load
     sub-matrices from slow Global Memory into fast Shared Memory so that every
     thread can reuse the data multiple times for calculating final results
     without costly re-accesses.</li>
 <li class=MsoNormal><b>Image Processing (Convolution):</b>&nbsp;A block of
     threads loads a small tile of an image into Shared Memory to share
     surrounding pixel data (the &quot;halo&quot;) required for the convolution
     operation, minimizing redundant Global Memory fetches.</li>
 <li class=MsoNormal><b>Data Reduction:</b>&nbsp;Initial partial sums are
     stored in a thread's private&nbsp;<b>Registers</b>, and then those partial
     sums are moved into&nbsp;<b>Shared Memory</b>&nbsp;for the final stage of
     summing within the thread block.</li>
</ol>

<p class=MsoNormal><b>Numerical/Conceptual Problem</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Problem:</b>&nbsp;A GPU Streaming Multiprocessor (SM)
     has 64 KB of configurable on-chip memory. If a kernel is launched that
     allocates 16 KB for L1 Cache, and the rest is dedicated to Shared Memory
     (SMEM), how many 4-byte floating-point numbers can a single thread block's
     Shared Memory hold?</li>
 <li class=MsoNormal><b>Solution:</b></li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal>Shared Memory (SMEM) Size = Total On-chip Memory - L1
      Cache Size</li>
  <li class=MsoNormal>64&nbsp;KB&#8722;16&nbsp;KB=48&nbsp;KB</li>
  <li class=MsoNormal>Size in Bytes:&nbsp;48×1024=49,152&nbsp;bytes</li>
  <li class=MsoNormal>Number of 4-byte floats =&nbsp;49,152&nbsp;bytes/4&nbsp;bytes/float=<b>12</b>,<b>288</b>&nbsp;floats</li>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>2. Shared Memory Architecture &amp; Bank Conflicts (Slide
4)</b></p>

<p class=MsoNormal><b>Easy Language Explanation</b></p>

<p class=MsoNormal>To achieve very high speed, Shared Memory is physically
split into multiple independent memory units called&nbsp;<b>banks</b>. Think of
this: a modern GPU might have 32 banks. If 32 threads in a warp access 32
different banks at the same time, the access is fully parallel and very fast.</p>

<p class=MsoNormal>A&nbsp;<b>bank conflict</b>&nbsp;occurs when multiple
threads in a warp try to access&nbsp;<i>different</i>&nbsp;memory addresses
that happen to fall within the&nbsp;<i>same</i>&nbsp;bank. Since a bank can
only handle one request at a time, the hardware must serialize these
accesses—meaning the requests are executed one after the other. This is a major
performance killer. The goal is to design your memory access pattern so that
threads in a warp access consecutive addresses, which are then interleaved
across different banks (e.g., address 0 in Bank 0, address 4 in Bank 1, address
8 in Bank 2, etc.).</p>

<p class=MsoNormal><b>Multiple-Choice Questions (MCQs)</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Question:</b>&nbsp;What is a&nbsp;<b>bank conflict</b>?</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal>A) When all threads in a warp access the exact same
      address in a bank (This is an efficient broadcast).</li>
  <li class=MsoNormal>B)&nbsp;<b>When multiple threads in a warp try to access
      different memory addresses that map to the same memory bank.</b></li>
  <li class=MsoNormal>C) When memory transactions fail to coalesce into a
      single request.</li>
  <li class=MsoNormal>D) When a thread accesses Global Memory instead of Shared
      Memory.</li>
 </ul>
 <li class=MsoNormal><b>Question:</b>&nbsp;A GPU has 32 shared memory banks. A
     warp of 32 threads performs a load. For maximum bandwidth (no conflicts),
     each thread should access an address that is interleaved by how many bytes
     relative to its neighbor? (Assume 4-byte word size).</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal>A) 1 byte</li>
  <li class=MsoNormal>B)&nbsp;<b>4 bytes</b></li>
  <li class=MsoNormal>C) 32 bytes</li>
  <li class=MsoNormal>D) 128 bytes</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Optimizing Matrix Transpose:</b>&nbsp;A straightforward
     matrix transpose kernel often leads to every thread in a warp accessing
     the same bank repeatedly. This is fixed by&nbsp;<b>padding</b>&nbsp;the
     shared memory array (adding an unused column) or using a technique
     called&nbsp;<b>swizzling</b>&nbsp;to intentionally map conflicting
     addresses to different banks.</li>
 <li class=MsoNormal><b>Strided Access Patterns:</b>&nbsp;In any algorithm
     where threads access shared memory with a stride (e.g.,&nbsp;shared_array[threadIdx.x
     * Stride]), the developer must ensure the stride is&nbsp;<b>not</b>&nbsp;a
     multiple of the number of banks to prevent conflicts.</li>
</ol>

<p class=MsoNormal><b>Numerical/Conceptual Problem</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Problem:</b>&nbsp;A GPU has 32 banks. A warp of 32
     threads is performing a read. Thread&nbsp;<i>T</i>&nbsp;accesses the
     4-byte integer at address&nbsp;SharedMem[T * 33]. Determine the bank
     conflict factor (how many transactions are required per read instruction).</li>
 <li class=MsoNormal><b>Solution:</b></li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal>Addresses map to banks:&nbsp;<i>Bank</i>=(Byte&nbsp;Address)/4(mod32).</li>
  <li class=MsoNormal>For thread&nbsp;<i>T</i>, the word index is&nbsp;<i>T</i>×33.</li>
  <li class=MsoNormal>The bank index is&nbsp;(<i>T</i>×33)(mod32).</li>
  <li class=MsoNormal>Since&nbsp;33(mod32)=1, the bank index simplifies
      to&nbsp;<i>T</i>(mod32).</li>
  <li class=MsoNormal>Thread 0 accesses Bank 0, Thread 1 accesses Bank 1, ...,
      Thread 31 accesses Bank 31.</li>
  <li class=MsoNormal>Since all threads access a unique bank, there are&nbsp;<b>no
      bank conflicts</b>. The conflict factor is&nbsp;<b>1 transaction</b>.</li>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>3. Warp Divergence (Slides 15 &amp; 16)</b></p>

<p class=MsoNormal><b>Easy Language Explanation</b></p>

<p class=MsoNormal>The GPU uses a&nbsp;<b>S</b>ingle&nbsp;<b>I</b>nstruction,&nbsp;<b>M</b>ultiple&nbsp;<b>T</b>hreads
(<b>SIMT</b>) execution model, where threads in a group (a&nbsp;<b>warp</b>&nbsp;of
32 threads) are forced to run the same instruction at the same time.</p>

<p class=MsoNormal><b>Warp Divergence</b>&nbsp;is what happens when
threads&nbsp;<i>within the same warp</i>&nbsp;take different paths, typically
because of an&nbsp;if-else&nbsp;statement or a loop where the condition is
data-dependent.</p>

<p class=MsoNormal>When divergence occurs, the GPU has to run each separate
path sequentially:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>All threads taking Path A execute while the others are
     disabled (masked off).</li>
 <li class=MsoNormal>All threads taking Path B execute while the others are
     disabled.</li>
</ol>

<p class=MsoNormal>Because the paths are executed one after the other, the
threads are no longer truly parallel, and performance drops significantly. The
solution is to organize threads so that all 32 threads in a warp follow the
same control flow path.</p>

<p class=MsoNormal><b>Multiple-Choice Questions (MCQs)</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Question:</b>&nbsp;What is the fundamental unit of
     scheduling and execution in NVIDIA's SIMT architecture?</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal>A) Thread Block</li>
  <li class=MsoNormal>B)&nbsp;<b>Warp</b>&nbsp;(32 threads)</li>
  <li class=MsoNormal>C) Streaming Multiprocessor (SM)</li>
  <li class=MsoNormal>D) Grid</li>
 </ul>
 <li class=MsoNormal><b>Question:</b>&nbsp;What is the direct consequence of
     warp divergence on a program's execution time?</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal>A) It increases the number of required register spills.</li>
  <li class=MsoNormal>B) It causes an increase in Global Memory latency.</li>
  <li class=MsoNormal>C)&nbsp;<b>It serializes the execution of the divergent
      code paths, increasing the total execution time.</b></li>
  <li class=MsoNormal>D) It causes an automatic re-launch of the kernel with
      smaller blocks.</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Sparse Data Structures:</b>&nbsp;Processing a sparse
     matrix where an&nbsp;if (value != 0)&nbsp;check is used. If the warp
     contains threads looking at both zero and non-zero values, divergence
     occurs.</li>
 <li class=MsoNormal><b>Boundary Checks:</b>&nbsp;In loops or array accesses, a
     thread checking an index against a boundary (e.g.,&nbsp;if (i &lt;
     array_size)) can cause divergence if the threads at the boundary line are
     split between taking the&nbsp;if&nbsp;and skipping it.</li>
 <li class=MsoNormal><b>Ray Tracing/Path Tracing:</b>&nbsp;When a ray hits an
     object, its computation path changes. If rays within the same warp hit
     different types of materials or miss entirely, the warp diverges.</li>
</ol>

<p class=MsoNormal><b>Numerical/Conceptual Problem</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Problem:</b>&nbsp;A warp of 32 threads encounters
     an&nbsp;if-else&nbsp;statement. The&nbsp;if&nbsp;path requires 120 clock
     cycles to execute, and 16 threads take it. The&nbsp;else&nbsp;path
     requires 80 clock cycles to execute, and the remaining 16 threads take it.
     Assuming no other stalls, what is the total execution time for the warp?</li>
 <li class=MsoNormal><b>Solution:</b></li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal>Due to warp divergence, the two paths must be executed
      sequentially (serialized).</li>
  <li class=MsoNormal>Total Execution Time = (Time for&nbsp;if&nbsp;path) +
      (Time for&nbsp;else&nbsp;path)</li>
  <li class=MsoNormal>Total Execution Time =&nbsp;120&nbsp;cycles+80&nbsp;cycles=<b>200</b>&nbsp;clock&nbsp;cycles</li>
 </ul>
</ul>

<p class=MsoNormal>You can learn more about how CUDA handles memory access and
bank conflicts in this video:&nbsp;<a
href="https://www.youtube.com/watch?v=CZgM3DEBplE" target="_blank">NVIDIA CUDA
Tutorial 9: Bank Conflicts</a>.</p>

<p class=MsoNormal>&nbsp;</p>

<div style='border:none;border-bottom:double windowtext 2.25pt;padding:0in 0in 1.0pt 0in'>

<p class=MsoNormal style='border:none;padding:0in'>&nbsp;</p>

</div>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal><b>GPU Programming Lecture 13 - Complete Guide</b></p>

<p class=MsoNormal>I'll break down each slide with explanations, MCQs, use
cases, and numerical problems.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 2: Memory Hierarchy</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal>Think of GPU memory like different types of storage in your
house:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Shared memory</b> = Kitchen counter (very fast, close,
     but limited space)</li>
 <li class=MsoNormal><b>Local memory</b> = Your pocket (tiny, just for you)</li>
 <li class=MsoNormal><b>Global memory</b> = Garage (huge but far away, slower
     to access)</li>
 <li class=MsoNormal><b>Constant/Texture memory</b> = Cookbook shelf
     (read-only, optimized for specific access patterns)</li>
</ul>

<p class=MsoNormal>The closer the memory, the faster you can access it!</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Which memory is the fastest in GPU?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) Global memory</li>
  <li class=MsoNormal>B) Shared memory &#10003;</li>
  <li class=MsoNormal>C) Constant memory</li>
  <li class=MsoNormal>D) Texture memory</li>
 </ul>
 <li class=MsoNormal><b>Which memory is accessible to both CPU and GPU?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) Shared memory</li>
  <li class=MsoNormal>B) Local memory</li>
  <li class=MsoNormal>C) Global memory &#10003;</li>
  <li class=MsoNormal>D) None of the above</li>
 </ul>
 <li class=MsoNormal><b>What is the main limitation of local memory?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) It's too slow</li>
  <li class=MsoNormal>B) It's not sufficient in size &#10003;</li>
  <li class=MsoNormal>C) It's not accessible to threads</li>
  <li class=MsoNormal>D) It's read-only</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Shared memory</b>: Storing tile data in matrix
     multiplication</li>
 <li class=MsoNormal><b>Global memory</b>: Storing input/output arrays for
     image processing</li>
 <li class=MsoNormal><b>Constant memory</b>: Storing filter coefficients in
     convolution</li>
 <li class=MsoNormal><b>Texture memory</b>: Storing images for graphics
     rendering</li>
</ul>

<p class=MsoNormal><b>Numerical Problem</b></p>

<p class=MsoNormal><b>Problem</b>: A kernel needs to process 1024 elements.
Each element requires 3 reads from global memory (100 cycles each) or can be
cached in shared memory (5 cycles per access). Calculate the speedup if using
shared memory.</p>

<p class=MsoNormal><b>Solution</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Global memory time: 1024 × 3 × 100 = 307,200 cycles</li>
 <li class=MsoNormal>Shared memory time: 1024 × 3 × 5 = 15,360 cycles</li>
 <li class=MsoNormal>Speedup = 307,200 / 15,360 = <b>20x faster</b></li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 4: Shared Memory Architecture</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal>Shared memory is divided into &quot;banks&quot; (like bank
teller windows). Each bank can serve one customer at a time. If multiple
threads try to access the same bank simultaneously, they must wait in line
(serialized access), causing delays called &quot;bank conflicts.&quot;</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>What happens when multiple threads access the same
     bank?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) They access simultaneously</li>
  <li class=MsoNormal>B) Their accesses are serialized &#10003;</li>
  <li class=MsoNormal>C) The program crashes</li>
  <li class=MsoNormal>D) Data is corrupted</li>
 </ul>
 <li class=MsoNormal><b>How are bank addresses organized?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) Randomly</li>
  <li class=MsoNormal>B) Striped across banks &#10003;</li>
  <li class=MsoNormal>C) In clusters</li>
  <li class=MsoNormal>D) Sequential</li>
 </ul>
 <li class=MsoNormal><b>What is a bank conflict?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) Memory corruption</li>
  <li class=MsoNormal>B) Multiple threads accessing the same bank
      simultaneously &#10003;</li>
  <li class=MsoNormal>C) Running out of memory</li>
  <li class=MsoNormal>D) Cache miss</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Matrix transpose</b>: Padding arrays to avoid bank
     conflicts</li>
 <li class=MsoNormal><b>Reduction operations</b>: Careful indexing to minimize
     conflicts</li>
 <li class=MsoNormal><b>Stencil computations</b>: Arranging shared memory
     access patterns</li>
</ul>

<p class=MsoNormal><b>Numerical Problem</b></p>

<p class=MsoNormal><b>Problem</b>: A GPU has 32 banks. If 32 threads access
shared memory with addresses [0, 32, 64, 96, ...], how many bank conflicts
occur?</p>

<p class=MsoNormal><b>Solution</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Each address = bank_id % 32</li>
 <li class=MsoNormal>Thread 0 &#8594; address 0 &#8594; bank 0</li>
 <li class=MsoNormal>Thread 1 &#8594; address 32 &#8594; bank 0</li>
 <li class=MsoNormal>All 32 threads &#8594; bank 0</li>
 <li class=MsoNormal><b>Result</b>: 31-way bank conflict (32 accesses
     serialized)</li>
 <li class=MsoNormal><b>Time penalty</b>: 32x slower</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 6-8: SIMT (Single Instruction Multiple Threads)</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal>Imagine a dance instructor (GPU) teaching 32 dancers
(threads in a warp). The instructor calls out ONE move at a time, and all
dancers perform it together. Some dancers can sit out if the move doesn't apply
to them, but the instructor still takes the same time to call the move.</p>

<p class=MsoNormal>SIMT = All threads execute the same instruction, but can
work on different data.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>What does SIMT stand for?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) Single Instruction Multiple Tasks</li>
  <li class=MsoNormal>B) Single Instruction Multiple Threads &#10003;</li>
  <li class=MsoNormal>C) Sequential Instruction Multiple Threads</li>
  <li class=MsoNormal>D) Synchronized Instruction Multiple Tasks</li>
 </ul>
 <li class=MsoNormal><b>In SIMT, what happens to inactive threads?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) They are deleted</li>
  <li class=MsoNormal>B) They keep their local data unchanged &#10003;</li>
  <li class=MsoNormal>C) They execute different instructions</li>
  <li class=MsoNormal>D) They cause errors</li>
 </ul>
 <li class=MsoNormal><b>What is a warp?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) A single thread</li>
  <li class=MsoNormal>B) A group of threads executing together &#10003;</li>
  <li class=MsoNormal>C) A memory location</li>
  <li class=MsoNormal>D) A GPU core</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Image processing</b>: Apply same filter to all pixels</li>
 <li class=MsoNormal><b>Vector addition</b>: Add corresponding elements</li>
 <li class=MsoNormal><b>Ray tracing</b>: Test same intersection logic for
     multiple rays</li>
</ul>

<p class=MsoNormal><b>Numerical Problem</b></p>

<p class=MsoNormal><b>Problem</b>: A warp has 32 threads. An instruction takes
4 cycles. If only 16 threads are active, how much time is wasted?</p>

<p class=MsoNormal><b>Solution</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Total time = 4 cycles (same regardless of active threads)</li>
 <li class=MsoNormal>Useful work = 16 threads</li>
 <li class=MsoNormal>Wasted capacity = 16 threads × 4 cycles = 64 thread-cycles
     wasted</li>
 <li class=MsoNormal><b>Efficiency</b>: 50%</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 9-10: Branch Divergence Impact</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal>Imagine 16 people need to go through security. 8 go through
the metal detector (Path A) and 8 through the full-body scanner (Path B). Since
there's only ONE security officer giving instructions, they must process all
metal detector people first, while scanner people wait, then vice versa. This
doubles the time!</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>What causes branch divergence?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) Too many threads</li>
  <li class=MsoNormal>B) Different threads taking different execution paths
      &#10003;</li>
  <li class=MsoNormal>C) Memory errors</li>
  <li class=MsoNormal>D) Slow GPU</li>
 </ul>
 <li class=MsoNormal><b>If 8 out of 32 threads take Path A and 24 take Path B,
     what is the utilization?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) 100%</li>
  <li class=MsoNormal>B) 75% &#10003; (24/32 at best case)</li>
  <li class=MsoNormal>C) 50%</li>
  <li class=MsoNormal>D) 25%</li>
 </ul>
 <li class=MsoNormal><b>How does the GPU handle both paths?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) Executes them simultaneously</li>
  <li class=MsoNormal>B) Serializes them &#10003;</li>
  <li class=MsoNormal>C) Ignores one path</li>
  <li class=MsoNormal>D) Crashes</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Collision detection</b>: Some objects collide, others
     don't</li>
 <li class=MsoNormal><b>Sparse matrix operations</b>: Some elements are zero</li>
 <li class=MsoNormal><b>Particle systems</b>: Different particle types with
     different physics</li>
</ul>

<p class=MsoNormal><b>Numerical Problem</b></p>

<p class=MsoNormal><b>Problem</b>: A warp of 32 threads has a branch where 20
threads execute Path A (100 cycles) and 12 threads execute Path B (80 cycles).
Calculate total execution time and efficiency.</p>

<p class=MsoNormal><b>Solution</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Path A time: 100 cycles (20 threads active, 12 idle)</li>
 <li class=MsoNormal>Path B time: 80 cycles (12 threads active, 20 idle)</li>
 <li class=MsoNormal><b>Total time</b>: 100 + 80 = 180 cycles</li>
 <li class=MsoNormal>Without divergence (best case): max(100, 80) = 100 cycles</li>
 <li class=MsoNormal><b>Efficiency loss</b>: (180 - 100) / 180 = 44% wasted</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 11-12: Quick Quiz Solution</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal>The quiz shows how to calculate warp divergence penalty.
When threads take different paths, execution time = sum of all path times, not
the maximum.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>In the example, how many execution cycles are needed?</b>
     </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) 200</li>
  <li class=MsoNormal>B) 400 &#10003;</li>
  <li class=MsoNormal>C) 300</li>
  <li class=MsoNormal>D) 100</li>
 </ul>
 <li class=MsoNormal><b>What percentage of resources are wasted?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) 25%</li>
  <li class=MsoNormal>B) 50% &#10003;</li>
  <li class=MsoNormal>C) 75%</li>
  <li class=MsoNormal>D) 0%</li>
 </ul>
 <li class=MsoNormal><b>How could we improve this scenario?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) Use more threads</li>
  <li class=MsoNormal>B) Group threads by path &#10003;</li>
  <li class=MsoNormal>C) Use slower memory</li>
  <li class=MsoNormal>D) Reduce warp size</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Analyzing performance bottlenecks</b></li>
 <li class=MsoNormal><b>Optimizing kernel code</b></li>
 <li class=MsoNormal><b>Predicting execution time</b></li>
</ul>

<p class=MsoNormal><b>Numerical Problem</b></p>

<p class=MsoNormal><b>Problem</b>: A kernel has 128 threads (4 warps). In each
warp, 16 threads execute Path A (50 cycles) and 16 execute Path B (30 cycles).
Calculate total time.</p>

<p class=MsoNormal><b>Solution</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Per warp time: 50 + 30 = 80 cycles</li>
 <li class=MsoNormal>Total warps: 4</li>
 <li class=MsoNormal><b>Total time</b>: 80 cycles (warps execute in parallel)</li>
 <li class=MsoNormal>Ideal time: 50 cycles</li>
 <li class=MsoNormal><b>Overhead</b>: 30 cycles per warp (60% penalty)</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 13: Solutions to Branch Divergence</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal><b>5 Strategies:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Profile</b>: Find where branches hurt performance (use
     tools)</li>
 <li class=MsoNormal><b>Branchless transforms</b>: Replace if-else with math
     (like result = condition * valueA + (1-condition) * valueB)</li>
 <li class=MsoNormal><b>Reorder data</b>: Sort inputs so similar cases group
     together</li>
 <li class=MsoNormal><b>Persistent threads</b>: Threads pick work from a queue
     dynamically</li>
 <li class=MsoNormal><b>Warp intrinsics</b>: Use special functions to detect
     and handle divergence</li>
</ol>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Which solution gives the largest speedup?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) Branchless transforms</li>
  <li class=MsoNormal>B) Data reordering &#10003;</li>
  <li class=MsoNormal>C) Profiling</li>
  <li class=MsoNormal>D) Warp intrinsics</li>
 </ul>
 <li class=MsoNormal><b>What is a branchless transform?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) Removing all branches</li>
  <li class=MsoNormal>B) Replacing branches with arithmetic operations &#10003;</li>
  <li class=MsoNormal>C) Using more threads</li>
  <li class=MsoNormal>D) Disabling warps</li>
 </ul>
 <li class=MsoNormal><b>When is data reordering most effective?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) When divergence is random</li>
  <li class=MsoNormal>B) When divergence is data-driven &#10003;</li>
  <li class=MsoNormal>C) When divergence is rare</li>
  <li class=MsoNormal>D) Never effective</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Sorting-based optimization</b>: Sort particles by type
     before processing</li>
 <li class=MsoNormal><b>Bitonic sort</b>: Naturally warp-aligned operations</li>
 <li class=MsoNormal><b>Stream compaction</b>: Remove empty elements before
     processing</li>
</ul>

<p class=MsoNormal><b>Numerical Problem</b></p>

<p class=MsoNormal><b>Problem</b>: Original code has 50% divergence taking 200
cycles. After data reordering, only 10% divergence remains. Estimate new
execution time.</p>

<p class=MsoNormal><b>Solution</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Original: 50% divergence &#8594; assume 1.5x overhead
     &#8594; 200 cycles</li>
 <li class=MsoNormal>Base time without divergence: 200 / 1.5 = 133 cycles</li>
 <li class=MsoNormal>New with 10% divergence: 133 × 1.1 = 146 cycles</li>
 <li class=MsoNormal><b>Speedup</b>: 200 / 146 = <b>1.37x faster</b> (27%
     improvement)</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 14: Persistent Threads</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal>Instead of creating workers for each task and dismissing
them (traditional approach), we hire a fixed number of workers who keep working
until all tasks are done. When a worker finishes a task, they immediately grab
the next one from a shared to-do list.</p>

<p class=MsoNormal><b>Benefits:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>No overhead creating/destroying threads</li>
 <li class=MsoNormal>Better load balancing</li>
 <li class=MsoNormal>All cores stay busy</li>
</ul>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>What characterizes persistent threads?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) They terminate after each task</li>
  <li class=MsoNormal>B) They stay alive throughout computation &#10003;</li>
  <li class=MsoNormal>C) They use less memory</li>
  <li class=MsoNormal>D) They are slower</li>
 </ul>
 <li class=MsoNormal><b>What synchronization primitive is commonly used?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) Mutex</li>
  <li class=MsoNormal>B) atomicAdd &#10003;</li>
  <li class=MsoNormal>C) Barrier</li>
  <li class=MsoNormal>D) Semaphore</li>
 </ul>
 <li class=MsoNormal><b>What is the main advantage?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) Simpler code</li>
  <li class=MsoNormal>B) Better load balancing &#10003;</li>
  <li class=MsoNormal>C) Less memory usage</li>
  <li class=MsoNormal>D) Faster individual threads</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Path tracing</b>: Different rays have different
     complexity</li>
 <li class=MsoNormal><b>Graph algorithms</b>: Nodes have varying numbers of
     neighbors</li>
 <li class=MsoNormal><b>Dynamic workloads</b>: Task complexity varies greatly</li>
</ul>

<p class=MsoNormal><b>Numerical Problem</b></p>

<p class=MsoNormal><b>Problem</b>: 1000 tasks with execution times varying from
10 to 100 cycles. Compare:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Traditional</b>: 100 threads, each assigned 10 tasks
     sequentially</li>
 <li class=MsoNormal><b>Persistent</b>: 100 threads with work queue</li>
</ul>

<p class=MsoNormal><b>Solution</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Traditional (worst case): One thread gets all 100-cycle
     tasks &#8594; 100 × 10 = 1000 cycles</li>
 <li class=MsoNormal>Persistent: Work balanced dynamically &#8594; total work /
     threads = (average 55 cycles × 1000) / 100 = 550 cycles</li>
 <li class=MsoNormal><b>Speedup</b>: 1000 / 550 = <b>1.82x faster</b></li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 15: Warp Divergence Analysis</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal>This slide clarifies the difference:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Branch divergence</b>: Different threads take different
     paths (exists in the code)</li>
 <li class=MsoNormal><b>Warp divergence</b>: Different threads <i>within the
     same warp</i> take different paths (performance problem)</li>
</ul>

<p class=MsoNormal>You can have branch divergence WITHOUT warp divergence if
you organize data smartly!</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>If threads 0-31 take Path A and threads 32-63 take Path
     B, how many warps have divergence?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) 0 &#10003;</li>
  <li class=MsoNormal>B) 1</li>
  <li class=MsoNormal>C) 2</li>
  <li class=MsoNormal>D) 4</li>
 </ul>
 <li class=MsoNormal><b>What is the key to avoiding warp divergence?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) Removing all branches</li>
  <li class=MsoNormal>B) Organizing data so warp threads agree &#10003;</li>
  <li class=MsoNormal>C) Using smaller warps</li>
  <li class=MsoNormal>D) Using more memory</li>
 </ul>
 <li class=MsoNormal><b>In the example, reorganizing data gave what benefit?</b>
     </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) Removed branch divergence</li>
  <li class=MsoNormal>B) Removed warp divergence &#10003;</li>
  <li class=MsoNormal>C) Used less memory</li>
  <li class=MsoNormal>D) Simplified code</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Particle sorting</b>: Sort by type before simulation</li>
 <li class=MsoNormal><b>Culling optimization</b>: Group visible objects
     together</li>
 <li class=MsoNormal><b>LOD rendering</b>: Group objects by detail level</li>
</ul>

<p class=MsoNormal><b>Numerical Problem</b></p>

<p class=MsoNormal><b>Problem</b>: 128 threads, warp size 32. Initially random:
each warp has 50% Path A (20 cycles), 50% Path B (30 cycles). After sorting: 2
warps fully Path A, 2 warps fully Path B. Calculate speedup.</p>

<p class=MsoNormal><b>Solution</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Before sorting</b>: </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Each warp: 20 + 30 = 50 cycles</li>
  <li class=MsoNormal>4 warps: 50 cycles each (parallel)</li>
  <li class=MsoNormal>Total: 50 cycles</li>
 </ul>
 <li class=MsoNormal><b>After sorting</b>: </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Path A warps: 20 cycles</li>
  <li class=MsoNormal>Path B warps: 30 cycles</li>
  <li class=MsoNormal>Total: max(20, 30) = 30 cycles (parallel)</li>
 </ul>
 <li class=MsoNormal><b>Speedup</b>: 50 / 30 = <b>1.67x faster</b></li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 16-17: Example Code with Data Distribution</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal>The code has an if-else branch, but whether it causes warp
divergence depends on the DATA:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Bad data</b>: [+1, -1, +2, -2, +3, -3, ...] &#8594;
     every warp has divergence</li>
 <li class=MsoNormal><b>Good data</b>: [+1, +2, +3, ..., -1, -2, -3, ...]
     &#8594; each warp is uniform</li>
</ul>

<p class=MsoNormal><b>Key insight</b>: Sort or organize your input data to
minimize warp divergence!</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Does this code always cause warp divergence?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) Yes</li>
  <li class=MsoNormal>B) No, it depends on data distribution &#10003;</li>
  <li class=MsoNormal>C) Only on old GPUs</li>
  <li class=MsoNormal>D) Never</li>
 </ul>
 <li class=MsoNormal><b>What preprocessing step can help?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) Sorting data &#10003;</li>
  <li class=MsoNormal>B) Adding more threads</li>
  <li class=MsoNormal>C) Using global memory</li>
  <li class=MsoNormal>D) Reducing data size</li>
 </ul>
 <li class=MsoNormal><b>What's the ideal data pattern?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>A) Random distribution</li>
  <li class=MsoNormal>B) Alternating positive/negative</li>
  <li class=MsoNormal>C) All positive values grouped together &#10003;</li>
  <li class=MsoNormal>D) Doesn't matter</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Image thresholding</b>: Group bright and dark pixels</li>
 <li class=MsoNormal><b>Collision detection</b>: Sort objects by spatial
     location</li>
 <li class=MsoNormal><b>Conditional updates</b>: Partition data by update type</li>
</ul>

<p class=MsoNormal><b>Numerical Problem</b></p>

<p class=MsoNormal><b>Problem</b>: Processing 1024 elements with warp size 32.</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Scenario A</b>: Random data &#8594; average 50%
     divergence per warp</li>
 <li class=MsoNormal><b>Scenario B</b>: Sorted data &#8594; first 16 warps Path
     A only, last 16 warps Path B only</li>
</ul>

<p class=MsoNormal>Path A: 10 cycles, Path B: 15 cycles</p>

<p class=MsoNormal><b>Solution</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Scenario A</b>: </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>Per warp: 10 + 15 = 25 cycles</li>
  <li class=MsoNormal>32 warps total: 25 cycles</li>
  <li class=MsoNormal>Total: 25 cycles</li>
 </ul>
 <li class=MsoNormal><b>Scenario B</b>: </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>16 warps Path A: 10 cycles</li>
  <li class=MsoNormal>16 warps Path B: 15 cycles</li>
  <li class=MsoNormal>All execute in parallel: max(10, 15) = 15 cycles</li>
 </ul>
 <li class=MsoNormal><b>Speedup</b>: 25 / 15 = <b>1.67x faster</b></li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Summary Numerical Challenge</b></p>

<p class=MsoNormal><b>Complex Problem</b>: Design an optimized kernel for
processing 4096 particles:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>2048 particles are Type A (simple physics, 50 cycles)</li>
 <li class=MsoNormal>2048 particles are Type B (complex physics, 150 cycles)</li>
 <li class=MsoNormal>Warp size: 32 threads</li>
 <li class=MsoNormal>GPU has 16 SMs</li>
</ul>

<p class=MsoNormal>Compare three approaches:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Naive</b>: Random order</li>
 <li class=MsoNormal><b>Sorted</b>: Type A first, then Type B</li>
 <li class=MsoNormal><b>Persistent threads</b>: Dynamic work queue</li>
</ol>

<p class=MsoNormal><b>Solution</b>:</p>

<p class=MsoNormal><b>Approach 1 (Naive)</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Each warp likely has 16 Type A + 16 Type B</li>
 <li class=MsoNormal>Time per warp: 50 + 150 = 200 cycles</li>
 <li class=MsoNormal>Total warps: 4096/32 = 128 warps</li>
 <li class=MsoNormal>With 16 SMs: 128/16 = 8 waves</li>
 <li class=MsoNormal><b>Total time</b>: 8 × 200 = 1600 cycles</li>
</ul>

<p class=MsoNormal><b>Approach 2 (Sorted)</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>64 warps Type A: 50 cycles</li>
 <li class=MsoNormal>64 warps Type B: 150 cycles</li>
 <li class=MsoNormal>Wave 1-4: Type A (4 × 50 = 200 cycles)</li>
 <li class=MsoNormal>Wave 5-8: Type B (4 × 150 = 600 cycles)</li>
 <li class=MsoNormal><b>Total time</b>: 200 + 600 = 800 cycles</li>
</ul>

<p class=MsoNormal><b>Approach 3 (Persistent)</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>16 blocks (one per SM), 32 threads each = 512 threads</li>
 <li class=MsoNormal>Total work: (2048 × 50) + (2048 × 150) = 409,600 cycles</li>
 <li class=MsoNormal>Distributed across 512 threads: 409,600 / 512 = 800 cycles</li>
 <li class=MsoNormal>Perfect load balancing</li>
 <li class=MsoNormal><b>Total time</b>: ~800 cycles</li>
</ul>

<p class=MsoNormal><b>Results</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Sorted vs Naive: <b>2x speedup</b></li>
 <li class=MsoNormal>Persistent vs Naive: <b>2x speedup</b></li>
 <li class=MsoNormal>Persistent provides best load balance for varying
     workloads</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal>This comprehensive guide covers all aspects of GPU
programming concepts related to memory hierarchy, SIMT execution, and branch
divergence optimization!</p>

<p class=MsoNormal>&nbsp;</p>

<div style='border:none;border-bottom:double windowtext 2.25pt;padding:0in 0in 1.0pt 0in'>

<p class=MsoNormal style='border:none;padding:0in'>&nbsp;</p>

</div>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>Of course! Here is a detailed breakdown of each slide from
the provided document, including explanations, multiple-choice questions
(MCQs), use cases, and numerical problems, all presented in an
easy-to-understand manner.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 1: Title Slide</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> This is the title page for the
     13th lecture on GPU Programming. It simply introduces the topic of the
     lecture.</li>
 <li class=MsoNormal><b>MCQ:</b> Not applicable for a title slide.</li>
 <li class=MsoNormal><b>Use Case:</b> Not applicable for a title slide.</li>
 <li class=MsoNormal><b>Numerical:</b> Not applicable for a title slide.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 2: Memory Hierarchy</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> Think of a GPU's memory like a
     workbench and a big warehouse.</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal><b>Shared Memory:</b> This is your workbench. It's small,
      but everything on it is super fast to access. Threads working together
      can quickly share data here.</li>
  <li class=MsoNormal><b>Global Memory:</b> This is the big warehouse. It's
      huge, but going to get something from it is much slower than picking it
      up from your workbench.</li>
  <li class=MsoNormal><b>Local Memory:</b> This is like your own pocket. It's
      very small and private to you, but you can't fit much in it.</li>
  <li class=MsoNormal><b>Constant &amp; Texture Memory:</b> These are special
      sections in the warehouse that are optimized for specific types of
      reading (e.g., reading the same value by many threads at once). Both the
      main computer (CPU) and the GPU can see and use these memory types.</li>
 </ul>
 <li class=MsoNormal><b>MCQ:</b> According to the slide, which type of GPU
     memory is described as &quot;much much faster&quot; than global memory? a)
     Local Memory b) Constant Memory c) Shared Memory d) Texture Memory</li>
</ul>

<p class=MsoNormal><b>Answer: c) Shared Memory</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Use Case:</b> In image processing, you might load a
     small 3x3 filter kernel (like a blur filter) into the fast <b>shared
     memory</b>. The large image you're filtering stays in the slower <b>global
     memory</b>. Each thread block can then quickly apply the filter to its
     part of the image using the shared copy, dramatically speeding up the
     process.</li>
 <li class=MsoNormal><b>Numerical:</b> If accessing global memory takes 500
     clock cycles and accessing shared memory takes 5 clock cycles, how many
     times faster is shared memory access compared to global memory access for
     this operation? <b>Answer:</b> 500 / 5 = 100 times faster.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 3: Shared Memory Architecture</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> This slide introduces the idea of
     shared memory. It's a special, high-speed memory area that can be shared
     by all the threads within a single &quot;block.&quot; It's the key for
     threads to cooperate and communicate with each other efficiently.</li>
 <li class=MsoNormal><b>MCQ:</b> What is the primary purpose of shared memory
     in a GPU? a) To store data that needs to be accessed by the CPU. b) To
     allow threads within the same block to communicate and share data quickly.
     c) To act as the main, long-term storage for the GPU. d) To provide a
     private memory space for each individual thread.</li>
</ul>

<p class=MsoNormal><b>Answer: b) To allow threads within the same block to
communicate and share data quickly.</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Use Case:</b> <b>Matrix Multiplication:</b> To multiply
     two large matrices, you can break them into small tiles. Each thread block
     loads one tile from each matrix into fast shared memory. The threads then
     perform the multiplication using these shared tiles, which is much faster
     than repeatedly reading from the slow global memory.</li>
 <li class=MsoNormal><b>Numerical:</b> A thread block has 128 threads. Each
     thread needs to store 4 floating-point numbers (4 bytes each) in shared
     memory for a collaborative calculation. What is the total amount of shared
     memory required by this block? <b>Answer:</b> 128 threads * 4
     numbers/thread * 4 bytes/number = 2048 bytes.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 4: Bank Conflicts</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> Shared memory is split into
     smaller sections called &quot;banks,&quot; like lanes on a highway.</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal>Normally, if different threads access different banks at
      the same time, it's like cars in different lanes—everything happens at
      once (parallel).</li>
  <li class=MsoNormal>A <b>bank conflict</b> happens when multiple threads from
      the same group (a warp) try to access the <i>same bank</i> at the same
      time. This is like multiple cars trying to merge into the same lane at
      once. They have to go one by one (serialized), which slows everything
      down.</li>
 </ul>
 <li class=MsoNormal><b>MCQ:</b> What is a &quot;bank conflict&quot; in the
     context of GPU shared memory? a) When a thread tries to access memory that
     is out of bounds. b) When multiple threads access the same memory bank
     simultaneously, causing serialized access. c) When the shared memory runs
     out of space. d) When the CPU and GPU try to write to the same memory
     location.</li>
</ul>

<p class=MsoNormal><b>Answer: b) When multiple threads access the same memory
bank simultaneously, causing serialized access.</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Use Case:</b> Imagine a parallel reduction where you
     sum up an array. A naive approach might have thread 0 access <b>data[0]</b>,
     thread 1 access <b>data[1]</b>, thread 2 access <b>data[2]</b>, etc. If
     the memory banks are organized such that <b>data[0]</b>, <b>data[32]</b>, <b>data[64]</b>
     are in the same bank, this pattern could cause conflicts. A better
     approach is to have threads access data with a &quot;stride&quot; to avoid
     hitting the same bank.</li>
 <li class=MsoNormal><b>Numerical:</b> Assume a shared memory system has 32
     banks. Bank 0 holds addresses 0, 32, 64...; Bank 1 holds addresses 1, 33,
     65... and so on. A 32-thread warp accesses the following addresses: <b>thread
     0 -&gt; address 0</b>, <b>thread 1 -&gt; address 64</b>, <b>thread 2 -&gt;
     address 2</b>, <b>thread 3 -&gt; address 3</b>. Is there a bank conflict? <b>Answer:</b>
     Yes. Thread 0 accesses address 0 (Bank 0) and Thread 1 accesses address 64
     (also Bank 0). Since two threads are trying to access Bank 0 at the same
     time, a 2-way bank conflict occurs.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 5: Single-Instruction, Multiple-Thread warp
execution on Fermi</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> This is a title slide for a
     section explaining how a specific NVIDIA GPU architecture (Fermi) executes
     instructions. It highlights the &quot;SIMT&quot; model, which we'll cover
     next.</li>
 <li class=MsoNormal><b>MCQ:</b> Not applicable for a title slide.</li>
 <li class=MsoNormal><b>Use Case:</b> Not applicable for a title slide.</li>
 <li class=MsoNormal><b>Numerical:</b> Not applicable for a title slide.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 6: SIMD Execution of Scalar Threads</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> <b>SIMD</b> stands for <b>S</b>ingle
     <b>I</b>nstruction, <b>M</b>ultiple <b>D</b>ata. Imagine a teacher giving
     one command (&quot;Stand up&quot;) to the entire class. The single
     instruction is &quot;stand up,&quot; and it's applied to multiple students
     (the data) all at once. GPUs do the same: they issue one instruction
     (e.g., &quot;add two numbers&quot;) that is executed simultaneously on
     many different data values by many threads.</li>
 <li class=MsoNormal><b>MCQ:</b> What does the acronym SIMD stand for? a)
     Single Instruction, Multiple Devices b) Single Instruction, Multiple Data
     c) Multiple Instructions, Single Data d) Simultaneous Instruction,
     Multiple Datasets</li>
</ul>

<p class=MsoNormal><b>Answer: b) Single Instruction, Multiple Data</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Use Case:</b> <b>Math Operations on Arrays:</b> Adding
     two large arrays, <b>C = A + B</b>. The GPU issues a single
     &quot;ADD&quot; instruction. The hardware then performs the addition <b>C[0]=A[0]+B[0]</b>,
     <b>C[1]=A[1]+B[1]</b>, <b>C[2]=A[2]+B[2]</b>, etc., all at the same time
     on different processing units.</li>
 <li class=MsoNormal><b>Numerical:</b> A GPU SIMD unit can process 8 data
     elements per clock cycle. How many clock cycles are needed to add two
     arrays, each containing 800 elements? <b>Answer:</b> 800 elements / 8
     elements per cycle = 100 clock cycles.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 7: Single Instruction Multiple Threads (SIMT)</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> <b>SIMT</b> is NVIDIA's version
     of SIMD, but with a key feature. It's like the teacher giving a command,
     but with a twist: &quot;If you are wearing a red shirt, clap your hands.
     Everyone else, stand still.&quot; The single instruction is &quot;clap if
     red.&quot; The threads (students) wearing red shirts become
     &quot;active&quot; and clap. The others become &quot;inactive&quot; and do
     nothing. The key is that the GPU can turn individual threads on or off for
     a given instruction, even though it's the same instruction for the whole
     group.</li>
 <li class=MsoNormal><b>MCQ:</b> What is the key feature of the SIMT model
     compared to a basic SIMD model? a) It can execute different instructions
     for different threads at the exact same time. b) It allows individual
     threads to be selectively active or inactive for a given instruction. c)
     It is always faster than SIMD. d) It uses more memory than SIMD.</li>
</ul>

<p class=MsoNormal><b>Answer: b) It allows individual threads to be selectively
active or inactive for a given instruction.</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Use Case:</b> <b>Graphics Rendering:</b> A pixel shader
     might have code like <b>if (pixel_is_opaque) { color_it(); } else {
     make_it_transparent(); }</b>. A group of pixels (a warp) being processed
     might include both opaque and transparent ones. The SIMT model allows the
     GPU to execute the <b>color_it</b> instruction for the opaque pixels while
     the transparent pixels are inactive, and then execute the <b>make_it_transparent</b>
     instruction for the transparent pixels while the opaque ones are inactive.</li>
 <li class=MsoNormal><b>Numerical:</b> A warp has 32 threads. An <b>if</b>
     condition is true for 10 threads and false for the remaining 22. When the
     code inside the <b>if</b> block is being executed, how many threads are
     active? <b>Answer:</b> 10 threads are active. The other 22 are inactive
     (masked off).</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 8: Problem with Thread Execution</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> This slide sets up the next big
     problem. While SIMT is powerful, it has a major weakness when threads in
     the same group need to do completely different things.</li>
 <li class=MsoNormal><b>MCQ:</b> Not applicable for a transition slide.</li>
 <li class=MsoNormal><b>Use Case:</b> Not applicable for a transition slide.</li>
 <li class=MsoNormal><b>Numerical:</b> Not applicable for a transition slide.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 9: Impact of Branch Divergence</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> This is the big problem: <b>Branch
     Divergence</b>. It happens when threads in the same warp take different
     paths (e.g., one goes into an <b>if</b> block, another into an <b>else</b>
     block). The GPU can't do both at once. It has to split the work.</li>
 <ol style='margin-top:0in' start=1 type=1>
  <li class=MsoNormal>First, it executes the <b>if</b> path for the threads
      that need it. The other threads have to wait (idle).</li>
  <li class=MsoNormal>Then, it executes the <b>else</b> path for the other
      threads, while the first group waits. This serialization kills
      performance. If half your threads go one way and half go the other, your
      performance is roughly cut in half.</li>
 </ol>
 <li class=MsoNormal><b>MCQ:</b> What happens when branch divergence occurs in
     a GPU warp? a) The program crashes. b) The different execution paths are
     run one after another, causing some threads to be idle. c) All threads
     execute both paths. d) The GPU automatically reorders the threads to avoid
     the divergence.</li>
</ul>

<p class=MsoNormal><b>Answer: b) The different execution paths are run one
after another, causing some threads to be idle.</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Use Case:</b> <b>Physics Simulation:</b> Imagine code
     that updates particle positions: <b>if (particle_collided) {
     handle_collision(); } else { update_position(); }</b>. If some particles
     in a warp have collided and others haven't, the warp will experience
     branch divergence, slowing down the entire simulation step.</li>
 <li class=MsoNormal><b>Numerical:</b> A warp of 32 threads encounters an <b>if-else</b>
     statement. 8 threads take the <b>if</b> path, which takes 50 cycles to
     execute. The other 24 threads take the <b>else</b> path, which takes 100
     cycles. What is the total time taken to execute this code block for the
     entire warp? <b>Answer:</b> The GPU must execute both paths sequentially.
     Total time = Time for <b>if</b> path + Time for <b>else</b> path = 50 +
     100 = 150 cycles.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 10: Quick quiz</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> This is a pause in the lecture to
     allow students to think about and test their understanding of the concepts
     covered so far (memory, SIMT, and divergence).</li>
 <li class=MsoNormal><b>MCQ:</b> Not applicable for a transition slide.</li>
 <li class=MsoNormal><b>Use Case:</b> Not applicable for a transition slide.</li>
 <li class=MsoNormal><b>Numerical:</b> Not applicable for a transition slide.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 11: Solution</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> This slide simply introduces the
     next section, which will present solutions to the problem of branch
     divergence.</li>
 <li class=MsoNormal><b>MCQ:</b> Not applicable for a transition slide.</li>
 <li class=MsoNormal><b>Use Case:</b> Not applicable for a transition slide.</li>
 <li class=MsoNormal><b>Numerical:</b> Not applicable for a transition slide.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 12: Some solutions to branch divergence</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> This slide lists several ways to
     fix or reduce branch divergence:</li>
 <ol style='margin-top:0in' start=1 type=1>
  <li class=MsoNormal><b>Profile:</b> Find the parts of your code where
      divergence is happening.</li>
  <li class=MsoNormal><b>Branchless Code:</b> For simple choices, use math
      tricks instead of <b>if-else</b>. For example, instead of <b>if(x) y=a;
      else y=b;</b>, you can write <b>y = x*a + (1-x)*b;</b> (if x is 0 or 1).</li>
  <li class=MsoNormal><b>Reorder Data:</b> This is a big one. If you can group
      all the data that will take path A together, and all the data that will
      take path B together, then different warps will handle each path,
      eliminating divergence <i>within</i> a warp.</li>
  <li class=MsoNormal><b>Persistent Threads:</b> Keep a small number of threads
      working all the time, pulling new tasks from a queue. This helps when
      some tasks are much harder than others.</li>
  <li class=MsoNormal><b>Warp Intrinsics:</b> Advanced functions to give you
      more control over the threads within a warp.</li>
 </ol>
 <li class=MsoNormal><b>MCQ:</b> Which of the following is a strategy for
     reducing branch divergence? a) Making warps larger. b) Using more global
     memory. c) Reordering data so that threads taking the same path are in the
     same warp. d) Avoiding all <b>if</b> statements in the code.</li>
</ul>

<p class=MsoNormal><b>Answer: c) Reordering data so that threads taking the
same path are in the same warp.</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Use Case:</b> <b>Data Reordering for Collision
     Detection:</b> You have a list of objects. First, run a kernel to check
     which objects are moving. Then, create two new lists: one for moving
     objects and one for stationary objects. Now, you can run one kernel on the
     &quot;moving&quot; list and another on the &quot;stationary&quot; list.
     Since all threads in each kernel are doing the same thing, there is no
     divergence.</li>
 <li class=MsoNormal><b>Numerical:</b> You have an array of 64 numbers. The
     first 16 are positive, and the next 48 are negative. Your code has an <b>if
     (value &gt; 0)</b> branch. If you process this array with 64 threads (2
     warps of 32), how many warps will experience divergence? <b>Answer:</b>
     Warp 0 (threads 0-31) will have divergence because it contains 16 positive
     and 16 negative values. Warp 1 (threads 32-63) will also have divergence
     because it contains 0 positive and 32 negative values. So, 2 warps will
     experience divergence. (Note: if the condition is always false for a warp,
     some GPUs optimize this away, but the potential for divergence exists).</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 13: Persistent threads idea</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> Instead of launching a massive
     number of threads that each do one tiny job and then quit, you launch a
     smaller, fixed number of &quot;worker&quot; threads. These workers stay
     running. They look at a central list of tasks (a &quot;work queue&quot;),
     grab a task, finish it, and then immediately grab the next one. They keep
     doing this until the task list is empty. This keeps the GPU busy and
     prevents it from being held up by a few slow threads in a group.</li>
 <li class=MsoNormal><b>MCQ:</b> What is the main idea behind the
     &quot;persistent threads&quot; model? a) Threads are launched for each
     individual task and then terminated. b) A small number of threads are
     launched that continuously pull new work from a queue until all tasks are
     done. c) All threads are forced to execute the exact same number of
     instructions. d) The CPU manages the execution of each GPU thread
     individually.</li>
</ul>

<p class=MsoNormal><b>Answer: b) A small number of threads are launched that
continuously pull new work from a queue until all tasks are done.</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Use Case:</b> <b>Ray Tracing:</b> Tracing a ray through
     a scene can take very different amounts of time. A ray that hits a nearby
     object is &quot;fast.&quot; A ray that bounces many times through empty
     space is &quot;slow.&quot; With persistent threads, a thread that finishes
     a &quot;fast&quot; ray can immediately start working on a new one, instead
     of waiting for all the other threads to finish their &quot;slow&quot;
     rays.</li>
 <li class=MsoNormal><b>Numerical:</b> You have 1024 independent tasks to
     process. You launch a kernel with 256 persistent threads. The threads pull
     tasks from a global queue. Assuming each task takes the same amount of
     time, how many tasks will each thread have to process on average? <b>Answer:</b>
     1024 tasks / 256 threads = 4 tasks per thread.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 14: Warp Divergence (Example)</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> This slide gives a clear example.
     Imagine 64 threads split into two warps (Warp 0: threads 0-31, Warp 1:
     threads 32-63).</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal><b>Bad Layout:</b> If threads 0-15 and 32-47 do task A,
      and threads 16-31 and 48-63 do task B, then <i>both</i> Warp 0 and Warp 1
      are split. Both warps suffer from divergence.</li>
  <li class=MsoNormal><b>Good Layout:</b> If you reorganize the data so that
      threads 0-31 all do task A and threads 32-63 all do task B, then Warp 0
      does only A and Warp 1 does only B. No warp is split, so there is <b>zero
      divergence</b>. The performance is much better.</li>
 </ul>
 <li class=MsoNormal><b>MCQ:</b> In the example with 64 threads and a warp size
     of 32, which thread organization completely avoids warp divergence? a)
     Threads 0-15 and 32-47 do Path A; Threads 16-31 and 48-63 do Path B. b)
     Threads 0-31 do Path A; Threads 32-63 do Path B. c) All even-numbered
     threads do Path A; all odd-numbered threads do Path B. d) It's impossible
     to avoid divergence in this scenario.</li>
</ul>

<p class=MsoNormal><b>Answer: b) Threads 0-31 do Path A; Threads 32-63 do Path
B.</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Use Case:</b> This is the practical application of the
     &quot;reorder data&quot; solution. The &quot;use case&quot; is the act of
     sorting or partitioning your data <i>before</i> running the main kernel,
     so that it matches the &quot;Good Layout&quot; described in the slide.</li>
 <li class=MsoNormal><b>Numerical:</b> A warp has 32 threads. Path A takes 10
     cycles. Path B takes 20 cycles.</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal><b>Scenario 1:</b> 32 threads take Path A. What is the
      execution time?</li>
  <li class=MsoNormal><b>Scenario 2:</b> 16 threads take Path A, 16 threads
      take Path B. What is the execution time? <b>Answer:</b></li>
  <li class=MsoNormal>Scenario 1: 10 cycles (no divergence).</li>
  <li class=MsoNormal>Scenario 2: 10 + 20 = 30 cycles (divergence causes
      serialization).</li>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 15: Example Code</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> This shows a simple piece of code
     with an <b>if-else</b> statement. The key takeaway is that this code <i>doesn't
     always</i> cause branch divergence. It only causes divergence if, within a
     single warp, some threads have <b>data[i] &gt; 0</b> and others do not. If
     all threads in a warp have <b>data[i] &gt; 0</b>, there's no divergence.</li>
 <li class=MsoNormal><b>MCQ:</b> Looking at the code <b>if (data[i] &gt; 0) ...
     else ...</b>, does this code <i>guarantee</i> branch divergence will
     occur? a) Yes, any <b>if-else</b> statement always causes divergence. b)
     No, it only causes divergence if threads within the same warp take
     different paths. c) Yes, because the CPU and GPU have to communicate. d)
     No, because the GPU compiler automatically removes all branches.</li>
</ul>

<p class=MsoNormal><b>Answer: b) No, it only causes divergence if threads
within the same warp take different paths.</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Use Case:</b> <b>Processing Sensor Data:</b> Imagine a
     kernel processing data from a sensor. The code might be <b>if
     (sensor_value &gt; threshold) { trigger_alarm(); }</b>. Whether this
     causes divergence depends on the data. If the sensor readings for a group
     of 32 threads are all above the threshold, no divergence occurs. If some
     are and some aren't, it does.</li>
 <li class=MsoNormal><b>Numerical:</b> A 32-thread warp is processing this
     code. The <b>data</b> array for this warp contains the values: <b>[1, -2,
     3, -4, 5, -6, ..., 31, -32]</b> (alternating positive and negative). How
     many threads will execute <b>computeA()</b> and how many will execute <b>computeB()</b>?
     Will this warp experience divergence? <b>Answer:</b> 16 threads will
     execute <b>computeA()</b> (the positive ones) and 16 will execute <b>computeB()</b>
     (the negative ones). Yes, this warp will experience divergence.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 16: Ideal Situation</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> This slide shows the &quot;dream
     scenario.&quot; To avoid divergence, you want to organize your data and
     your threads so that all threads within any given warp make the same
     decision. In other words, for any <b>if</b> statement, a whole warp should
     either all go into the <b>if</b> block or all go into the <b>else</b>
     block.</li>
 <li class=MsoNormal><b>MCQ:</b> What is the &quot;ideal situation&quot; for
     avoiding branch divergence in a GPU? a) Having a single thread per warp. b)
     Ensuring that all threads within a warp follow the same execution path. c)
     Using only global memory and avoiding shared memory. d) Making sure every
     thread in the entire grid follows the same execution path.</li>
</ul>

<p class=MsoNormal><b>Answer: b) Ensuring that all threads within a warp follow
the same execution path.</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Use Case:</b> This is the <i>goal</i> you are trying to
     achieve when you apply the &quot;data reordering&quot; technique. The use
     case is the <i>result</i> of successfully reordering your data to be
     &quot;warp-aligned&quot; with the branches in your code.</li>
 <li class=MsoNormal><b>Numerical:</b> You have 128 particles. 80 are
     &quot;active&quot; and 48 are &quot;inactive.&quot; Your code has a branch
     <b>if (particle.is_active)</b>. You want to process this with 128 threads
     (4 warps). How would you arrange the particles in memory to achieve the
     &quot;ideal situation&quot; and avoid divergence? <b>Answer:</b> Place all
     80 &quot;active&quot; particles in the first 80 positions of the array,
     and all 48 &quot;inactive&quot; particles in the last 48 positions. Then,
     Warp 0 (threads 0-31), Warp 1 (32-63), and Warp 2 (64-95) will all see <b>is_active</b>
     as true and execute the <b>if</b> path. Warp 3 (96-127) will all see <b>is_active</b>
     as false and execute the <b>else</b> path. No warp is split, so there is
     no divergence.</li>
</ul>

<p class=MsoNormal>&nbsp;</p>

<div style='border:none;border-bottom:double windowtext 2.25pt;padding:0in 0in 1.0pt 0in'>

<p class=MsoNormal style='border:none;padding:0in'>&nbsp;</p>

</div>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal><b>Slide 1: GPU Programming Lecture-13</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>This is the title slide for the lecture. It simply
introduces the topic as &quot;GPU Programming&quot; and specifies it's Lecture
13. There's no deep content here—it's like the cover page of a presentation to
set the stage for what's coming, which is advanced concepts in how GPUs handle
memory and thread execution.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is the main topic of this lecture? a) CPU Programming
     b) GPU Programming c) Memory Management in RAM d) Web Development <b>Answer:
     b</b></li>
 <li class=MsoNormal>What lecture number is this? a) 10 b) 13 c) 15 d) 20 <b>Answer:
     b</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Introducing a course module in a university class on
     parallel computing.</li>
 <li class=MsoNormal>Starting a webinar or tutorial series on CUDA or OpenCL
     programming for beginners.</li>
 <li class=MsoNormal>As a header in documentation for GPU optimization
     techniques.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>If a course has 20 lectures and this is Lecture 13, how
     many lectures are left? <b>Solution:</b> 20 - 13 = 7. Explanation:
     Subtract the current lecture number from the total to find remaining ones.</li>
 <li class=MsoNormal>Suppose lectures are numbered from 1 to N, and this is the
     13th. What percentage of the course is completed if N=25? <b>Solution:</b>
     (13 / 25) * 100 = 52%. Explanation: Divide the current lecture by total
     and multiply by 100 for percentage.</li>
</ol>

<p class=MsoNormal><b>Slide 2: Memory Hierarchy</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>This slide talks about how memory is organized in GPUs.
Shared memory is super fast compared to global memory. Local memory isn't
enough for everything, so we use it carefully. Global, constant, and texture
memories can be accessed by both the GPU (device) and the CPU (host), making
data sharing easy between them.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Which memory is much faster than global memory? a) Shared
     memory b) Local memory c) Constant memory d) Texture memory <b>Answer: a</b></li>
 <li class=MsoNormal>What types of memory are available to both host and CPU?
     a) Shared and local b) Global, constant, and texture c) Only global d)
     Registers <b>Answer: b</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Optimizing matrix multiplication in CUDA by using shared
     memory to reduce global memory accesses.</li>
 <li class=MsoNormal>In graphics rendering, using texture memory for image data
     that needs fast, read-only access.</li>
 <li class=MsoNormal>Scientific simulations where constant memory holds fixed
     values like physical constants shared across threads.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>If accessing global memory takes 400 cycles and shared
     memory takes 4 cycles, how many times faster is shared memory? <b>Solution:</b>
     400 / 4 = 100 times. Explanation: Divide the slower access time by the
     faster one to get the speed ratio.</li>
 <li class=MsoNormal>Suppose a kernel accesses global memory 100 times but
     optimizes to use shared memory for 80 accesses. If global access costs 500
     units and shared costs 5 units, what's the total cost savings? <b>Solution:</b>
     Original cost = 100 * 500 = 50,000. Optimized = (20 * 500) + (80 * 5) =
     10,000 + 400 = 10,400. Savings = 50,000 - 10,400 = 39,600. Explanation:
     Calculate original total, then subtract optimized total.</li>
</ol>

<p class=MsoNormal><b>Slide 3: GPU Memory Hierarchy</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>This slide breaks down the levels of memory in a GPU. Global
memory is accessible by everyone (threads and CPU). L2 cache is shared across
all threads. L1 cache is per streaming multiprocessor (SM). Shared memory is
like a fast scratchpad for threads in the same block to share data quickly.
Registers are the fastest, private to each thread and last only as long as the
thread runs.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Which memory is accessible by all threads and the CPU? a)
     Registers b) Shared memory c) Global memory d) L1 cache <b>Answer: c</b></li>
 <li class=MsoNormal>What is the lifetime of registers? a) The entire program
     b) The thread lifetime c) The block lifetime d) Permanent <b>Answer: b</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>In deep learning training, using global memory for large
     datasets and shared memory for temporary computations within a block.</li>
 <li class=MsoNormal>Video game engines where L1/L2 caches speed up frequent
     data access in rendering pipelines.</li>
 <li class=MsoNormal>Parallel sorting algorithms that use shared memory to
     exchange data between threads efficiently.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>If a GPU has 4 SMs and each has 64KB L1 cache, what's the
     total L1 cache size? <b>Solution:</b> 4 * 64 = 256 KB. Explanation:
     Multiply number of SMs by per-SM cache size.</li>
 <li class=MsoNormal>Suppose shared memory per block is 48KB, and you have 2
     blocks per SM. If each thread uses 100 bytes from shared, how many threads
     can fit per block without overflow? <b>Solution:</b> 48,000 bytes / 100 =
     480 threads. Explanation: Convert KB to bytes (48 * 1,024 &#8776; 48,000
     for simplicity), then divide by per-thread usage.</li>
</ol>

<p class=MsoNormal><b>Slide 4: Shared Memory Architecture</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>Shared memory is split into cache lines of 128 bytes each.
Each line has 32 banks, and each bank holds 4 bytes. Banks can only handle one
access per cycle, so if multiple threads hit the same bank, it slows down. This
setup allows fast data sharing but needs careful access patterns to avoid
conflicts.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is the size of each cache line in shared memory? a)
     32 bytes b) 128 bytes c) 4 bytes d) 512 bytes <b>Answer: b</b></li>
 <li class=MsoNormal>How many banks are in each cache line? a) 4 b) 32 c) 128
     d) 16 <b>Answer: b</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Tiled matrix multiplication where threads load data into
     shared memory banks without conflicts for faster computation.</li>
 <li class=MsoNormal>Image processing filters (e.g., convolution) using shared
     memory to store neighborhood pixels.</li>
 <li class=MsoNormal>Reduction operations like summing arrays, where bank
     organization minimizes serialization.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>If a cache line is 128 bytes with 32 banks, what's the
     bank size? <b>Solution:</b> 128 / 32 = 4 bytes. Explanation: Divide total
     line size by number of banks.</li>
 <li class=MsoNormal>Suppose 16 threads access shared memory. If they all hit
     different banks, time = 1 cycle. If 4 hit the same bank, how many cycles
     for that bank? <b>Solution:</b> 4 cycles (serialized). Explanation:
     Serialize accesses to the same bank, so number of conflicting threads
     equals cycles needed.</li>
</ol>

<p class=MsoNormal><b>Slide 5: Bank Conflicts</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>Shared memory is divided into banks that can only read/write
one thing at a time. Addresses are striped across banks. If two or more threads
try to access the same bank simultaneously, their requests get queued up
(serialized), which slows things down. This is called a bank conflict, and it's
bad for performance.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What happens when multiple threads access the same bank at
     once? a) They access in parallel b) Accesses are serialized c) The GPU
     crashes d) Data is duplicated <b>Answer: b</b></li>
 <li class=MsoNormal>How are bank addresses organized? a) Random b) Striped c)
     Sequential d) Hierarchical <b>Answer: b</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Avoiding conflicts in stencil computations for simulations
     (e.g., fluid dynamics) by padding arrays.</li>
 <li class=MsoNormal>In FFT (Fast Fourier Transform) algorithms on GPU,
     structuring data to access different banks.</li>
 <li class=MsoNormal>Parallel prefix sum where thread access patterns are
     designed to minimize bank overlaps.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>If 3 threads conflict on one bank, how many serialized
     accesses occur? <b>Solution:</b> 3. Explanation: Each conflicting access
     is done one after another.</li>
 <li class=MsoNormal>In a 32-bank setup, if threads 0-31 access addresses where
     address % 32 = bank, and all access bank 0, what's the serialization
     factor? <b>Solution:</b> 32 (all serialized). Explanation: All threads hit
     one bank, so factor equals number of threads.</li>
</ol>

<p class=MsoNormal><b>Slide 6: Single-Instruction, Multiple-Thread Warp
Execution on Fermi</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>On Fermi architecture (an older NVIDIA GPU), warps (groups
of threads) are scheduled and executed. There are warp schedulers that dispatch
instructions to units. The slide shows how multiple warps run instructions over
time, like Warp 8 instruction 11, then others, illustrating how the GPU
multitasks threads efficiently.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What does SIMT stand for in this context? a) Single
     Instruction Multiple Threads b) Simple Instruction Multi Task c) Single
     Input Multiple Threads d) System Instruction Multi Thread <b>Answer: a</b></li>
 <li class=MsoNormal>What hardware component dispatches instructions? a) Warp
     Scheduler b) Global Memory c) Registers d) L2 Cache <b>Answer: a</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Understanding scheduling in ray tracing applications on
     older GPUs.</li>
 <li class=MsoNormal>Optimizing kernel launches for computer vision tasks like
     edge detection.</li>
 <li class=MsoNormal>Simulating particle systems where many threads (warps)
     compute positions concurrently.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>If there are 2 warp schedulers and each handles 4 warps
     per cycle, how many warps per cycle total? <b>Solution:</b> 2 * 4 = 8.
     Explanation: Multiply schedulers by warps per scheduler.</li>
 <li class=MsoNormal>Suppose a warp has 32 threads, and execution takes 10
     cycles per instruction. For 5 instructions, total cycles? <b>Solution:</b>
     5 * 10 = 50. Explanation: Multiply instructions by cycles per instruction.</li>
</ol>

<p class=MsoNormal><b>Slide 7: SIMD Execution of Scalar Threads</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>All threads run the same code (kernel), but they're grouped
into warps that execute the same instruction at once using SIMD (Single
Instruction Multiple Data) pipelines. This saves space on control logic because
one instruction controls many threads, like a boss giving the same order to a
team.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What do all threads run? a) Different kernels b) The same
     kernel c) Only host code d) Sequential code <b>Answer: b</b></li>
 <li class=MsoNormal>What is a warp? a) Threads grouped into a SIMD instruction
     b) A type of memory c) A cache level d) A scheduler <b>Answer: a</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Vector addition in linear algebra, where threads in a warp
     add corresponding elements.</li>
 <li class=MsoNormal>Image blurring where each warp processes a row of pixels
     identically.</li>
 <li class=MsoNormal>Financial modeling simulations running the same
     computation on multiple data points.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>If a warp has 32 threads and SIMD pipeline processes 8 per
     cycle, how many cycles for one warp? <b>Solution:</b> 32 / 8 = 4.
     Explanation: Divide total threads by threads per cycle.</li>
 <li class=MsoNormal>For 100 threads (3 warps of 32 + 1 of 4), if each warp
     takes 5 cycles, total cycles assuming sequential? <b>Solution:</b> 4 * 5 =
     20 (last warp partial). Explanation: Count full warps and adjust for
     partial.</li>
</ol>

<p class=MsoNormal><b>Slide 8: Single Instruction Multiple Threads (SIMT)</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>In SIMT, threads can be turned on or off selectively. Only
active threads process instructions and data, while inactive ones keep their
data safe but sit idle. This handles branches, like in an if-statement where
some threads (e.g., threadIdx.x &lt; 4) do one thing, and others skip.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What happens to inactive threads in SIMT? a) Their data is
     deleted b) Data remains unchanged c) They crash d) They process anyway <b>Answer:
     b</b></li>
 <li class=MsoNormal>How are threads activated/deactivated? a) Selectively for
     instructions b) All at once c) Only on host d) Never <b>Answer: a</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Conditional rendering in graphics where only certain
     pixels are processed.</li>
 <li class=MsoNormal>Sparse matrix operations where threads skip zero values.</li>
 <li class=MsoNormal>Machine learning inference with activation functions that
     branch based on values.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>If 32 threads in a warp, and condition activates 20,
     what's the active fraction? <b>Solution:</b> 20 / 32 &#8776; 0.625 or
     62.5%. Explanation: Divide active by total.</li>
 <li class=MsoNormal>Suppose activation takes 2 cycles for active threads. For
     16 active out of 32, effective cycles per instruction? <b>Solution:</b> 2
     (but half idle, efficiency 50%). Explanation: Cycles same, but utilization
     = active/total.</li>
</ol>

<p class=MsoNormal><b>Slide 9: Problem with Thread Execution</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>GPUs use SIMD to save space, grouping threads into warps.
But if threads in a warp take different paths in code (branch divergence), the
GPU can't run them together—it has to run one path, then the other, serializing
them. This wastes time because not all threads are working simultaneously.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Why does the GPU serialize paths? a) Can't execute both
     simultaneously b) To save memory c) For security d) Always does it <b>Answer:
     a</b></li>
 <li class=MsoNormal>When does branch divergence occur? a) Threads in warps
     take different paths b) All threads same path c) No branches d) Only in
     loops <b>Answer: a</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Path tracing in rendering where rays diverge based on
     hits.</li>
 <li class=MsoNormal>Sorting algorithms like quicksort with conditional swaps.</li>
 <li class=MsoNormal>Simulations of irregular data like graph traversals.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>In a warp of 32, if 16 diverge to path A and 16 to B, each
     path 5 cycles, total cycles? <b>Solution:</b> 5 + 5 = 10. Explanation:
     Serialize paths, add their cycles.</li>
 <li class=MsoNormal>Efficiency if divergence halves active threads per path in
     warp of 32? <b>Solution:</b> (16/32) = 50% per path, overall 50%.
     Explanation: Average active/total.</li>
</ol>

<p class=MsoNormal><b>Slide 10: Impact of Branch Divergence</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>If half the threads in a warp (size 16 example) go one way
and half another, the GPU runs one half while the other idles, then switches.
This means only 50% of the hardware is used at a time, causing a big
performance hit—like half your team slacking off.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is performance efficiency if half threads active? a)
     1.0 b) 0.5 c) 0.0 d) 2.0 <b>Answer: b</b></li>
 <li class=MsoNormal>In the example, how many threads are active at a time? a)
     16 b) 8 c) 4 d) 32 <b>Answer: b</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Analyzing performance in conditional image processing
     (e.g., thresholding).</li>
 <li class=MsoNormal>Optimizing AI models with if-else in activation layers.</li>
 <li class=MsoNormal>Weather simulations where conditions vary by grid point.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>For warp size 16, 8 on A, 8 on B. Efficiency? <b>Solution:</b>
     8/16 = 0.5. Explanation: Active/total per execution phase.</li>
 <li class=MsoNormal>If divergence causes 50% loss, and original time is 100
     ms, new time? <b>Solution:</b> 100 / 0.5 = 200 ms. Explanation: Divide by
     efficiency to get adjusted time.</li>
</ol>

<p class=MsoNormal><b>Slide 11: Quick Quiz</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>The quiz asks: For adding two vectors of size 1000, which is
better for thread utilization—Scenario 1: 1 grid of 10 blocks, each 100
threads; or Scenario 2: 1 grid of 8 blocks, each 128 threads? It's testing
understanding of how block sizes affect warp efficiency.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is N in the vector addition? a) 100 b) 1000 c) 128 d)
     10 <b>Answer: b</b></li>
 <li class=MsoNormal>How many blocks in Scenario 1? a) 8 b) 10 c) 1 d) 128 <b>Answer:
     b</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Teaching block sizing in CUDA programming courses.</li>
 <li class=MsoNormal>Optimizing vector operations in numerical libraries like
     cuBLAS.</li>
 <li class=MsoNormal>Parallel data processing in big data analytics.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>In Scenario 1, total threads? <b>Solution:</b> 10 * 100 =
     1000. Explanation: Blocks * threads per block.</li>
 <li class=MsoNormal>If warp size 32, how many partial warps in a 100-thread
     block? <b>Solution:</b> 100 / 32 = 3 full + 4 threads (partial warp).
     Explanation: Integer divide and remainder.</li>
</ol>

<p class=MsoNormal><b>Slide 12: Solution</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>Scenario 2 is better. In Scenario 1, most blocks have 3 full
warps (96 threads) + a partial warp of 4 threads, averaging 78.125%
utilization. Scenario 2 has mostly 4 full warps (128 threads), but last block
adjusted, averaging 97.656%—better because fewer wasted threads in partial
warps.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Which scenario has higher average thread utilization? a)
     Scenario 1 b) Scenario 2 c) Both same d) Neither <b>Answer: b</b></li>
 <li class=MsoNormal>What is utilization in Scenario 1? a) 97.656% b) 78.125%
     c) 50% d) 100% <b>Answer: b</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Tuning block sizes for matrix-vector multiplication.</li>
 <li class=MsoNormal>Efficient parallel reduction in sum operations.</li>
 <li class=MsoNormal>Load balancing in GPU-accelerated databases.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>For Scenario 1: Utilization calculation? <b>Solution:</b>
     (3<i>32 + 4)/ (4</i>32) = 100/128 &#8776; 78.125%. Explanation: Active
     threads over max possible per block's warps.</li>
 <li class=MsoNormal>If N=1000, blocks=8 at 128 threads: Total threads? <b>Solution:</b>
     8*128=1024 (extra 24 idle). Explanation: Blocks * size, note overflow.</li>
</ol>

<p class=MsoNormal><b>Slide 13: Some Solutions to Branch Divergence</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>To fix branch divergence: Profile code to find branches,
replace small ones with math tricks (branchless), reorder data so similar items
are together, use persistent threads that keep working from a queue, or warp
intrinsics for control. Results vary—data reordering gives big speedups, others
moderate.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is the first step to fix divergence? a) Profile and
     identify branches b) Use more memory c) Increase threads d) Ignore it <b>Answer:
     a</b></li>
 <li class=MsoNormal>Which gives big wins for data-driven divergence? a)
     Reorder/group inputs b) Branchless transforms c) Warp intrinsics d) All
     same <b>Answer: a</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Optimizing ray marching in games by reordering rays.</li>
 <li class=MsoNormal>Machine learning with skewed data using work queues.</li>
 <li class=MsoNormal>Graph algorithms like BFS with persistent threads for
     irregular work.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>If reordering eliminates 80% divergence in a 32-thread
     warp, new efficiency? <b>Solution:</b> From 50% to 90% (assume half to 10%
     idle). Explanation: Reduce idle fraction proportionally.</li>
 <li class=MsoNormal>Suppose branchless transform speeds up by 1.5x. Original
     time 200 ms, new? <b>Solution:</b> 200 / 1.5 &#8776; 133.33 ms.
     Explanation: Divide by speedup factor.</li>
</ol>

<p class=MsoNormal><b>Slide 14: Persistent Threads Idea</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>Instead of many short threads, launch a few long-lived ones
(one block per SM) that stay running. They pull tasks from a global queue until
everything's done. This keeps the GPU busy, especially for uneven workloads, by
having threads loop and grab work dynamically.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What do persistent threads do? a) Terminate after one task
     b) Stay alive and fetch new work c) Only run on host d) Use local memory
     only <b>Answer: b</b></li>
 <li class=MsoNormal>How is work fetched? a) From a global queue b) Statically
     assigned c) Randomly d) From registers <b>Answer: a</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Load-balancing in Monte Carlo simulations with varying
     computation per particle.</li>
 <li class=MsoNormal>Task-based parallelism in video encoding where frames have
     different complexity.</li>
 <li class=MsoNormal>Dynamic scheduling in AI training for mini-batches of
     uneven size.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>If 4 SMs, one block each, total tasks 1000, each task 10
     cycles. Min cycles if balanced? <b>Solution:</b> 1000 * 10 / 4 = 2500.
     Explanation: Total work divided by SMs.</li>
 <li class=MsoNormal>With atomicAdd for index, if 32 threads, how many
     increments per loop if all grab? <b>Solution:</b> 32 (one per thread).
     Explanation: Each thread increments once per iteration.</li>
</ol>

<p class=MsoNormal><b>Slide 15: Warp Divergence</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>In a 64-thread launch (2 warps), if condition splits
half-half per warp, it causes divergence within warps (bad). But if data is
organized so whole warps take the same path (e.g., Warp 0 all A, Warp 1 all B),
no intra-warp divergence—even if global branching exists.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is warp size in the example? a) 16 b) 32 c) 64 d) 128
     <b>Answer: b</b></li>
 <li class=MsoNormal>How to avoid warp divergence? a) Organize data for uniform
     paths per warp b) Increase branches c) Use smaller warps d) Ignore
     organization <b>Answer: a</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Conditional filtering in data analytics, grouping similar
     data.</li>
 <li class=MsoNormal>Physics engines where particles are sorted by state.</li>
 <li class=MsoNormal>Neural networks with branch-based activations, sorting
     inputs.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>In 64 threads, 2 warps. If split evenly per warp,
     divergence factor? <b>Solution:</b> 2 (half idle per path). Explanation:
     Serialize two halves.</li>
 <li class=MsoNormal>If reorganized, efficiency gain from 50% to 100%, speedup?
     <b>Solution:</b> 100% / 50% = 2x. Explanation: Ratio of new to old
     efficiency.</li>
</ol>

<p class=MsoNormal><b>Slide 16: Example Code</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>The code has an if-else: if data[i] &gt; 0, compute A; else
compute B. This branches, and whether it hurts depends on data distribution—if
positives and negatives are mixed in a warp, divergence happens; if grouped,
warps can be uniform.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What causes branch divergence here? a) Data[i] values
     distribution b) ComputeA function c) Result array d) No branches <b>Answer:
     a</b></li>
 <li class=MsoNormal>In ideal situation for 64 threads? a) Mixed data causes
     divergence b) Grouped data avoids it c) Always diverges d) No effect <b>Answer:
     b</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Positive/negative value processing in financial risk
     models.</li>
 <li class=MsoNormal>Image segmentation based on pixel intensity thresholds.</li>
 <li class=MsoNormal>Sorting positive/negative numbers in parallel.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>In warp of 32, if 20 positive, 12 negative, efficiency per
     path? <b>Solution:</b> Max(20/32, 12/32) &#8776; 62.5%. Explanation:
     Higher of the splits, but overall average.</li>
 <li class=MsoNormal>If computeA takes 4 cycles, computeB 6, diverged time? <b>Solution:</b>
     4 + 6 = 10 cycles. Explanation: Serialize paths.</li>
</ol>

<p class=MsoNormal><b>Slide 17: if (data[i] &gt; 0) ...</b></p>

<p class=MsoNormal><b>Explanation in Easy Language</b></p>

<p class=MsoNormal>Repeating the code, it emphasizes keeping branches
&quot;warp-aligned&quot;—meaning threads in the same warp should make the same
decision (all true or all false) to avoid slowdowns. Branching is okay if it's
uniform within warps.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What should be ideal for threads in a warp? a) Same
     control-flow decisions b) Different decisions c) No decisions d) Host
     decisions <b>Answer: a</b></li>
 <li class=MsoNormal>Can you have branching in code? a) Yes, if warp-aligned b)
     No, never c) Only in loops d) Only if-else <b>Answer: a</b></li>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Threshold-based classification in ML inference.</li>
 <li class=MsoNormal>Conditional updates in game AI behaviors.</li>
 <li class=MsoNormal>Data filtering in scientific computing.</li>
</ul>

<p class=MsoNormal><b>Numericals</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>If 32 threads, all same path, efficiency? <b>Solution:</b>
     32/32 = 100%. Explanation: Full utilization.</li>
 <li class=MsoNormal>If mixed 50-50, time multiplier? <b>Solution:</b> 2x
     (serialize two paths). Explanation: Number of paths serialized.</li>
</ol>

<div style='border:none;border-bottom:double windowtext 2.25pt;padding:0in 0in 1.0pt 0in'>

<p class=MsoNormal style='border:none;padding:0in'>&nbsp;</p>

</div>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>Here is a detailed explanation, MCQs, use cases, and
numericals for each slide from your GPU Programming Lecture 13 PDF, based on
the content and additional research.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 1: Memory Hierarchy</b></p>

<p class=MsoNormal>Explanation:<br>
GPUs have a memory hierarchy with different levels: global memory (slowest,
accessible by all threads), L2 cache, L1 cache, shared memory (fast, accessible
by threads in a block), and registers (fastest, per thread). Shared memory is
much faster than global memory and is used for communication between threads in
a block.<span style='font-family:"Arial",sans-serif'>&#8203;&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
Which memory is fastest for thread communication within a block?<br>
A) Global memory<br>
B) L2 cache<br>
C) Shared memory<br>
D) Registers<br>
Answer:&nbsp;C) Shared memory</p>

<p class=MsoNormal>Use Case:<br>
Shared memory is used in matrix multiplication to store sub-matrices for fast
access by threads in a block.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>Numerical:<br>
If a block has 32 threads and each thread accesses 4 bytes, the total shared
memory used is&nbsp;32×4=12832×4=128&nbsp;bytes.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 2: Shared Memory Architecture</b></p>

<p class=MsoNormal>Explanation:<br>
Shared memory is divided into 32 banks, each 4 bytes wide. Each bank can be
accessed by one thread per cycle. If multiple threads access the same bank,
their accesses are serialized, causing a bank conflict.<span style='font-family:
"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
What happens if two threads in a warp access the same bank in shared memory?<br>
A) Both accesses happen in parallel<br>
B) Accesses are serialized<br>
C) No conflict occurs<br>
D) The GPU crashes<br>
Answer:&nbsp;B) Accesses are serialized</p>

<p class=MsoNormal>Use Case:<br>
Bank conflicts are avoided in FFT algorithms by ensuring each thread accesses a
different bank.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>Numerical:<br>
If 32 threads access 32 different banks, the memory bandwidth is maximized. If
all threads access the same bank, bandwidth drops by a factor of 32.<span
style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 3: Warp Execution and Divergence</b></p>

<p class=MsoNormal>Explanation:<br>
GPUs execute threads in groups called warps (32 threads). All threads in a warp
execute the same instruction at the same time. If threads in a warp take
different paths (branch divergence), the GPU serializes execution, reducing
efficiency.<span style='font-family:"Arial",sans-serif'>&#8203;&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
What is the impact of branch divergence in a warp?<br>
A) All threads execute in parallel<br>
B) Only active threads execute, others wait<br>
C) The GPU stops execution<br>
D) Threads are reassigned to other warps<br>
Answer:&nbsp;B) Only active threads execute, others wait</p>

<p class=MsoNormal>Use Case:<br>
Branch divergence is minimized in ray tracing by grouping rays with similar
paths.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>Numerical:<br>
If 16 threads in a warp take path A and 16 take path B, only 16 threads are
active at a time, resulting in 50% utilization.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 4: Persistent Threads</b></p>

<p class=MsoNormal>Explanation:<br>
Persistent threads keep threads alive for the entire computation, fetching new
work from a global queue. This avoids launch latency and optimizes memory
usage.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
What is a benefit of persistent threads?<br>
A) Faster kernel launches<br>
B) Reduced memory usage<br>
C) Elimination of launch latency<br>
D) Increased warp size<br>
Answer:&nbsp;C) Elimination of launch latency</p>

<p class=MsoNormal>Use Case:<br>
Persistent threads are used in network packet processing for low-latency
demands.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>Numerical:<br>
If a kernel launches 1000 tasks and each thread processes 10 tasks, only 100
threads are needed instead of 1000.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 5: Warp Divergence Example</b></p>

<p class=MsoNormal>Explanation:<br>
If threads in a warp follow different execution paths, the GPU serializes the
execution, reducing SIMD utilization. Grouping threads with similar paths
avoids this.<span style='font-family:"Arial",sans-serif'>&#8203;&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
How can warp divergence be minimized?<br>
A) Increase warp size<br>
B) Group threads with similar paths<br>
C) Use more registers<br>
D) Increase global memory<br>
Answer:&nbsp;B) Group threads with similar paths</p>

<p class=MsoNormal>Use Case:<br>
Warp divergence is minimized in sorting algorithms by sorting data before
processing.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>Numerical:<br>
If 32 threads are split into two groups of 16, each group processes its path
separately, resulting in 50% utilization.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal>Each slide's explanation, MCQ, use case, and numerical
example is provided above for clarity and practical understanding. If you need
further details or more examples, please letease let me know!</p>

<p class=MsoNormal>there are more slides</p>

<p class=MsoNormal>Yes, there are additional slides in the document. Based on
the extracted content, here are detailed entries for the remaining key slides
from the lecture, each with an easy explanation, MCQs, use cases, and
numericals.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 7: SIMD Execution of Scalar Threads</b></p>

<p class=MsoNormal>Explanation:<br>
All threads in a GPU execute the same instruction together (SIMD - Single
Instruction, Multiple Data), grouped into warps. Threads in a warp follow a
single program counter, meaning they ideally do the same task at the same time
for maximum efficiency.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQs:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>In SIMD execution, what do all threads do?</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal>A) Different instructions</li>
  <li class=MsoNormal>B) Same instruction on different data</li>
  <li class=MsoNormal>C) Same data with different instructions</li>
  <li class=MsoNormal>D) Different tasks at different times</li>
 </ul>
 <li class=MsoNormal>Warps in GPU contain:</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal>A) Only one thread</li>
  <li class=MsoNormal>B) Multiple threads executing in lockstep</li>
  <li class=MsoNormal>C) Unrelated threads</li>
  <li class=MsoNormal>D) Only memory addresses</li>
 </ul>
</ol>

<p class=MsoNormal>Use Cases:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Graphics rendering, where many pixels can be processed
     identically.</li>
 <li class=MsoNormal>Matrix operations or vector addition.</li>
</ul>

<p class=MsoNormal>Numerical:<br>
If a warp has 32 threads, how many instructions are executed at the same time
when running a vector addition kernel?<br>
(Solution: 32 instructions, one per thread, executed together.)</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 8: Problem with Thread Execution and Branch
Divergence</b></p>

<p class=MsoNormal>Explanation:<br>
When threads in a warp take different paths due to an&nbsp;if-else, only some
threads can execute at any moment, causing others to wait. This phenomenon is
called branch divergence and reduces performance.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQs:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Branch divergence occurs when:</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal>A) All threads take the same path</li>
  <li class=MsoNormal>B) Threads in a warp take different paths</li>
  <li class=MsoNormal>C) Memory is not enough</li>
  <li class=MsoNormal>D) All warps finish together</li>
 </ul>
 <li class=MsoNormal>Main effect of branch divergence on GPU:</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal>A) Increases performance</li>
  <li class=MsoNormal>B) Combines memory</li>
  <li class=MsoNormal>C) Reduces resource utilization</li>
  <li class=MsoNormal>D) Changes thread priority</li>
 </ul>
</ol>

<p class=MsoNormal>Use Cases:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Voice recognition processes where some data require more
     processing, causing divergence.</li>
 <li class=MsoNormal>Image processing filters with conditional logic.</li>
</ul>

<p class=MsoNormal>Numerical:<br>
If a warp of 16 threads has 10 taking Path A and 6 taking Path B, what percent
of the time are SIMD lanes idle during branching?<br>
(Solution: First Path A, 6 idle; then Path B, 10 idle. Average idle percentage
= [(6+10)/32]*100% = 50%.)</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 9: Impact of Branch Divergence</b></p>

<p class=MsoNormal>Explanation:<br>
The more threads that diverge, the fewer are active at once. If, in a 16-thread
warp, 8 threads do one task and 8 do another, only half are active at a time,
leading to 50% efficiency of the warp.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQs:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>If 8 out of 16 threads are active at a time due to
     divergence, utilization is:</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal>A) 25%</li>
  <li class=MsoNormal>B) 50%</li>
  <li class=MsoNormal>C) 75%</li>
  <li class=MsoNormal>D) 100%</li>
 </ul>
 <li class=MsoNormal>What happens to the inactive threads during divergence?</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal>A) They process other tasks</li>
  <li class=MsoNormal>B) They remain idle</li>
  <li class=MsoNormal>C) They use more memory</li>
  <li class=MsoNormal>D) They execute both branches</li>
 </ul>
</ol>

<p class=MsoNormal>Use Cases:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Sorting algorithms where partitioning causes divergence.</li>
 <li class=MsoNormal>Path-finding or decision-making in AI code running on GPU.</li>
</ul>

<p class=MsoNormal>Numerical:<br>
Calculate efficiency if, in a 32-thread warp: 12 take Branch X, and 20 take
Branch Y.<br>
(Solution: First execute X, 20 idle; then Y, 12 idle. Total active slots: 32.
Idle slots: 20+12=32. Utilization = 32/(32+32)=50%.)</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 10-12: Thread Block Scheduling and Utilization Quiz</b></p>

<p class=MsoNormal>Explanation:<br>
This section explains how threads are grouped into warps inside blocks. If
blocks do not fill warps completely, the last warp underutilizes available
hardware, lowering overall efficiency.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQs:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is the effect of having blocks that don't fill a full
     warp?</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal>A) 100% utilization</li>
  <li class=MsoNormal>B) Lower efficiency</li>
  <li class=MsoNormal>C) Speed up execution</li>
  <li class=MsoNormal>D) Increase warp size</li>
 </ul>
 <li class=MsoNormal>If a block has 40 threads and the warp size is 32, how
     many warps are fully utilized?</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal>A) 2 full warps</li>
  <li class=MsoNormal>B) 1 full warp and 1 partial</li>
  <li class=MsoNormal>C) 3 full warps</li>
  <li class=MsoNormal>D) 5 warps</li>
 </ul>
</ol>

<p class=MsoNormal>Use Cases:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Large matrix multiplications requiring block and warp
     tuning.</li>
 <li class=MsoNormal>Simulation kernels in scientific computations.</li>
</ul>

<p class=MsoNormal>Numerical:<br>
A kernel launches 3 blocks: each with 36 threads, warp size is 32. How many
threads are underutilized in all blocks?<br>
(Solution: Each block, last warp has 36-32=4 threads. Across 3 blocks: 4×3=12
underutilized threads.)</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 13: Solutions to Branch Divergence</b></p>

<p class=MsoNormal>Explanation:<br>
Fixes include branchless code, reordering input data, grouping threads by
expected behavior, using persistent threads (work queues), or warp intrinsics
for fine control. Some solutions give moderate speedup, but data reordering can
almost eliminate most divergence.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQs:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Data reordering can:</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal>A) Eliminate branch divergence</li>
  <li class=MsoNormal>B) Reduce memory size</li>
  <li class=MsoNormal>C) Stop persistent threads</li>
  <li class=MsoNormal>D) Slow down computation</li>
 </ul>
 <li class=MsoNormal>Using persistent threads is useful for:</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal>A) Uniform workloads</li>
  <li class=MsoNormal>B) Non-uniform/skewed workloads</li>
  <li class=MsoNormal>C) Memory access only</li>
  <li class=MsoNormal>D) Increasing cache size</li>
 </ul>
</ol>

<p class=MsoNormal>Use Cases:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Sorting data by key so similar branches are grouped.</li>
 <li class=MsoNormal>Simulations where workloads vary in complexity.</li>
</ul>

<p class=MsoNormal>Numerical:<br>
If branchless code increases speed by 20%, what's the new runtime if the
original kernel takes 50 ms?<br>
(Solution: New runtime = 50 ms × 0.8 = 40 ms.)</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 14: Persistent Threads Model</b></p>

<p class=MsoNormal>Explanation:<br>
Instead of creating one thread per data item, launch as many threads as needed
to fill all streaming multiprocessors (SMs), and keep them alive, pulling jobs
from a work queue until all tasks are finished.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQs:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Persistent threads:</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal>A) Terminate immediately after a task</li>
  <li class=MsoNormal>B) Stay alive to fetch new work</li>
  <li class=MsoNormal>C) Only used in CPU programming</li>
  <li class=MsoNormal>D) Increase branch divergence</li>
 </ul>
 <li class=MsoNormal>Which model is best for irregular workloads?</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal>A) Standard kernel launches</li>
  <li class=MsoNormal>B) Persistent thread model</li>
  <li class=MsoNormal>C) Single-threaded</li>
  <li class=MsoNormal>D) Pure SIMD</li>
 </ul>
</ol>

<p class=MsoNormal>Use Cases:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>GPU-based rendering for large animations.</li>
 <li class=MsoNormal>Parallel processing of sparse or scattered data.</li>
</ul>

<p class=MsoNormal>Numerical:<br>
If a persistent thread system has 8 SMs and each SM runs one persistent block
with 64 threads, how many threads are running simultaneously?<br>
(Solution: 8 SMs × 64 threads = 512 threads.)</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 15-17: Data Pattern and Warp-Aligned Branching</b></p>

<p class=MsoNormal>Explanation:<br>
If threads in the same warp make the same decisions, there is no warp-level
divergence. Grouping data by expected branch outcome helps ensure all threads
in a warp follow the same path, ensuring full utilization of SIMD hardware.<span
style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQs:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Warp-aligned control flow means:</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal>A) All warps diverge</li>
  <li class=MsoNormal>B) All threads in a warp take the same branch</li>
  <li class=MsoNormal>C) Maximum memory usage</li>
  <li class=MsoNormal>D) Multiple branches per warp</li>
 </ul>
 <li class=MsoNormal>Grouping data by branch patterns:</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal>A) Increases divergence</li>
  <li class=MsoNormal>B) Eliminates intra-warp divergence</li>
  <li class=MsoNormal>C) Reduces speed</li>
  <li class=MsoNormal>D) Destroys data order</li>
 </ul>
</ol>

<p class=MsoNormal>Use Cases:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Sorting inputs for GPU-based classifiers.</li>
 <li class=MsoNormal>Segmenting workloads by characteristics or expected
     results.</li>
</ul>

<p class=MsoNormal>Numerical:<br>
Given 64 threads and two branches, if you group threads so 32 take Branch A
(warp 0) and 32 take Branch B (warp 1), what is the efficiency?<br>
(Solution: Both warps fully utilized, 100% efficiency.)</p>

<p class=MsoNormal>&nbsp;</p>

<div style='border:none;border-bottom:double windowtext 2.25pt;padding:0in 0in 1.0pt 0in'>

<p class=MsoNormal style='border:none;padding:0in'>&nbsp;</p>

</div>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>Thanks, Pankaj! I’ve gone through the slides in your <b>GPU
Programming Lecture 13</b> PDF. Here's how I’ll break it down for each slide:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
     <b>Simple explanation</b></li>
 <li class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
     <b>MCQs</b></li>
 <li class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
     <b>Use cases</b></li>
 <li class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
     <b>Numerical examples</b></li>
</ul>

<p class=MsoNormal>Let’s start with the first few slides. I’ll cover <b>Slides
2 to 5</b> in this message, and continue with the rest once you confirm the
format works for you.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 2: Memory Hierarchy</b></p>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
<b>Explanation</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Shared memory</b> is very fast but limited in size.</li>
 <li class=MsoNormal><b>Local memory</b> is small and not enough for large
     data.</li>
 <li class=MsoNormal><b>Global, Constant, and Texture memory</b> are accessible
     by both CPU and GPU, but slower than shared memory.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
<b>MCQ</b>: <b>Q:</b> Which type of memory is fastest in GPU architecture?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Global memory</li>
 <li class=MsoNormal>B) Shared memory</li>
 <li class=MsoNormal>C) Texture memory</li>
 <li class=MsoNormal>D) Constant memory<br>
     <b>Answer:</b> B) Shared memory</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
<b>Use Case</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Use <b>shared memory</b> for frequently accessed data in
     parallel computations like matrix multiplication.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
<b>Numerical Example</b>: If shared memory access takes <b>1 cycle</b>, and
global memory takes <b>400 cycles</b>, then:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Accessing shared memory 1000 times = 1000 cycles</li>
 <li class=MsoNormal>Accessing global memory 1000 times = 400,000 cycles<br>
     &#8594; Shared memory is <b>400x faster</b> in this case.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 3: GPU Memory Hierarchy</b></p>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
<b>Explanation</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>GPU memory is organized in layers: <b>Registers &#8594;
     Shared &#8594; Global &#8594; Constant/Texture</b>.</li>
 <li class=MsoNormal>Closer to the core = faster access.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
<b>MCQ</b>: <b>Q:</b> Which memory type is closest to GPU cores?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Global memory</li>
 <li class=MsoNormal>B) Registers</li>
 <li class=MsoNormal>C) Texture memory</li>
 <li class=MsoNormal>D) Constant memory<br>
     <b>Answer:</b> B) Registers</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
<b>Use Case</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Use <b>registers</b> for temporary variables in kernel
     functions for fastest access.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
<b>Numerical Example</b>: Assume:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Register access = 0.5 cycles</li>
 <li class=MsoNormal>Shared memory = 1 cycle</li>
 <li class=MsoNormal>Global memory = 400 cycles<br>
     &#8594; Using registers for loop counters or intermediate results saves
     time.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 4: Shared Memory Architecture</b></p>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
<b>Explanation</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Shared memory is divided among threads in a block.</li>
 <li class=MsoNormal>Threads can <b>cooperate</b> using shared memory.</li>
 <li class=MsoNormal>It’s <b>on-chip</b>, so very fast.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
<b>MCQ</b>: <b>Q:</b> Shared memory is shared among:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) All threads in the grid</li>
 <li class=MsoNormal>B) All threads in a warp</li>
 <li class=MsoNormal>C) All threads in a block</li>
 <li class=MsoNormal>D) All threads in the device<br>
     <b>Answer:</b> C) All threads in a block</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
<b>Use Case</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>In <b>image processing</b>, threads in a block can share
     pixel data using shared memory for filtering.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
<b>Numerical Example</b>: If each block has 48 KB shared memory and each thread
uses 1 KB:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Max threads per block = 48 KB / 1 KB = <b>48 threads</b></li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 5: Bank Conflicts</b></p>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
<b>Explanation</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Shared memory is split into <b>banks</b>.</li>
 <li class=MsoNormal>If multiple threads access the same bank at the same time
     &#8594; <b>bank conflict</b> &#8594; slower execution.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
<b>MCQ</b>: <b>Q:</b> What happens during a bank conflict?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Threads are executed in parallel</li>
 <li class=MsoNormal>B) Threads are serialized</li>
 <li class=MsoNormal>C) Threads are skipped</li>
 <li class=MsoNormal>D) Threads are duplicated<br>
     <b>Answer:</b> B) Threads are serialized</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
<b>Use Case</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>In <b>sorting algorithms</b>, avoid bank conflicts by
     aligning memory access patterns.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
<b>Numerical Example</b>: If 4 threads access 4 different banks &#8594;
parallel access = 1 cycle<br>
If 4 threads access same bank &#8594; serialized = 4 cycles<br>
&#8594; Bank conflict causes <b>4x slowdown</b></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal>Would you like me to continue with the next set of slides
(6–10)? Or would you prefer this in a downloadable format like a Word or
PowerPoint file?</p>

<p class=MsoNormal>Sources</p>

<p class=MsoNormal><b>Provide your feedback on BizChat</b></p>

<p class=MsoNormal><b>You said: </b></p>

<p class=MsoNormal>continue</p>

<p class=MsoNormal><b>Copilot said: </b></p>

<p class=MsoNormal>Copilot</p>

<p class=MsoNormal>Great! Let’s continue with <b>Slides 6 to 10</b> from your
GPU Programming Lecture 13.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 6: SIMT Warp Execution on Fermi</b></p>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
<b>Explanation</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>GPUs use <b>SIMT (Single Instruction, Multiple Threads)</b>
     model.</li>
 <li class=MsoNormal>A <b>warp</b> is a group of 32 threads that execute the
     same instruction.</li>
 <li class=MsoNormal>On <b>Fermi architecture</b>, warps are scheduled and
     executed in parallel.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
<b>MCQ</b>: <b>Q:</b> How many threads are typically in a warp on Fermi
architecture?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) 8</li>
 <li class=MsoNormal>B) 16</li>
 <li class=MsoNormal>C) 32</li>
 <li class=MsoNormal>D) 64<br>
     <b>Answer:</b> C) 32</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
<b>Use Case</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>In <b>vector addition</b>, each thread in a warp can
     process one element of the vector simultaneously.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
<b>Numerical Example</b>: If each thread takes 1 cycle to execute an
instruction, and there are 32 threads in a warp:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Total time = 1 cycle (executed in parallel)</li>
 <li class=MsoNormal>If executed sequentially = 32 cycles<br>
     &#8594; SIMT gives <b>32x speedup</b></li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 7: SIMD Execution of Scalar Threads</b></p>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
<b>Explanation</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>SIMD (Single Instruction, Multiple Data)</b> means one
     instruction operates on multiple data points.</li>
 <li class=MsoNormal>GPU threads are scalar but grouped into warps for
     SIMD-like execution.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
<b>MCQ</b>: <b>Q:</b> What does SIMD stand for?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Single Instruction Multiple Devices</li>
 <li class=MsoNormal>B) Single Instruction Multiple Data</li>
 <li class=MsoNormal>C) Single Instruction Multiple Dimensions</li>
 <li class=MsoNormal>D) Single Instruction Multiple Drivers<br>
     <b>Answer:</b> B) Single Instruction Multiple Data</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
<b>Use Case</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Useful in <b>image processing</b>, where the same
     operation (e.g., brightness adjustment) is applied to many pixels.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
<b>Numerical Example</b>: If a SIMD unit processes 4 data points per
instruction and we have 128 data points:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Number of instructions = 128 / 4 = <b>32 instructions</b></li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 8: SIMT – Selective Thread Activation</b></p>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
<b>Explanation</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>In SIMT, threads can be <b>activated or deactivated</b>
     based on conditions.</li>
 <li class=MsoNormal>Only active threads execute instructions; inactive threads
     skip them.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
<b>MCQ</b>: <b>Q:</b> In SIMT, what happens to inactive threads during
execution?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) They execute different instructions</li>
 <li class=MsoNormal>B) They are terminated</li>
 <li class=MsoNormal>C) They remain idle</li>
 <li class=MsoNormal>D) They switch to another warp<br>
     <b>Answer:</b> C) They remain idle</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
<b>Use Case</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>In <b>conditional logic</b>, like filtering data, only
     threads meeting the condition are activated.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
<b>Numerical Example</b>: If 16 threads in a warp and only 10 meet the
condition:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Active threads = 10</li>
 <li class=MsoNormal>Idle threads = 6<br>
     &#8594; Utilization = 10/16 = <b>62.5%</b></li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 9: Problem with Thread Execution</b></p>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
<b>Explanation</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>When threads in a warp follow different paths (due to if-else),
     GPU must <b>serialize</b> execution.</li>
 <li class=MsoNormal>This reduces parallel efficiency.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
<b>MCQ</b>: <b>Q:</b> What is the main issue when threads in a warp follow
different execution paths?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Memory overflow</li>
 <li class=MsoNormal>B) Thread duplication</li>
 <li class=MsoNormal>C) Serialization</li>
 <li class=MsoNormal>D) Deadlock<br>
     <b>Answer:</b> C) Serialization</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
<b>Use Case</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>In <b>decision-based algorithms</b>, like classification,
     serialization can occur if threads take different branches.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
<b>Numerical Example</b>: If 16 threads split into 2 paths:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Path A: 8 threads</li>
 <li class=MsoNormal>Path B: 8 threads<br>
     &#8594; GPU executes A (8 active, 8 idle), then B (8 active, 8 idle)<br>
     &#8594; Effective utilization = <b>50%</b></li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 10: Impact of Branch Divergence</b></p>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
<b>Explanation</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Branch divergence causes <b>low utilization</b> of GPU
     cores.</li>
 <li class=MsoNormal>If half the threads take one path and half take another,
     only half are active at a time.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
<b>MCQ</b>: <b>Q:</b> What is the utilization when half the threads in a warp
are active due to branch divergence?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) 25%</li>
 <li class=MsoNormal>B) 50%</li>
 <li class=MsoNormal>C) 75%</li>
 <li class=MsoNormal>D) 100%<br>
     <b>Answer:</b> B) 50%</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
<b>Use Case</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>In <b>data-dependent branching</b>, like threshold-based
     filtering, divergence can reduce performance.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
<b>Numerical Example</b>: Warp size = 16</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>8 threads &#8594; Path A</li>
 <li class=MsoNormal>8 threads &#8594; Path B<br>
     &#8594; Only 8 threads active at a time<br>
     &#8594; Utilization = 8/16 = <b>50%</b></li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal>Would you like me to continue with <b>Slides 11 to 15</b>
next? Or would you prefer a compiled document or presentation format at this
point?</p>

<p class=MsoNormal><b>Provide your feedback on BizChat</b></p>

<p class=MsoNormal><b>You said: </b></p>

<p class=MsoNormal>yes</p>

<p class=MsoNormal><b>Copilot said: </b></p>

<p class=MsoNormal>Copilot</p>

<p class=MsoNormal>Here’s the continuation with <b>Slides 11 to 15</b>:</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 11: Quick Quiz</b></p>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
<b>Explanation</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>This slide likely presents a quiz to test understanding of
     <b>SIMT</b>, <b>branch divergence</b>, and <b>memory hierarchy</b>
     concepts.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
<b>MCQ (based on previous slides)</b>: <b>Q:</b> What causes warp divergence?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) All threads executing the same path</li>
 <li class=MsoNormal>B) Threads accessing different memory banks</li>
 <li class=MsoNormal>C) Threads in a warp taking different execution paths</li>
 <li class=MsoNormal>D) Threads using shared memory<br>
     <b>Answer:</b> C) Threads in a warp taking different execution paths</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
<b>Use Case</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Use quizzes to reinforce GPU programming concepts in
     training or classroom settings.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
<b>Numerical Example</b>: If 32 threads in a warp split into 10 for Path A and
22 for Path B:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Utilization during Path A = 10/32 = <b>31.25%</b></li>
 <li class=MsoNormal>Utilization during Path B = 22/32 = <b>68.75%</b></li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 12: Solution</b></p>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
<b>Explanation</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>This slide likely shows the correct answer or explanation
     for the quiz.</li>
 <li class=MsoNormal>It may demonstrate how to <b>align threads</b> or <b>reorder
     data</b> to avoid divergence.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
<b>MCQ</b>: <b>Q:</b> What is a common solution to branch divergence?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Increase warp size</li>
 <li class=MsoNormal>B) Use persistent threads</li>
 <li class=MsoNormal>C) Reorder data so threads in a warp follow the same path</li>
 <li class=MsoNormal>D) Use more shared memory<br>
     <b>Answer:</b> C) Reorder data so threads in a warp follow the same path</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
<b>Use Case</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>In <b>data preprocessing</b>, reorder inputs to group
     similar conditions together for better warp efficiency.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
<b>Numerical Example</b>: Before reordering:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Warp 0: 16 threads &#8594; mixed paths &#8594; 50%
     utilization<br>
     After reordering:</li>
 <li class=MsoNormal>Warp 0: 16 threads &#8594; same path &#8594; 100%
     utilization<br>
     &#8594; <b>2x speedup</b></li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 13: Solutions to Branch Divergence</b></p>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
<b>Explanation</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Several strategies to reduce divergence: </li>
 <ol style='margin-top:0in' start=1 type=1>
  <li class=MsoNormal><b>Profile</b> code to find divergence points.</li>
  <li class=MsoNormal>Use <b>branchless</b> logic for small conditions.</li>
  <li class=MsoNormal><b>Reorder inputs</b> to group similar paths.</li>
  <li class=MsoNormal>Use <b>persistent threads</b> for dynamic workloads.</li>
  <li class=MsoNormal>Apply <b>warp intrinsics</b> for fine control.</li>
 </ol>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
<b>MCQ</b>: <b>Q:</b> Which technique helps eliminate most divergence in
data-driven workloads?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Branchless transforms</li>
 <li class=MsoNormal>B) Warp intrinsics</li>
 <li class=MsoNormal>C) Data reordering</li>
 <li class=MsoNormal>D) Persistent threads<br>
     <b>Answer:</b> C) Data reordering</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
<b>Use Case</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>In <b>sorting or filtering</b>, reordering data before GPU
     processing can drastically improve performance.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
<b>Numerical Example</b>: If branchless logic improves performance by 1.5x, and
data reordering improves by 3x:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Original time = 300 ms</li>
 <li class=MsoNormal>Branchless = 200 ms</li>
 <li class=MsoNormal>Reordering = 100 ms</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 14: Persistent Threads Idea</b></p>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
<b>Explanation</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Instead of launching many short-lived threads, launch a
     few <b>long-lived threads</b>.</li>
 <li class=MsoNormal>These threads <b>fetch tasks dynamically</b> from a global
     queue.</li>
 <li class=MsoNormal>Keeps GPU busy and avoids idle time.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
<b>MCQ</b>: <b>Q:</b> What is the key benefit of persistent threads?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Reduced memory usage</li>
 <li class=MsoNormal>B) Increased thread count</li>
 <li class=MsoNormal>C) Better load balancing</li>
 <li class=MsoNormal>D) Faster register access<br>
     <b>Answer:</b> C) Better load balancing</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
<b>Use Case</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>In <b>task-based workloads</b> like ray tracing or graph
     traversal, persistent threads ensure all SMs stay busy.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
<b>Numerical Example</b>: If 1000 tasks and 10 persistent threads:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Each thread processes 100 tasks<br>
     &#8594; No idle time, full GPU utilization<br>
     Compared to launching 1000 threads &#8594; overhead and imbalance</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 15: Warp Divergence</b></p>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
<b>Explanation</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Warp divergence occurs when threads in the same warp take
     different paths.</li>
 <li class=MsoNormal>If threads are grouped poorly, divergence happens multiple
     times.</li>
 <li class=MsoNormal><b>Better grouping</b> can eliminate divergence.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
<b>MCQ</b>: <b>Q:</b> How can warp divergence be reduced?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Increase shared memory</li>
 <li class=MsoNormal>B) Use more threads per block</li>
 <li class=MsoNormal>C) Group threads with similar control flow</li>
 <li class=MsoNormal>D) Use texture memory<br>
     <b>Answer:</b> C) Group threads with similar control flow</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
<b>Use Case</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>In <b>conditional rendering</b>, group pixels with similar
     conditions to avoid warp divergence.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
<b>Numerical Example</b>: Threads 0–15 &#8594; Path A<br>
Threads 16–31 &#8594; Path B<br>
&#8594; 2 warps, each 50% utilized<br>
Reorganize:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Warp 0 &#8594; all Path A</li>
 <li class=MsoNormal>Warp 1 &#8594; all Path B<br>
     &#8594; 100% utilization</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal>Would you like me to wrap up with <b>Slide 16–17</b> next,
or compile everything into a downloadable document or presentation?</p>

<p class=MsoNormal><b>Provide your feedback on BizChat</b></p>

<p class=MsoNormal><b>You said: </b></p>

<p class=MsoNormal>yes</p>

<p class=MsoNormal><b>Copilot said: </b></p>

<p class=MsoNormal>Copilot</p>

<p class=MsoNormal>Here’s the final set of slides from <b>Lecture 13 (Slides
16–17)</b>:</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 16: Example Code – Branch Divergence</b></p>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
<b>Explanation</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>The code shows a conditional operation: </li>
</ul>

<p class=MsoNormal>C++</p>

<p class=MsoNormal>if (data[i] &gt; 0)</p>

<p class=MsoNormal>result[i] = computeA(data[i]);</p>

<p class=MsoNormal>else</p>

<p class=MsoNormal>result[i] = computeB(data[i]);</p>

<p class=MsoNormal>Show more lines</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>This causes <b>branch divergence</b> because threads may
     follow different paths (A or B).</li>
 <li class=MsoNormal>Whether it causes <b>warp divergence</b> depends on how data[i]
     is distributed across threads.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
<b>MCQ</b>: <b>Q:</b> What determines if branch divergence leads to warp
divergence?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Number of threads</li>
 <li class=MsoNormal>B) Distribution of data across threads</li>
 <li class=MsoNormal>C) Size of shared memory</li>
 <li class=MsoNormal>D) Type of kernel<br>
     <b>Answer:</b> B) Distribution of data across threads</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
<b>Use Case</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>In <b>signal processing</b>, if signals are positive or
     negative, threads may diverge. Grouping similar signals avoids warp
     divergence.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
<b>Numerical Example</b>: 64 threads (2 warps of 32):</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>If half threads in each warp follow different paths
     &#8594; 50% utilization</li>
 <li class=MsoNormal>If all threads in a warp follow same path &#8594; 100%
     utilization<br>
     &#8594; Reordering data improves performance</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 17: Warp-Aligned Branching</b></p>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
<b>Explanation</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Threads in the same warp should ideally make the <b>same
     control-flow decisions</b>.</li>
 <li class=MsoNormal>You can still use branching, but ensure it’s <b>warp-aligned</b>
     to avoid divergence.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
<b>MCQ</b>: <b>Q:</b> What is the best practice to avoid warp divergence?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Use more warps</li>
 <li class=MsoNormal>B) Align branching decisions within warps</li>
 <li class=MsoNormal>C) Use constant memory</li>
 <li class=MsoNormal>D) Increase thread block size<br>
     <b>Answer:</b> B) Align branching decisions within warps</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
<b>Use Case</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>In <b>classification tasks</b>, group similar classes
     together so threads in a warp follow the same path.</li>
</ul>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
<b>Numerical Example</b>: If 32 threads in a warp:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>All follow Path A &#8594; 100% utilization</li>
 <li class=MsoNormal>16 follow A, 16 follow B &#8594; 50% utilization<br>
     &#8594; Warp-aligned branching doubles efficiency</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
That wraps up all slides from your <b>GPU Programming Lecture 13</b> with:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Simple explanations</li>
 <li class=MsoNormal>MCQs</li>
 <li class=MsoNormal>Use cases</li>
 <li class=MsoNormal>Numerical examples</li>
</ul>

<p class=MsoNormal>Would you like me to compile this into a <b>PowerPoint
presentation</b>, <b>Word document</b>, or <b>PDF</b> for easy sharing or
revision?</p>

<p class=MsoNormal>&nbsp;</p>

</div>

</body>

</html>
