<html>

<head>
<meta http-equiv=Content-Type content="text/html; charset=windows-1252">
<meta name=Generator content="Microsoft Word 15 (filtered)">
<style>
<!--
 /* Font Definitions */
 @font-face
	{font-family:Wingdings;
	panose-1:5 0 0 0 0 0 0 0 0 0;}
@font-face
	{font-family:"Cambria Math";
	panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
	{font-family:Aptos;}
@font-face
	{font-family:"Segoe UI Emoji";
	panose-1:2 11 5 2 4 2 4 2 2 3;}
 /* Style Definitions */
 p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:8.0pt;
	margin-left:0in;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;}
h1
	{mso-style-link:"Heading 1 Char";
	margin-top:.25in;
	margin-right:0in;
	margin-bottom:4.0pt;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:20.0pt;
	font-family:"Aptos Display",sans-serif;
	color:#0F4761;
	font-weight:normal;}
h2
	{mso-style-link:"Heading 2 Char";
	margin-top:8.0pt;
	margin-right:0in;
	margin-bottom:4.0pt;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:16.0pt;
	font-family:"Aptos Display",sans-serif;
	color:#0F4761;
	font-weight:normal;}
h3
	{mso-style-link:"Heading 3 Char";
	margin-top:8.0pt;
	margin-right:0in;
	margin-bottom:4.0pt;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:14.0pt;
	font-family:"Aptos",sans-serif;
	color:#0F4761;
	font-weight:normal;}
h4
	{mso-style-link:"Heading 4 Char";
	margin-top:4.0pt;
	margin-right:0in;
	margin-bottom:2.0pt;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#0F4761;
	font-weight:normal;
	font-style:italic;}
h5
	{mso-style-link:"Heading 5 Char";
	margin-top:4.0pt;
	margin-right:0in;
	margin-bottom:2.0pt;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#0F4761;
	font-weight:normal;}
h6
	{mso-style-link:"Heading 6 Char";
	margin-top:2.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#595959;
	font-weight:normal;
	font-style:italic;}
p.MsoHeading7, li.MsoHeading7, div.MsoHeading7
	{mso-style-link:"Heading 7 Char";
	margin-top:2.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#595959;}
p.MsoHeading8, li.MsoHeading8, div.MsoHeading8
	{mso-style-link:"Heading 8 Char";
	margin:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#272727;
	font-style:italic;}
p.MsoHeading9, li.MsoHeading9, div.MsoHeading9
	{mso-style-link:"Heading 9 Char";
	margin:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#272727;}
p.MsoTitle, li.MsoTitle, div.MsoTitle
	{mso-style-link:"Title Char";
	margin-top:0in;
	margin-right:0in;
	margin-bottom:4.0pt;
	margin-left:0in;
	font-size:28.0pt;
	font-family:"Aptos Display",sans-serif;
	letter-spacing:-.5pt;}
p.MsoTitleCxSpFirst, li.MsoTitleCxSpFirst, div.MsoTitleCxSpFirst
	{mso-style-link:"Title Char";
	margin:0in;
	font-size:28.0pt;
	font-family:"Aptos Display",sans-serif;
	letter-spacing:-.5pt;}
p.MsoTitleCxSpMiddle, li.MsoTitleCxSpMiddle, div.MsoTitleCxSpMiddle
	{mso-style-link:"Title Char";
	margin:0in;
	font-size:28.0pt;
	font-family:"Aptos Display",sans-serif;
	letter-spacing:-.5pt;}
p.MsoTitleCxSpLast, li.MsoTitleCxSpLast, div.MsoTitleCxSpLast
	{mso-style-link:"Title Char";
	margin-top:0in;
	margin-right:0in;
	margin-bottom:4.0pt;
	margin-left:0in;
	font-size:28.0pt;
	font-family:"Aptos Display",sans-serif;
	letter-spacing:-.5pt;}
p.MsoSubtitle, li.MsoSubtitle, div.MsoSubtitle
	{mso-style-link:"Subtitle Char";
	margin-top:0in;
	margin-right:0in;
	margin-bottom:8.0pt;
	margin-left:0in;
	line-height:115%;
	font-size:14.0pt;
	font-family:"Aptos",sans-serif;
	color:#595959;
	letter-spacing:.75pt;}
a:link, span.MsoHyperlink
	{color:#467886;
	text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
	{color:#96607D;
	text-decoration:underline;}
p.MsoListParagraph, li.MsoListParagraph, div.MsoListParagraph
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:8.0pt;
	margin-left:.5in;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;}
p.MsoListParagraphCxSpFirst, li.MsoListParagraphCxSpFirst, div.MsoListParagraphCxSpFirst
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:.5in;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;}
p.MsoListParagraphCxSpMiddle, li.MsoListParagraphCxSpMiddle, div.MsoListParagraphCxSpMiddle
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:.5in;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;}
p.MsoListParagraphCxSpLast, li.MsoListParagraphCxSpLast, div.MsoListParagraphCxSpLast
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:8.0pt;
	margin-left:.5in;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;}
p.MsoQuote, li.MsoQuote, div.MsoQuote
	{mso-style-link:"Quote Char";
	margin-top:8.0pt;
	margin-right:0in;
	margin-bottom:8.0pt;
	margin-left:0in;
	text-align:center;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#404040;
	font-style:italic;}
p.MsoIntenseQuote, li.MsoIntenseQuote, div.MsoIntenseQuote
	{mso-style-link:"Intense Quote Char";
	margin-top:.25in;
	margin-right:.6in;
	margin-bottom:.25in;
	margin-left:.6in;
	text-align:center;
	line-height:115%;
	border:none;
	padding:0in;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#0F4761;
	font-style:italic;}
span.MsoIntenseEmphasis
	{color:#0F4761;
	font-style:italic;}
span.MsoIntenseReference
	{font-variant:small-caps;
	color:#0F4761;
	letter-spacing:.25pt;
	font-weight:bold;}
span.Heading1Char
	{mso-style-name:"Heading 1 Char";
	mso-style-link:"Heading 1";
	font-family:"Aptos Display",sans-serif;
	color:#0F4761;}
span.Heading2Char
	{mso-style-name:"Heading 2 Char";
	mso-style-link:"Heading 2";
	font-family:"Aptos Display",sans-serif;
	color:#0F4761;}
span.Heading3Char
	{mso-style-name:"Heading 3 Char";
	mso-style-link:"Heading 3";
	font-family:"Times New Roman",serif;
	color:#0F4761;}
span.Heading4Char
	{mso-style-name:"Heading 4 Char";
	mso-style-link:"Heading 4";
	font-family:"Times New Roman",serif;
	color:#0F4761;
	font-style:italic;}
span.Heading5Char
	{mso-style-name:"Heading 5 Char";
	mso-style-link:"Heading 5";
	font-family:"Times New Roman",serif;
	color:#0F4761;}
span.Heading6Char
	{mso-style-name:"Heading 6 Char";
	mso-style-link:"Heading 6";
	font-family:"Times New Roman",serif;
	color:#595959;
	font-style:italic;}
span.Heading7Char
	{mso-style-name:"Heading 7 Char";
	mso-style-link:"Heading 7";
	font-family:"Times New Roman",serif;
	color:#595959;}
span.Heading8Char
	{mso-style-name:"Heading 8 Char";
	mso-style-link:"Heading 8";
	font-family:"Times New Roman",serif;
	color:#272727;
	font-style:italic;}
span.Heading9Char
	{mso-style-name:"Heading 9 Char";
	mso-style-link:"Heading 9";
	font-family:"Times New Roman",serif;
	color:#272727;}
span.TitleChar
	{mso-style-name:"Title Char";
	mso-style-link:Title;
	font-family:"Aptos Display",sans-serif;
	letter-spacing:-.5pt;}
span.SubtitleChar
	{mso-style-name:"Subtitle Char";
	mso-style-link:Subtitle;
	font-family:"Times New Roman",serif;
	color:#595959;
	letter-spacing:.75pt;}
span.QuoteChar
	{mso-style-name:"Quote Char";
	mso-style-link:Quote;
	color:#404040;
	font-style:italic;}
span.IntenseQuoteChar
	{mso-style-name:"Intense Quote Char";
	mso-style-link:"Intense Quote";
	color:#0F4761;
	font-style:italic;}
.MsoPapDefault
	{margin-bottom:8.0pt;
	line-height:115%;}
@page WordSection1
	{size:8.5in 11.0in;
	margin:1.0in 1.0in 1.0in 1.0in;}
div.WordSection1
	{page:WordSection1;}
 /* List Definitions */
 ol
	{margin-bottom:0in;}
ul
	{margin-bottom:0in;}
-->
</style>

</head>

<body lang=EN-US link="#467886" vlink="#96607D" style='word-wrap:break-word'>

<div class=WordSection1>

<p class=MsoNormal><b>Slide 1 – GPU Programming</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
GPU programming involves writing code that runs on the <b>Graphics Processing
Unit (GPU)</b> instead of the CPU. It allows massive <b>parallel computation</b>,
meaning many small operations happen at the same time, making it great for
tasks like image processing or AI.</p>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>GPU programming mainly focuses on:<br>
     a) Sequential execution<br>
     b) Parallel execution <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     c) Power saving<br>
     d) I/O operations</li>
</ol>

<p class=MsoNormal><b>Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Training deep learning models</li>
 <li class=MsoNormal>Image/video rendering</li>
 <li class=MsoNormal>Physics simulations</li>
</ul>

<p class=MsoNormal><b>Numerical Example:</b><br>
If 1000 operations can run in parallel and each takes 2 µs on GPU, total time =
2 µs (not 2000 µs like CPU).</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 2 – CUDA Runtime System</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
The CUDA Runtime system is part of NVIDIA’s CUDA platform. It manages GPU
memory, kernel launches, device synchronization, and provides APIs for C/C++
developers.</p>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>CUDA runtime system is responsible for:<br>
     a) Compiling C code<br>
     b) Managing GPU resources <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     c) Drawing graphics<br>
     d) Scheduling CPU threads</li>
</ol>

<p class=MsoNormal><b>Use Case:</b><br>
Running CUDA kernels without needing to manually handle low-level driver APIs.</p>

<p class=MsoNormal><b>Numerical:</b><br>
Launching a kernel with 256 blocks × 512 threads = <b>131,072 total threads</b>.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 3 – Recap: Warps</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
A <b>warp</b> is a group of 32 threads that execute the same instruction
simultaneously on a GPU. All threads in a warp follow the same execution path.</p>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>How many threads form a warp in CUDA?<br>
     a) 16<br>
     b) 32 <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     c) 64<br>
     d) 128</li>
</ol>

<p class=MsoNormal><b>Use Case:</b><br>
Efficient matrix multiplication—each warp handles a portion of the matrix.</p>

<p class=MsoNormal><b>Numerical:</b><br>
If a block has 128 threads, it contains 128 / 32 = <b>4 warps</b>.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 4 – Context Switching in GPU</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
A <b>context switch</b> occurs when the GPU pauses one task (kernel) and
resumes another. The GPU saves and restores task states (registers, memory
mappings, etc.).</p>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Context switching in GPU means:<br>
     a) Changing CPU states<br>
     b) Saving and loading GPU task states <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     c) Shutting down the GPU<br>
     d) Flushing caches</li>
</ol>

<p class=MsoNormal><b>Use Case:</b><br>
Running both graphics and deep learning workloads on the same GPU.</p>

<p class=MsoNormal><b>Numerical:</b><br>
If each context switch takes 5 µs and happens 100 times per second &#8594;
overhead = 0.5 ms.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 5 – Why Context Switching Happens</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
Occurs due to:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Time-sharing (multiple apps use GPU)</li>
 <li class=MsoNormal>Preemption (higher-priority tasks interrupt lower ones)</li>
 <li class=MsoNormal>Multi-tenancy (cloud sharing)</li>
</ol>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Which is NOT a cause of GPU context switching?<br>
     a) Multi-tenancy<br>
     b) Preemption<br>
     c) Time-sharing<br>
     d) Cache miss <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span></li>
</ol>

<p class=MsoNormal><b>Use Case:</b><br>
Cloud GPU instances where multiple users share hardware.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 6 – CUDA Streams</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
A <b>stream</b> is a sequence of GPU operations that execute in order. Multiple
streams can run concurrently, enabling overlap of computation and data
transfer.</p>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>CUDA streams help in:<br>
     a) Sequential execution<br>
     b) Overlapping data transfer and compute <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     c) Memory allocation only<br>
     d) Debugging</li>
</ol>

<p class=MsoNormal><b>Use Case:</b><br>
Overlapping memory copy (H&#8596;D) with kernel execution for speedup.</p>

<p class=MsoNormal><b>Numerical:</b><br>
If data transfer = 2 ms, kernel = 3 ms, overlapping saves &#8594; 5 ms – 3 ms =
<b>2 ms gain</b>.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 7–8 – Example (Page-Locked Memory)</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
Page-locked (pinned) memory stays in physical RAM, not swapped to disk. GPU can
directly access it using <b>DMA (Direct Memory Access)</b> for faster data
transfer.</p>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Page-locked memory is:<br>
     a) Swappable memory<br>
     b) Fixed in physical RAM <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     c) Virtual memory<br>
     d) GPU cache</li>
</ol>

<p class=MsoNormal><b>Use Case:</b><br>
Used when streaming large datasets (e.g., real-time video frames).</p>

<p class=MsoNormal><b>Numerical:</b><br>
Pinned memory copy = 6 GB/s vs pageable memory = 3 GB/s &#8594; 2× faster.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 9 – Stencil Computation</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
A <b>stencil</b> computes new array values based on neighbors (e.g., smoothing,
diffusion).<br>
Example:<br>
B[i] = (A[i-1] + A[i] + A[i+1]) / 3</p>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is a stencil pattern?<br>
     a) Random data sampling<br>
     b) Neighbor-based computation <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     c) GPU cache layout<br>
     d) Memory coalescing method</li>
</ol>

<p class=MsoNormal><b>Use Case:</b><br>
Used in heat diffusion, blurring filters, and PDE solvers.</p>

<p class=MsoNormal><b>Numerical:</b><br>
If A = [2, 4, 6],<br>
B[2] = (2 + 4 + 6)/3 = <b>4</b>.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 10 – 2D Stencil</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
A 2D stencil uses a point and its 4 neighbors (up, down, left, right) to
compute a new value — like blurring an image pixel.</p>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>A 5-point stencil in 2D uses:<br>
     a) 1 neighbor<br>
     b) 4 neighbors + center <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     c) 8 neighbors<br>
     d) Only diagonals</li>
</ol>

<p class=MsoNormal><b>Use Case:</b><br>
Image smoothing, noise reduction, edge detection.</p>

<p class=MsoNormal><b>Numerical:</b><br>
B[i][j] = (A[i][j] + A[i-1][j] + A[i+1][j] + A[i][j-1] + A[i][j+1])/5</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 11–12 – Implementing with Shared Memory</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
Shared memory allows threads in a block to reuse nearby data, reducing global
memory access and improving speed.</p>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Shared memory is located:<br>
     a) On host RAM<br>
     b) Inside GPU chip <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     c) On disk<br>
     d) In PCIe</li>
</ol>

<p class=MsoNormal><b>Use Case:</b><br>
Speeding up stencil or matrix operations.</p>

<p class=MsoNormal><b>Numerical:</b><br>
If global memory = 400 GB/s, shared = 1 TB/s &#8594; shared is 2.5× faster.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 13 – __syncthreads()</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
A synchronization barrier in CUDA. It ensures all threads in a block reach the
same point before proceeding — important when sharing data.</p>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>__syncthreads() ensures:<br>
     a) Data copying<br>
     b) Thread synchronization <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     c) Memory allocation<br>
     d) Kernel launch</li>
</ol>

<p class=MsoNormal><b>Use Case:</b><br>
Used in tiled matrix multiplication and shared-memory stencil codes.</p>

<p class=MsoNormal><b>Numerical:</b><br>
If 128 threads per block, all 128 must reach __syncthreads() before continuing.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 14 – Stencil Computation (3D Formula)</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
A <b>7-point stencil</b> in 3D uses center + 6 neighbors (±x, ±y, ±z) for
weighted averaging.</p>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>How many points in a 3D 7-point stencil?<br>
     a) 3<br>
     b) 5<br>
     c) 7 <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     d) 9</li>
</ol>

<p class=MsoNormal><b>Use Case:</b><br>
Simulating heat, pressure, or wave propagation in 3D spaces.</p>

<p class=MsoNormal><b>Numerical:</b><br>
B[i,j,k] = 0.25<i>A[i,j,k] + 0.1</i>(A[i±1,j,k]+A[i,j±1,k]+A[i,j,k±1])</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 15 – Example Use Case (Heat Diffusion)</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
Each cell’s new temperature is based on its own and neighbors’ temperatures — a
perfect GPU parallel task.</p>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Heat diffusion uses what stencil type?<br>
     a) 5-point<br>
     b) 7-point <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     c) 3-point<br>
     d) None</li>
</ol>

<p class=MsoNormal><b>Use Case:</b><br>
Thermal simulations, air flow, fluid dynamics.</p>

<p class=MsoNormal><b>Numerical:</b><br>
T[i][j][k] = 0.25<i>T[i][j][k] + 0.1</i>(sum of 6 neighbors)</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 16 – Arithmetic Intensity (AI) Calculation</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
AI = (total floating-point operations) / (total bytes transferred).<br>
Higher AI &#8594; better GPU utilization.</p>

<p class=MsoNormal><b>MCQ:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Arithmetic Intensity measures:<br>
     a) Power usage<br>
     b) Operation per byte <span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span><br>
     c) Memory latency<br>
     d) GPU cores</li>
</ol>

<p class=MsoNormal><b>Use Case:</b><br>
Used in roofline analysis to find performance bottlenecks.</p>

<p class=MsoNormal><b>Numerical:</b><br>
For a 7-point stencil:<br>
13 operations, 7 loads × 4 B = 28 B<br>
AI = 13 / 28 = <b>0.46 OP/B</b></p>

<p class=MsoNormal>&nbsp;</p>

<div style='border:none;border-bottom:double windowtext 2.25pt;padding:0in 0in 1.0pt 0in'>

<p class=MsoNormal style='border:none;padding:0in'>&nbsp;</p>

</div>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal><b>Slide 2: CUDA Runtime System</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
This slide explains how CUDA manages the workload on the GPU. The work is
divided into blocks of threads. The CUDA system has a &quot;to-do list&quot; of
these blocks. It assigns blocks to the Streaming Multiprocessors (SMs) — the
core processing units of the GPU. If a block uses too many resources (like too
many threads or too much shared memory), the system will assign fewer blocks to
that SM to avoid overloading it. The goal is to keep all SMs as busy as
possible.</p>

<p class=MsoNormal><b>MCQ:</b><br>
What does the CUDA runtime system do if a block requires more resources than an
SM has available?<br>
a) It stops the program.<br>
b) It reduces the number of blocks assigned to that SM.<br>
c) It increases the SM's memory.<br>
d) It moves the block to the CPU.<br>
<b>Answer:</b>&nbsp;b) It reduces the number of blocks assigned to that SM.</p>

<p class=MsoNormal><b>Use Case:</b><br>
Imagine you're running a complex video rendering job. The CUDA runtime ensures
that the GPU's processors are never overwhelmed by giving them a manageable
number of tasks at a time, preventing crashes and ensuring smooth operation.</p>

<p class=MsoNormal><b>Numerical:</b><br>
If an SM can hold a maximum of 1536 threads and 48 KB of shared memory, and
each block uses 256 threads and 8 KB of shared memory, what is the maximum
number of blocks that can run on this SM?<br>
<b>Answer:</b>&nbsp;We are limited by both threads and memory.</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Thread limit: 1536 / 256 = 6 blocks.</li>
 <li class=MsoNormal>Memory limit: 48 KB / 8 KB = 6 blocks.<br>
     So, the maximum number of blocks is&nbsp;<b>6</b>.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 3: Recap: Warps</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
When a block is assigned to an SM, it's split into smaller groups called&nbsp;<b>warps</b>&nbsp;(usually
32 threads). Threads in a warp have consecutive IDs. The most important point
is that all 32 threads in a warp execute the&nbsp;<i>same instruction</i>&nbsp;at
the same time, like soldiers marching in step (this is called SIMD). If one
thread in the warp has to wait for data, the entire warp stalls. To avoid
wasting time, the GPU instantly switches to another warp that is ready to run.</p>

<p class=MsoNormal><b>MCQ:</b><br>
How many threads typically form a warp in modern NVIDIA GPUs?<br>
a) 16<br>
b) 32<br>
c) 64<br>
d) 128<br>
<b>Answer:</b>&nbsp;b) 32</p>

<p class=MsoNormal><b>Use Case:</b><br>
In a pixel shader for a game, the same lighting calculation needs to be done
for many pixels. Grouping these pixels into warps allows the GPU to calculate
the color for 32 pixels simultaneously, making the process extremely fast.</p>

<p class=MsoNormal><b>Numerical:</b><br>
A kernel is launched with 150 threads per block. How many warps will be created
per block, and how many threads will be in the last, incomplete warp?<br>
<b>Answer:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Warps per block: 150 / 32 = 4.6875 &#8594;&nbsp;<b>4 full
     warps</b>.</li>
 <li class=MsoNormal>Threads in the last warp: 150 - (4 * 32) = 150 - 128
     =&nbsp;<b>22 threads</b>.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 4 &amp; 5: Context Switching in GPU</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
Context switching is the GPU's ability to pause one task and quickly switch to
another. The &quot;context&quot; is all the information needed to resume a task
later, like a bookmark. The key takeaway from Slide 5 is that GPU context
switching is very fast because the data for an active block (its registers and
shared memory) stays on the SM until the block finishes. The main goal is to
have so many tasks ready to go that the GPU never has to wait for slow memory
operations; the latency is &quot;hidden&quot; by always having other work to
do.</p>

<p class=MsoNormal><b>MCQ:</b><br>
Why is context switching on a GPU very fast compared to a CPU?<br>
a) GPUs have more memory.<br>
b) GPUs do not need to save and restore registers and shared memory for active
blocks.<br>
c) GPUs run at a higher clock speed.<br>
d) GPUs have a simpler instruction set.<br>
<b>Answer:</b>&nbsp;b) GPUs do not need to save and restore registers and
shared memory for active blocks.</p>

<p class=MsoNormal><b>Use Case:</b><br>
On a cloud gaming server, one user might be playing a game (graphics task)
while another is training an AI model (compute task). The GPU rapidly switches
between these tasks, giving the illusion that both are running simultaneously
without slowdown.</p>

<p class=MsoNormal><b>Numerical:</b><br>
If a memory access takes 400 cycles and the GPU can switch to a new warp in 1
cycle, how many warps must be available to completely hide the memory latency?<br>
<b>Answer:</b>&nbsp;To keep the GPU busy, you need enough warps to cover the
wait time. 400 cycles of latency / 1 cycle to switch =&nbsp;<b>400 warps</b>&nbsp;needed
in theory (in practice, other factors also apply).</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 6 &amp; 7: CUDA Streams</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
A&nbsp;<b>stream</b>&nbsp;is like a single-lane road where tasks (like memory
copies and kernel launches) must happen in the order they are placed. By using
multiple streams, you create multiple lanes, allowing tasks from different
streams to overlap and run concurrently. This is crucial for hiding the latency
of data transfers between the CPU and GPU. Slide 7 introduces&nbsp;<b>page-locked
memory</b>, which is special host memory that the OS guarantees will never be
moved to disk, allowing the GPU to copy data to/from it directly and very
efficiently.</p>

<p class=MsoNormal><b>MCQ:</b><br>
What is the primary purpose of using multiple CUDA streams?<br>
a) To make kernels run faster on a single SM.<br>
b) To overlap data transfers and kernel execution.<br>
c) To reduce the total number of threads.<br>
d) To allocate more shared memory.<br>
<b>Answer:</b>&nbsp;b) To overlap data transfers and kernel execution.</p>

<p class=MsoNormal><b>Use Case:</b><br>
In a live video processing application, one stream can be processing the
current frame (kernel execution) while another stream is simultaneously copying
the next frame from the camera to the GPU (memory transfer). This pipelining
ensures smooth, real-time processing.</p>

<p class=MsoNormal><b>Numerical:</b><br>
A kernel execution takes 5 ms, and a memory transfer takes 3 ms. If run in a
single stream, the total time for one cycle (transfer, kernel, transfer back)
is 3+5+3=11 ms. With two streams, the transfers and kernels can overlap. What
is the approximate average time per cycle with two streams?<br>
<b>Answer:</b>&nbsp;With good overlap, the slowest operation (the 5 ms kernel)
often becomes the bottleneck. So, the average time per cycle can approach&nbsp;<b>5
ms</b>, more than doubling the throughput.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 8: Example (Streams and DMA)</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
This slide visually reinforces the concept of streams. It shows that in a
single stream (Stream 0), operations happen one after another. In multiple
streams (Stream 1), operations from different streams can interleave, so a
memory transfer for one task can happen at the same time as a kernel execution
for another task. It also highlights that the GPU uses&nbsp;<b>Direct Memory
Access (DMA)</b>&nbsp;to copy data directly from page-locked host memory
without bothering the CPU.</p>

<p class=MsoNormal><b>MCQ:</b><br>
What is the role of DMA in CUDA?<br>
a) To execute kernel functions.<br>
b) To manage threads within a warp.<br>
c) To copy data between host and device without CPU intervention.<br>
d) To allocate shared memory.<br>
<b>Answer:</b>&nbsp;c) To copy data between host and device without CPU
intervention.</p>

<p class=MsoNormal><b>Use Case:</b><br>
A scientific simulation processes multiple datasets. Using two streams, the GPU
can download Dataset B from the CPU's page-locked memory while it's still
computing on Dataset A, effectively cutting the total simulation time in half.</p>

<p class=MsoNormal><b>Numerical:</b><br>
If a memory transfer (H2D or D2H) takes 2 ms and a kernel takes 4 ms, draw a
simple timeline for two streams showing how the operations (H2D, Kernel, D2H
for two datasets) can overlap to reduce total execution time compared to one
stream.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 9 &amp; 10: Stencil Computation</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
A&nbsp;<b>stencil</b>&nbsp;is a pattern used to update each point in a grid
based on its neighbors. A simple 1D example is making each element in a new
array the average of itself and its left and right neighbors (a 3-point
stencil). Stencils are perfect for GPUs because every output can be calculated
independently (massive parallelism) and they often have predictable memory
access patterns.</p>

<p class=MsoNormal><b>MCQ:</b><br>
In a 1D 3-point stencil, the value of B[i] is calculated using which elements
from array A?<br>
a) A[i] and A[i+1]<br>
b) A[i-2], A[i-1], and A[i]<br>
c) A[i-1], A[i], and A[i+1]<br>
d) A[i], A[i+1], and A[i+2]<br>
<b>Answer:</b>&nbsp;c) A[i-1], A[i], and A[i+1]</p>

<p class=MsoNormal><b>Use Case:</b><br>
<b>Image Blurring:</b>&nbsp;To blur an image, you can use a 2D stencil where
each pixel's new color is the average of itself and its surrounding pixels.
This is a direct application of a stencil operation.</p>

<p class=MsoNormal><b>Numerical:</b><br>
Given the array A = [1, 4, 2, 8, 5], apply a 3-point stencil (average) to
compute array B. Assume boundary elements (B[0] and B[4]) are simply copies of
A[0] and A[4].<br>
<b>Answer:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>B[0] = A[0] = 1</li>
 <li class=MsoNormal>B[1] = (A[0]+A[1]+A[2])/3 = (1+4+2)/3 = 7/3 &#8776; 2.33</li>
 <li class=MsoNormal>B[2] = (A[1]+A[2]+A[3])/3 = (4+2+8)/3 = 14/3 &#8776; 4.67</li>
 <li class=MsoNormal>B[3] = (A[2]+A[3]+A[4])/3 = (2+8+5)/3 = 15/3 = 5.00</li>
 <li class=MsoNormal>B[4] = A[4] = 5<br>
     So, B = [1, 2.33, 4.67, 5.00, 5]</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 11, 12, 13 &amp; 14: Implementing with Shared
Memory</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
These slides show how to optimize a stencil using&nbsp;<b>shared memory</b>.
Global memory (the GPU's main memory) is slow. To speed up the stencil, a block
of threads first loads the data it needs from global memory into a small, fast,
on-chip memory called&nbsp;<b>shared memory</b>. This includes not only the
data for their own output points but also a &quot;halo&quot; of neighboring
points needed for the stencil calculation. Threads then read from this fast
shared memory to perform the computation, which is much faster.</p>

<p class=MsoNormal><b>MCQ:</b><br>
What is the primary reason for using shared memory in a stencil computation?<br>
a) To store the final result permanently.<br>
b) To reduce access latency by caching data reused by multiple threads.<br>
c) To communicate with the CPU.<br>
d) To increase the total global memory available.<br>
<b>Answer:</b>&nbsp;b) To reduce access latency by caching data reused by
multiple threads.</p>

<p class=MsoNormal><b>Use Case:</b><br>
In a&nbsp;<b>3D fluid dynamics simulation</b>, calculating pressure at each
point requires data from all 6 surrounding points. By loading a 3D tile of data
into shared memory, a block of threads can compute on this tile very
efficiently, as each data point fetched from slow global memory is reused many
times by different threads.</p>

<p class=MsoNormal><b>Numerical:</b><br>
For a 1D stencil with a radius of 2, a block has 128 threads. How many elements
must be loaded into shared memory?<br>
<b>Answer:</b>&nbsp;The block needs to calculate 128 output points. Each point
needs 2 neighbors on the left and right. So, total elements = 128 (center) + 2
(left halo) + 2 (right halo) =&nbsp;<b>132 elements</b>.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 15: Example Use Case (Heat Diffusion)</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
This slide provides a concrete example of a 7-point stencil used in a 3D heat
diffusion simulation. The new temperature at a point is a weighted average of
its own current temperature and the temperatures of its six neighbors (up,
down, left, right, front, back). This models how heat spreads through a
material over time.</p>

<p class=MsoNormal><b>MCQ:</b><br>
In the 3D heat diffusion stencil, how many neighboring points are used to
update the value of a single grid point?<br>
a) 4<br>
b) 6<br>
c) 8<br>
d) 10<br>
<b>Answer:</b>&nbsp;b) 6</p>

<p class=MsoNormal><b>Use Case:</b><br>
<b>Engineering Design:</b>&nbsp;Simulating how heat builds up in a processor
chip or an engine block to design better cooling systems.</p>

<p class=MsoNormal><b>Numerical:</b><br>
If the temperature at a point (i,j,k) is 100°, and its six neighbors have
temperatures: 90°, 95°, 85°, 105°, 80°, and 110°, use the formula from the
slide (0.25*center + 0.1*(sum of neighbors)) to calculate the new temperature.<br>
<b>Answer:</b><br>
Sum of neighbors = 90+95+85+105+80+110 = 565.<br>
New Temperature = (0.25 * 100) + (0.1 * 565) = 25 + 56.5 =&nbsp;<b>81.5°</b>.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 16: Arithmetic Intensity (AI) Calculation</b></p>

<p class=MsoNormal><b>Explanation:</b><br>
<b>Arithmetic Intensity (AI)</b>&nbsp;is a key performance metric. It measures
how much computational work (in Floating-Point Operations or FLOPs) you do for
each byte of data moved from memory. A higher AI means you are more
compute-bound (limited by how fast the GPU can calculate), which is good. A
lower AI means you are memory-bound (limited by how fast you can get data).
This slide shows how to calculate AI for a stencil and how using shared memory
(with larger tiles,&nbsp;T) increases AI by reusing data more.</p>

<p class=MsoNormal><b>MCQ:</b><br>
A higher Arithmetic Intensity (AI) indicates that a kernel is more likely to
be:<br>
a) Memory-bound<br>
b) Compute-bound<br>
c) Latency-bound<br>
d) Thread-bound<br>
<b>Answer:</b>&nbsp;b) Compute-bound</p>

<p class=MsoNormal><b>Use Case:</b><br>
When optimizing a deep learning model, a developer might choose a larger batch
size. This increases the AI of the matrix multiplication operations because
more computations are performed on the same weight matrices that are loaded
into memory, leading to better GPU utilization.</p>

<p class=MsoNormal><b>Numerical:</b><br>
Using the slide's formula, calculate the AI for a 3D 7-point stencil with a
tile size of T=10.<br>
<b>Answer:</b><br>
AI = (13 * (T - 2)³) / (4 * T³)<br>
= (13 * (8)³) / (4 * (10)³)<br>
= (13 * 512) / (4 * 1000)<br>
= 6656 / 4000 =&nbsp;<b>1.664 OP/B</b></p>

<p class=MsoNormal>&nbsp;</p>

<div style='border:none;border-bottom:double windowtext 2.25pt;padding:0in 0in 1.0pt 0in'>

<p class=MsoNormal style='border:none;padding:0in'>&nbsp;</p>

</div>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>It's great you're diving into GPU Programming! I'll break
down the concepts from each slide, provide MCQs, potential use cases, and
numerical examples where applicable.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128187;</span>
Slide-by-Slide Explanation and Resources</b></p>

<p class=MsoNormal><b>Slide 2: CUDA Runtime System (RT)</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Explanation:</b> The <b>CUDA Runtime System</b> acts as
     the GPU's operating system and scheduler. It manages the execution of your
     code on the hardware. It assigns blocks of threads to an SM (Streaming
     Multiprocessor)<sup>1</sup>. If the requested number of blocks exceeds the
     SM's resource limits (like threads/SM or shared memory) <sup>22</sup>, the
     runtime <b>automatically reduces</b> the number of blocks assigned until
     the usage is under the limit<sup>3</sup>. It continuously tracks which
     blocks need to run and assigns new ones as the SMs complete previous tasks<sup>4</sup>.</li>
 <li class=MsoNormal><b>Key Concept:</b> Resource Management and Scheduling.</li>
 <li class=MsoNormal><b>Use Case:</b> Automatically manages the execution of a
     large data-parallel job (like a massive matrix multiplication) across the
     GPU's available processing units.</li>
</ul>

<table class=MsoNormalTable border=0 cellspacing=3 cellpadding=0>
 <thead>
  <tr>
   <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
   <p class=MsoNormal><b>Resource Type</b></p>
   </td>
   <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
   <p class=MsoNormal><b>Example (Conceptual)</b></p>
   </td>
  </tr>
 </thead>
 <tr>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal><b>Threads/Block</b></p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Maximum threads allowed in a single block (e.g., 1024) <sup>5</sup></p>
  </td>
 </tr>
 <tr>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal><b>Shared Memory</b></p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Fast, on-chip memory available to threads within a block <sup>6</sup></p>
  </td>
 </tr>
 <tr>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal><b>Threads/SM</b></p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Maximum number of threads an SM can track and schedule
  simultaneously <sup>7</sup></p>
  </td>
 </tr>
</table>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 3: Recap: Warps</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Explanation:</b> Once a <b>block</b> of threads is
     assigned to an SM, it is immediately broken down into smaller execution
     units called <b>warps</b><sup>8</sup>. Threads within a warp have
     consecutive and increasing IDs<sup>9</sup>. All threads in a warp execute
     the <i>same instruction at the same time</i> but on different data (<b>SIMD</b>
     - Single Instruction, Multiple Data)<sup>10</sup>. If a thread in a warp
     is waiting for data, the entire warp <b>stalls</b><sup>11</sup>. To hide
     this waiting time (latency), the SM performs a <b>fast context switch</b>
     to an available, ready warp<sup>12</sup>.</li>
 <li class=MsoNormal><b>Key Concept:</b> Fine-Grained Parallelism and Latency
     Hiding.</li>
 <li class=MsoNormal>Use Case: Any highly parallel task benefits, but
     especially those involving complex memory access patterns where some
     threads might stall, allowing the GPU to keep the SM busy with other
     warps.</li>
</ul>

<p class=MsoNormal>| Numerical Question | Calculation / Answer |</p>

<p class=MsoNormal>| :--- | :--- |</p>

<p class=MsoNormal>| If a block has 256 threads, and a warp size is 32, how
many warps are in the block? | $256 / 32 = **8 \text{ warps**}$. |</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 4 &amp; 5: Context Switching in GPU</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Explanation (CPU vs. GPU):</b> A context switch is the
     process of saving the state of a running task and loading a new one<sup>13</sup>.
     The &quot;context&quot; includes things like thread/warp states
     (registers, program counters) and memory mappings<sup>14</sup>.</li>
 <li class=MsoNormal><b>GPU Context Switching Speed:</b> On a GPU, context
     switching between warps is incredibly <b>fast</b><sup>15</sup>. Registers
     and shared memory, once allocated for a block, are held until the block is
     complete, so they <b>don't need to be saved and restored</b> during a
     simple warp switch<sup>161616</sup>.</li>
 <li class=MsoNormal><b>Goal &amp; Benefit:</b> The main goal of fast context
     switching is to ensure the memory bus is <b>saturated</b><sup>17</sup>. By
     quickly switching to a ready warp, the GPU hides <b>memory latency</b> and
     keeps the hardware busy, a technique known as <b>increasing active threads</b>
     or <b>Instruction Level Parallelism (ILP)</b><sup>18</sup>.</li>
 <li class=MsoNormal><b>Why it Happens:</b></li>
 <ol style='margin-top:0in' start=1 type=1>
  <li class=MsoNormal><b>Time-sharing:</b> Multiple applications (e.g.,
      graphics and CUDA) share the GPU<sup>19191919</sup>.</li>
  <li class=MsoNormal><b>Preemption:</b> A lower-priority compute task might be
      suspended for a higher-priority task, like graphics rendering<sup>20202020</sup>.</li>
  <li class=MsoNormal><b>Multi-tenancy:</b> In cloud environments, tasks from
      different users interleave execution<sup>21212121</sup>.</li>
 </ol>
 <li class=MsoNormal><b>Key Concept:</b> Latency Hiding via Rapid
     Multithreading.</li>
 <li class=MsoNormal><b>Use Case:</b> Running a demanding 3D video game
     (graphics) while simultaneously running an AI model (CUDA compute) in the
     background.</li>
</ul>

<table class=MsoNormalTable border=0 cellspacing=3 cellpadding=0>
 <thead>
  <tr>
   <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
   <p class=MsoNormal><b>MCQ</b></p>
   </td>
   <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
   <p class=MsoNormal><b>Options</b></p>
   </td>
  </tr>
 </thead>
 <tr>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal><b>Q:</b> The primary benefit of fast context switching
  between warps is to:</p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>A) Reduce total execution time. B) <b>Hide memory latency</b>.
  C) Save global memory. D) Increase L2 cache hits.</p>
  </td>
 </tr>
</table>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 6: CUDA Streams</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Explanation:</b> A <b>CUDA Stream</b> is a <b>sequence
     of device operations</b> (like a memory copy or a kernel launch) that are
     executed <b>in order</b><sup>22</sup>. By using multiple streams (stream0,
     stream1, etc.), you can achieve <b>overlap</b> between kernel execution
     and data transfers, improving overall throughput <sup>23232323</sup>. Operations
     in different streams may run concurrently<sup>24</sup>. You can
     synchronize streams with cudaStreamSynchronize()<sup>25</sup>.</li>
 <li class=MsoNormal><b>Key Concept:</b> Concurrent Execution and Overlap of
     Work.</li>
 <li class=MsoNormal>Use Case: Pipelining a large dataset processing task.
     While the GPU is calculating the result for the first chunk of data
     (Kernel 1), the next chunk is being copied to the GPU (memCopy Down 2) and
     the result of a previous chunk is being copied back to the host (memCopy
     Up 1).</li>
</ul>

<p class=MsoNormal>| Numerical Example | Scenario |</p>

<p class=MsoNormal>| :--- | :--- |</p>

<p class=MsoNormal>| Without Streams: | Copy Down 1 (5ms) $\to$ Kernel 1 (10ms)
$\to$ Copy Up 1 (5ms) = 20ms |</p>

<p class=MsoNormal>| With Streams (Overlap): | Copy Down 1 runs, then Kernel 1
starts while Copy Down 2 starts, etc. The total time will be closer to the
longest stage repeated (e.g., 5ms + 10ms + 5ms = 20ms for the first block, but
then subsequent blocks will overlap). |</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 7 &amp; 8: Page-Locked Memory and DMA</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Explanation:</b> To efficiently move data between the
     host (CPU memory) and the device (GPU memory) within a stream, the host
     memory is often allocated as <b>page-locked</b> (or <i>pinned</i>) memory<sup>26</sup>.</li>
 <li class=MsoNormal><b>Page-Locked Memory:</b> This memory is guaranteed by
     the OS to <b>never be swapped</b> to disk's virtual memory<sup>27</sup>. It
     remains in the physical memory of the system<sup>28</sup>. This is
     important because it allows the <b>GPU to use Direct Memory Access (DMA)</b>
     to copy data from this location <b>without intervention from the CPU</b><sup>29</sup>.
     This speeds up transfers<sup>30</sup>.</li>
 <li class=MsoNormal><b>Caution:</b> Page-locked memory is a finite resource,
     so it must be used carefully<sup>31</sup>.</li>
 <li class=MsoNormal><b>Key Concept:</b> Efficient Host-Device Data Transfer.</li>
 <li class=MsoNormal><b>Use Case:</b> High-throughput data processing where
     fast data transfer is critical, like real-time video analysis where frames
     must be moved to the GPU quickly for processing.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 9-11: Stencil Computation (1D)</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Explanation:</b> A <b>Stencil Computation</b> defines a
     pattern of neighboring data points used to compute a new value for a point
     on a grid or array<sup>32</sup>. The &quot;stencil&quot; describes which
     neighbors are used and how they're combined<sup>33</sup>.</li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal><b>1D Example:</b> In a 1D array, calculating $B[i] =
      (A[i-1] + A[i] + A[i+1]) / 3$ is a <b>3-point stencil</b> (center, left,
      right neighbor)<sup>34</sup>.</li>
 </ul>
 <li class=MsoNormal><b>Why Stencils Suit CUDA:</b></li>
</ul>

<ul style='margin-top:0in' type=disc>
 <ol style='margin-top:0in' start=1 type=1>
  <li class=MsoNormal><b>Data Parallelism:</b> Each output element can be
      computed independently, making it ideal for CUDA threads<sup>35</sup>.</li>
  <li class=MsoNormal><b>Regular Memory Access:</b> The patterns are
      predictable, which aids in memory coalescing (efficient access)<sup>36</sup>.</li>
  <li class=MsoNormal><b>Shared Memory:</b> Neighboring data is often reused by
      nearby threads, which can be stored in the fast shared memory<sup>37</sup>.</li>
 </ol>
</ul>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Implementation with Shared Memory:</b> To implement a
     stencil efficiently, a block reads its required input elements <i>plus</i>
     a <b>halo</b> (or ghost) of <i>radius</i> elements at each boundary from
     global memory into fast shared memory<sup>38</sup>. This allows the block
     to compute its <b>blockDim.x</b> output elements using the fast, on-chip
     data<sup>39</sup>. * <b>Key Concept:</b> Localized Computation and Data
     Reuse.</li>
 <li class=MsoNormal><b>Use Case:</b> Image blur/sharpening filters (2D
     stencil) or solving partial differential equations (PDEs) in physics
     simulations (3D stencil).</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 14: 3D Stencil and Arithmetic Intensity (AI)</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Explanation (3D Stencil):</b> The code calculates a
     weighted sum of the <b>center point</b> and its <b>six immediate
     axis-aligned neighbors</b> ($\pm x, \pm y, \pm z$)<sup>40</sup>. This is a
     <b>7-point stencil</b> in 3D<sup>41</sup>. The weights ($c0$ to $c6$) are
     the stencil coefficients<sup>42</sup>.</li>
 <li class=MsoNormal><b>Use Case Example:</b> A <b>heat diffusion simulation</b>
     uses a 7-point stencil to update a grid point's temperature based on its
     current value and the temperature of its 6 neighbors (e.g., $T[i][j][k]$
     is updated based on its neighbors)<sup>43</sup>.</li>
 <li class=MsoNormal><b>Arithmetic Intensity (AI):</b> AI is a measure of
     compute efficiency: $\text{AI} = \frac{\text{Total number of
     operations}}{\text{Total amount of data moved}}$<sup>44</sup>.</li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>For a 3D 7-point stencil, each thread performs <b>13
      floating-point operations</b> (6 additions, 6 multiplications, 1 final
      operation, or similar)<sup>45</sup>.</li>
  <li class=MsoNormal>To compute one output point, 7 input values are loaded
      (center + 6 neighbors)46. Assuming 4 bytes per float47, the initial AI
      is:</li>
 </ul>
</ul>

<p class=MsoNormal>$$\text{AI} = \frac{13 \text{ Ops}}{7 \text{ loads} \times 4
\text{ Bytes/load}} = \frac{13}{28} \approx **0.46 \text{ Ops/B**}$$</p>

<p class=MsoNormal><sup>48</sup></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Impact of Shared Memory (Tiling):</b> When using shared
     memory, data is <i>reused</i>. For a tile of size $T \times T \times T$,
     the number of operations scales as $\approx 13 \times T^3$<sup>49</sup>,
     but the amount of data moved scales as $\approx 4 \times T^3$ (only the <i>surfaces</i>
     of the tile must be loaded as the interior is reused).</li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>The AI for a tile of size $T$ is given by:</li>
 </ul>
</ul>

<p class=MsoNormal>$$\text{AI} = \frac{13 \times (T-2)^3 \text{ Ops}}{4 \times
T^3 \text{ Bytes/load}}$$</p>

<p class=MsoNormal><sup>50</sup></p>

<ul style='margin-top:0in' type=disc>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal><b>Larger $T$ (tile size) increases the AI</b> because
      more input values are reused within the tile<sup>51</sup>.</li>
 </ul>
</ul>

<table class=MsoNormalTable border=0 cellspacing=3 cellpadding=0>
 <thead>
  <tr>
   <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
   <p class=MsoNormal><b>Numerical Example</b></p>
   </td>
   <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
   <p class=MsoNormal><b>Calculation</b></p>
   </td>
  </tr>
 </thead>
 <tr>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Calculate the AI for a $T=4$ tile using the formula: $\text{AI}
  = \frac{13 \times (T-2)^3}{4 \times T^3}$</p>
  </td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'></td>
  <td style='border:solid windowtext 1.0pt;padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>$\text{AI} = \frac{13 \times (4-2)^3}{4 \times 4^3} =
  \frac{13 \times 2^3}{4 \times 64} = \frac{13 \times 8}{256} \approx **0.41
  \text{ Ops/B**}$ <sup>52</sup></p>
  </td>
 </tr>
</table>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<div style='border:none;border-bottom:double windowtext 2.25pt;padding:0in 0in 1.0pt 0in'>

<p class=MsoNormal style='border:none;padding:0in'>&nbsp;</p>

</div>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal><b>GPU Programming Lecture 06 - Comprehensive Guide</b></p>

<p class=MsoNormal>I'll break down each slide with explanations, MCQs, use
cases, and numerical problems.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 3: Recap - Warps</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal>A <b>warp</b> is a group of 32 threads that execute together
on a GPU. Think of it like a bus that carries 32 passengers - they all travel
the same route at the same time. All threads in a warp execute the same
instruction simultaneously (SIMT - Single Instruction, Multiple Threads).</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>How many threads are in a warp?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) 16</li>
  <li class=MsoNormal>b) 32 &#10003;</li>
  <li class=MsoNormal>c) 64</li>
  <li class=MsoNormal>d) 128</li>
 </ul>
 <li class=MsoNormal><b>What happens if threads in a warp take different
     execution paths?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) They execute in parallel</li>
  <li class=MsoNormal>b) Warp divergence occurs &#10003;</li>
  <li class=MsoNormal>c) Program crashes</li>
  <li class=MsoNormal>d) Nothing happens</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Image Processing</b>: Processing 32 pixels
     simultaneously</li>
 <li class=MsoNormal><b>Matrix Operations</b>: Computing 32 elements of a row
     in parallel</li>
 <li class=MsoNormal><b>Physics Simulations</b>: Updating positions of 32
     particles together</li>
</ul>

<p class=MsoNormal><b>Numerical Problem</b></p>

<p class=MsoNormal><b>Q</b>: If you launch a kernel with 256 threads per block,
how many warps are created per block? <b>Solution</b>: 256 threads ÷ 32
threads/warp = <b>8 warps per block</b></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 4: Context Switching in GPU</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal><b>Context switching</b> is like pausing one video game to
play another, then coming back to the first game exactly where you left off.
The GPU saves all information about what it was doing (like which instruction
each thread was on, what values are in registers) so it can resume later.</p>

<p class=MsoNormal><b>What gets saved:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Program counters (which line of code each thread is on)</li>
 <li class=MsoNormal>Register values (variables each thread is using)</li>
 <li class=MsoNormal>Memory mappings (where data is stored)</li>
 <li class=MsoNormal>Which warps were active</li>
</ul>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>What is NOT saved during GPU context switching?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Thread states</li>
  <li class=MsoNormal>b) Register values</li>
  <li class=MsoNormal>c) Power consumption history &#10003;</li>
  <li class=MsoNormal>d) Memory mappings</li>
 </ul>
 <li class=MsoNormal><b>Which requires context switching?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Multiple kernels from same application</li>
  <li class=MsoNormal>b) Graphics and compute tasks sharing GPU &#10003;</li>
  <li class=MsoNormal>c) Single kernel execution</li>
  <li class=MsoNormal>d) Reading from global memory</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Cloud Gaming</b>: Switching between multiple users'
     gaming sessions</li>
 <li class=MsoNormal><b>Multi-tasking Systems</b>: Running machine learning
     training while rendering graphics</li>
 <li class=MsoNormal><b>Virtual Desktop Infrastructure (VDI)</b>: Multiple
     virtual machines sharing one GPU</li>
</ul>

<p class=MsoNormal><b>Numerical Problem</b></p>

<p class=MsoNormal><b>Q</b>: If context switching overhead is 50 microseconds
and you switch between 10 applications every second, what percentage of time is
spent on context switching? <b>Solution</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Time per second = 10 switches × 50 &#956;s = 500 &#956;s =
     0.5 ms</li>
 <li class=MsoNormal>Percentage = (0.5 ms / 1000 ms) × 100 = <b>0.05%</b></li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 5: Why Context Switching Happens in GPUs</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal>Context switching happens for three main reasons:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Time-sharing</b>: Like sharing a playground - multiple
     programs take turns using the GPU</li>
 <li class=MsoNormal><b>Preemption</b>: Urgent tasks (like displaying graphics)
     can interrupt less urgent tasks (like computation)</li>
 <li class=MsoNormal><b>Multi-tenancy</b>: In cloud computing, multiple users
     share the same GPU (like roommates sharing a kitchen)</li>
</ol>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>What is preemption?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Running two tasks simultaneously</li>
  <li class=MsoNormal>b) High-priority task interrupting low-priority task
      &#10003;</li>
  <li class=MsoNormal>c) Deleting a task</li>
  <li class=MsoNormal>d) Allocating memory</li>
 </ul>
 <li class=MsoNormal><b>Multi-tenancy is most common in:</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Personal laptops</li>
  <li class=MsoNormal>b) Gaming consoles</li>
  <li class=MsoNormal>c) Cloud GPU services &#10003;</li>
  <li class=MsoNormal>d) Mobile phones</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Cloud ML Platforms</b>: AWS, Google Cloud sharing GPUs
     among users</li>
 <li class=MsoNormal><b>Gaming + Streaming</b>: GPU rendering game while
     encoding video stream</li>
 <li class=MsoNormal><b>Research Clusters</b>: Multiple researchers submitting
     jobs to shared GPU resources</li>
</ul>

<p class=MsoNormal><b>Numerical Problem</b></p>

<p class=MsoNormal><b>Q</b>: A GPU spends 80% time on graphics (priority 1) and
would spend 100% on compute (priority 2) without preemption. If graphics
preempts compute every 10ms for 8ms, what's compute's effective utilization? <b>Solution</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Compute gets: 10ms - 8ms = 2ms per 10ms cycle</li>
 <li class=MsoNormal>Utilization = (2/10) × 100 = <b>20%</b></li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 6: CUDA Streams</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal><b>CUDA Streams</b> are like checkout lanes at a
supermarket. Each stream is an independent lane where operations (like copying
data, running kernels) happen in order within that lane, but different lanes
work simultaneously.</p>

<p class=MsoNormal><b>Benefits:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Overlap data transfers with computation</li>
 <li class=MsoNormal>Run multiple kernels concurrently</li>
 <li class=MsoNormal>Better GPU utilization</li>
</ul>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Operations in the same CUDA stream execute:</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) In random order</li>
  <li class=MsoNormal>b) Simultaneously</li>
  <li class=MsoNormal>c) Sequentially &#10003;</li>
  <li class=MsoNormal>d) Never</li>
 </ul>
 <li class=MsoNormal><b>What can be overlapped using streams?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Host-to-Device copy and kernel execution &#10003;</li>
  <li class=MsoNormal>b) Two writes to same memory</li>
  <li class=MsoNormal>c) Single thread operations</li>
  <li class=MsoNormal>d) CPU and disk operations</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Video Processing</b>: Decode frame 1 while processing
     frame 2 and encoding frame 3</li>
 <li class=MsoNormal><b>Deep Learning</b>: Transfer next batch while computing
     current batch</li>
 <li class=MsoNormal><b>Real-time Systems</b>: Process sensor data streams
     independently</li>
</ul>

<p class=MsoNormal><b>Numerical Problem</b></p>

<p class=MsoNormal><b>Q</b>: Without streams: Data copy takes 10ms, kernel
takes 30ms, copy back takes 10ms (total 50ms). With streams and 3 batches
pipelined, what's the total time if operations overlap perfectly? <b>Solution</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>First batch: 10 + 30 + 10 = 50ms</li>
 <li class=MsoNormal>Batches 2 &amp; 3: Only add kernel time (30ms each) due to
     overlap</li>
 <li class=MsoNormal>Total = 50 + 30 + 30 = <b>110ms</b> (vs 150ms without
     streams)</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 7-8: Page-Locked Memory Example</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal><b>Page-locked (Pinned) Memory</b> is special host memory
that the operating system promises never to move to disk (swap space). It's
like having a reserved parking spot that's always available.</p>

<p class=MsoNormal><b>Normal memory</b>: Can be swapped to disk if RAM is full <b>Page-locked
memory</b>: Always stays in physical RAM</p>

<p class=MsoNormal><b>Why it matters:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>GPU can directly access it via DMA (Direct Memory Access)</li>
 <li class=MsoNormal>DMA works without CPU involvement</li>
 <li class=MsoNormal>Faster transfers between host and device</li>
</ul>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Page-locked memory is also called:</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Virtual memory</li>
  <li class=MsoNormal>b) Pinned memory &#10003;</li>
  <li class=MsoNormal>c) Cache memory</li>
  <li class=MsoNormal>d) Shared memory</li>
 </ul>
 <li class=MsoNormal><b>What is the risk of using too much page-locked memory?</b>
     </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Faster execution</li>
  <li class=MsoNormal>b) Reduced available system RAM &#10003;</li>
  <li class=MsoNormal>c) GPU overheating</li>
  <li class=MsoNormal>d) No risk</li>
 </ul>
 <li class=MsoNormal><b>DMA stands for:</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Direct Memory Allocation</li>
  <li class=MsoNormal>b) Direct Memory Access &#10003;</li>
  <li class=MsoNormal>c) Dynamic Memory Assignment</li>
  <li class=MsoNormal>d) Data Management Algorithm</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Real-time Video Streaming</b>: Fast frame transfers to
     GPU</li>
 <li class=MsoNormal><b>High-frequency Trading</b>: Minimizing latency in data
     transfers</li>
 <li class=MsoNormal><b>Medical Imaging</b>: Quick transfer of large CT/MRI
     scans</li>
</ul>

<p class=MsoNormal><b>Numerical Problem</b></p>

<p class=MsoNormal><b>Q</b>: Regular memory transfer: 1 GB at 4 GB/s.
Page-locked memory: 1 GB at 12 GB/s. Calculate time saved using page-locked
memory. <b>Solution</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Regular time = 1 GB / 4 GB/s = 0.25 seconds</li>
 <li class=MsoNormal>Page-locked time = 1 GB / 12 GB/s = 0.083 seconds</li>
 <li class=MsoNormal>Time saved = 0.25 - 0.083 = <b>0.167 seconds (167 ms)</b></li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 9: Stencil Computation Introduction</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal>A <b>stencil</b> is a pattern that defines how to compute a
new value based on neighboring values. Imagine applying a blur filter to an
image - each pixel's new value is the average of itself and its neighbors.</p>

<p class=MsoNormal><b>1D Example (3-point stencil):</b></p>

<p class=MsoNormal>B[i] = (A[i-1] + A[i] + A[i+1]) / 3</p>

<p class=MsoNormal>```</p>

<p class=MsoNormal>Each element looks at: left neighbor, itself, right neighbor</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>### MCQs</p>

<p class=MsoNormal>1. **In a 3-point stencil, how many neighbors does each
element use?**</p>

<p class=MsoNormal>   - a) 1</p>

<p class=MsoNormal>   - b) 2 &#10003;</p>

<p class=MsoNormal>   - c) 3</p>

<p class=MsoNormal>   - d) 4</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>2. **Stencil computations are good for GPUs because:**</p>

<p class=MsoNormal>   - a) They require complex branching</p>

<p class=MsoNormal>   - b) They have data parallelism &#10003;</p>

<p class=MsoNormal>   - c) They use random memory access</p>

<p class=MsoNormal>   - d) They need few threads</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>### Use Cases</p>

<p class=MsoNormal>- **Image Blur**: Average pixel with neighbors</p>

<p class=MsoNormal>- **Edge Detection**: Sobel/Prewitt filters</p>

<p class=MsoNormal>- **Audio Processing**: Moving average filters</p>

<p class=MsoNormal>- **Weather Prediction**: Grid-based simulations</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>### Numerical Problem</p>

<p class=MsoNormal>**Q**: Apply 3-point stencil averaging to array A = [10, 20,
30, 40, 50]. Calculate B[2].</p>

<p class=MsoNormal>**Solution**:</p>

<p class=MsoNormal>```</p>

<p class=MsoNormal>B[2] = (A[1] + A[2] + A[3]) / 3</p>

<p class=MsoNormal>B[2] = (20 + 30 + 40) / 3</p>

<p class=MsoNormal>B[2] = 90 / 3 = 30</p>

<p class=MsoNormal>```</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>---</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>## Slide 10: 2D and 3D Stencils</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>### Easy Explanation</p>

<p class=MsoNormal>**2D Stencil (5-point)**: Used in image processing</p>

<p class=MsoNormal>```</p>

<p class=MsoNormal>    [top]</p>

<p class=MsoNormal>[left][center][right]</p>

<p class=MsoNormal>    [bottom]</p>

<p class=MsoNormal>Each point uses 4 neighbors (up, down, left, right) + itself</p>

<p class=MsoNormal><b>3D Stencil (7-point)</b>: Used in physics simulations</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Center point + 6 neighbors (front, back, left, right, up,
     down)</li>
</ul>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>A 5-point 2D stencil uses:</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Only corner neighbors</li>
  <li class=MsoNormal>b) Center + 4 axis-aligned neighbors &#10003;</li>
  <li class=MsoNormal>c) All 8 surrounding points</li>
  <li class=MsoNormal>d) Random 5 points</li>
 </ul>
 <li class=MsoNormal><b>7-point 3D stencil is used for:</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Image filtering</li>
  <li class=MsoNormal>b) Heat diffusion simulation &#10003;</li>
  <li class=MsoNormal>c) Audio processing</li>
  <li class=MsoNormal>d) Text analysis</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<p class=MsoNormal><b>2D Stencils:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Image sharpening/blurring</li>
 <li class=MsoNormal>Gaussian blur</li>
 <li class=MsoNormal>Laplacian edge detection</li>
</ul>

<p class=MsoNormal><b>3D Stencils:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Heat diffusion in 3D objects</li>
 <li class=MsoNormal>Fluid dynamics (Navier-Stokes)</li>
 <li class=MsoNormal>Wave propagation</li>
 <li class=MsoNormal>Seismic modeling</li>
</ul>

<p class=MsoNormal><b>Numerical Problem</b></p>

<p class=MsoNormal><b>Q</b>: For a 100×100 image using 5-point stencil
(ignoring boundaries), how many total stencil operations are performed? <b>Solution</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Interior points = (100-2) × (100-2) = 98 × 98 = 9,604</li>
 <li class=MsoNormal>Each operation uses 5 values</li>
 <li class=MsoNormal>Total memory accesses = 9,604 × 5 = <b>48,020 accesses</b></li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 11-12: Implementing With Shared Memory</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal><b>Problem</b>: Global memory is slow (hundreds of cycles) <b>Solution</b>:
Load data into shared memory (fast, ~1 cycle)</p>

<p class=MsoNormal><b>Strategy:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>Each thread block loads a tile of data into shared memory</li>
 <li class=MsoNormal>Synchronize threads (__syncthreads())</li>
 <li class=MsoNormal>Compute stencil values from shared memory</li>
 <li class=MsoNormal>Write results to global memory</li>
</ol>

<p class=MsoNormal><b>Benefit</b>: Neighboring threads reuse the same data from
fast shared memory instead of slow global memory.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Shared memory is:</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Shared across all thread blocks</li>
  <li class=MsoNormal>b) Shared within a thread block &#10003;</li>
  <li class=MsoNormal>c) Same as global memory</li>
  <li class=MsoNormal>d) Located on CPU</li>
 </ul>
 <li class=MsoNormal><b>Why use shared memory for stencils?</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Unlimited size</li>
  <li class=MsoNormal>b) Neighbors reuse loaded data &#10003;</li>
  <li class=MsoNormal>c) Slower than global memory</li>
  <li class=MsoNormal>d) Required by CUDA</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Convolution Operations</b>: Neural network layers</li>
 <li class=MsoNormal><b>Matrix Multiplication</b>: Tiled approach</li>
 <li class=MsoNormal><b>Image Filtering</b>: 2D stencils with shared memory
     tiles</li>
 <li class=MsoNormal><b>Scientific Computing</b>: PDE solvers</li>
</ul>

<p class=MsoNormal><b>Numerical Problem</b></p>

<p class=MsoNormal><b>Q</b>: Block size 16×16. Global memory latency 400
cycles, shared memory 4 cycles. Without shared memory, each thread accesses 5
values. Calculate speedup. <b>Solution</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Without shared: 5 × 400 = 2000 cycles per thread</li>
 <li class=MsoNormal>With shared: Load once (400 cycles) + 5 × 4 = 420 cycles</li>
 <li class=MsoNormal>Speedup = 2000 / 420 &#8776; <b>4.76x</b></li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 13: __syncthreads()</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal>__syncthreads() is a <b>barrier</b> - like a meeting point
where all threads in a block must arrive before any can continue.</p>

<p class=MsoNormal><b>Why needed:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Ensure all threads finish loading data into shared memory
     before anyone reads it</li>
 <li class=MsoNormal>Prevent race conditions (reading before writing is
     complete)</li>
</ul>

<p class=MsoNormal><b>Important</b>: Only synchronizes threads within the same
block, not across different blocks.</p>

<p class=MsoNormal><b>MCQs</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>__syncthreads() synchronizes:</b> </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) All threads in the grid</li>
  <li class=MsoNormal>b) All threads in a block &#10003;</li>
  <li class=MsoNormal>c) All warps</li>
  <li class=MsoNormal>d) Only one warp</li>
 </ul>
 <li class=MsoNormal><b>What happens without __syncthreads() in stencil code?</b>
     </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal>a) Faster execution</li>
  <li class=MsoNormal>b) Race conditions/incorrect results &#10003;</li>
  <li class=MsoNormal>c) Compilation error</li>
  <li class=MsoNormal>d) Nothing</li>
 </ul>
</ol>

<p class=MsoNormal><b>Use Cases</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Shared Memory Operations</b>: After loading, before
     computing</li>
 <li class=MsoNormal><b>Reduction Operations</b>: Tree-based sum/max operations</li>
 <li class=MsoNormal><b>Matrix Transpose</b>: Ensure all writes complete before
     reads</li>
 <li class=MsoNormal><b>Parallel Prefix Sum</b>: Between computation stages</li>
</ul>

<p class=MsoNormal><b>Numerical Problem</b></p>

<p class=MsoNormal><b>Q</b>: Block has 256 threads. If __syncthreads() takes 10
cycles and you use it 3 times, what's the total synchronization overhead? <b>Solution</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Total overhead = 3 × 10 = <b>30 cycles per block</b>
     (Note: All threads wait, but the time is counted once per synchronization
     point)</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 14: Stencil Computation Code</b></p>

<p class=MsoNormal><b>Easy Explanation</b></p>

<p class=MsoNormal>This code computes a 7-point 3D stencil:</p>

<p class=MsoNormal>cuda</p>

<p class=MsoNormal>out[i,j,k] = c0 * in[i,j,k]           // center</p>

<p class=MsoNormal>           + c1 * in[i,j,k-1]         // left</p>

<p class=MsoNormal>           + c2 * in[i,j,k+1]         // right</p>

<p class=MsoNormal>           + c3 * in[i,j-1,k]         // down</p>

<p class=MsoNormal>           + c4 * in[i,j+1,k]         // up</p>

<p class=MsoNormal>           + c5 * in[i-1,j,k]         // back</p>

<p class=MsoNormal>           + c6 * in[i+1,j,k]         // front</p>

<p class=MsoNormal>```</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>**Indexing**: 3D array flattened to 1D</p>

<p class=MsoNormal>- Position (i,j,k) &#8594; index = i×N×N + j×N + k</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>### MCQs</p>

<p class=MsoNormal>1. **How many memory reads per output element?**</p>

<p class=MsoNormal>   - a) 1</p>

<p class=MsoNormal>   - b) 3</p>

<p class=MsoNormal>   - c) 7 &#10003;</p>

<p class=MsoNormal>   - d) 13</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>2. **What do c0, c1, ..., c6 represent?**</p>

<p class=MsoNormal>   - a) Coordinates</p>

<p class=MsoNormal>   - b) Weights/coefficients &#10003;</p>

<p class=MsoNormal>   - c) Memory addresses</p>

<p class=MsoNormal>   - d) Thread IDs</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>### Use Cases</p>

<p class=MsoNormal>- **Heat Transfer**: Temperature diffusion in 3D objects</p>

<p class=MsoNormal>- **Electromagnetic Simulations**: Field propagation</p>

<p class=MsoNormal>- **Medical Imaging**: 3D denoising/smoothing</p>

<p class=MsoNormal>- **Computational Fluid Dynamics**: Pressure/velocity fields</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>### Numerical Problem</p>

<p class=MsoNormal>**Q**: Grid is 64×64×64, coefficients all equal to 1. What's
the output value if all 7 input values are 100?</p>

<p class=MsoNormal>**Solution**:</p>

<p class=MsoNormal>```</p>

<p class=MsoNormal>out = 1×100 + 1×100 + 1×100 + 1×100 + 1×100 + 1×100 + 1×100</p>

<p class=MsoNormal>out = 7 × 100 = 700</p>

<p class=MsoNormal>```</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>---</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>## Slide 15: Example Use Case - Heat Diffusion</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>### Easy Explanation</p>

<p class=MsoNormal>**Heat Diffusion Equation:**</p>

<p class=MsoNormal>```</p>

<p class=MsoNormal>T_new[i,j,k] = 0.25 × T[i,j,k] + 0.1 × (sum of 6 neighbors)</p>

<p class=MsoNormal>```</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>This simulates how heat spreads through a 3D material:</p>

<p class=MsoNormal>- 25% of current temperature retained</p>

<p class=MsoNormal>- 10% of each neighbor's heat contributes</p>

<p class=MsoNormal>- Each iteration represents a time step</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>### MCQs</p>

<p class=MsoNormal>1. **In heat diffusion, what does T represent?**</p>

<p class=MsoNormal>   - a) Time</p>

<p class=MsoNormal>   - b) Temperature &#10003;</p>

<p class=MsoNormal>   - c) Thread ID</p>

<p class=MsoNormal>   - d) Tile size</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>2. **Why skip boundary cells?**</p>

<p class=MsoNormal>   - a) They don't need computation</p>

<p class=MsoNormal>   - b) They lack all 6 neighbors &#10003;</p>

<p class=MsoNormal>   - c) Faster execution</p>

<p class=MsoNormal>   - d) To save memory</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>### Use Cases</p>

<p class=MsoNormal>- **Thermal Engineering**: CPU/GPU heat dissipation modeling</p>

<p class=MsoNormal>- **Building Design**: HVAC system optimization</p>

<p class=MsoNormal>- **Manufacturing**: Cooling process simulation</p>

<p class=MsoNormal>- **Geology**: Underground temperature distribution</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>### Numerical Problem</p>

<p class=MsoNormal>**Q**: Current temp T[5,5,5] = 100°C, all 6 neighbors =
80°C. Calculate new temperature.</p>

<p class=MsoNormal>**Solution**:</p>

<p class=MsoNormal>```</p>

<p class=MsoNormal>T_new = 0.25 × 100 + 0.1 × (80 + 80 + 80 + 80 + 80 + 80)</p>

<p class=MsoNormal>T_new = 25 + 0.1 × 480</p>

<p class=MsoNormal>T_new = 25 + 48</p>

<p class=MsoNormal>T_new = 73°C</p>

<p class=MsoNormal>```</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>---</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>## Slide 16: Arithmetic Intensity (AI) Calculation</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>### Easy Explanation</p>

<p class=MsoNormal>**Arithmetic Intensity (AI)** = Operations / Bytes
Transferred</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>It measures computational work per byte of data moved -
higher is better!</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>**Without Tiling:**</p>

<p class=MsoNormal>- 13 floating-point operations (7 reads + 6 multiplies/adds)</p>

<p class=MsoNormal>- 7 values × 4 bytes = 28 bytes</p>

<p class=MsoNormal>- AI = 13/28 = **0.46 OP/B** (LOW - memory bound)</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>**With Tiling (size T³):**</p>

<p class=MsoNormal>- Reuse data across (T-2)³ threads</p>

<p class=MsoNormal>- AI = [13 × (T-2)³] / [4 × T³]</p>

<p class=MsoNormal>- Larger T &#8594; better reuse &#8594; higher AI</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>### MCQs</p>

<p class=MsoNormal>1. **What does arithmetic intensity measure?**</p>

<p class=MsoNormal>   - a) Speed of operations</p>

<p class=MsoNormal>   - b) Operations per byte moved &#10003;</p>

<p class=MsoNormal>   - c) Memory size</p>

<p class=MsoNormal>   - d) Number of threads</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>2. **Higher AI means:**</p>

<p class=MsoNormal>   - a) More memory bound</p>

<p class=MsoNormal>   - b) More compute bound &#10003;</p>

<p class=MsoNormal>   - c) Slower execution</p>

<p class=MsoNormal>   - d) Fewer operations</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>3. **Why does larger tile size increase AI?**</p>

<p class=MsoNormal>   - a) More operations</p>

<p class=MsoNormal>   - b) Better data reuse &#10003;</p>

<p class=MsoNormal>   - c) Faster memory</p>

<p class=MsoNormal>   - d) More threads</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>### Use Cases</p>

<p class=MsoNormal>- **Performance Optimization**: Identify memory vs compute
bottlenecks</p>

<p class=MsoNormal>- **Algorithm Selection**: Choose algorithms based on
hardware capabilities</p>

<p class=MsoNormal>- **Roofline Modeling**: Predict maximum achievable
performance</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>### Numerical Problems</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>**Q1**: Calculate AI without tiling.</p>

<p class=MsoNormal>**Solution**:</p>

<p class=MsoNormal>```</p>

<p class=MsoNormal>Operations = 13 (7 reads, 6 arithmetic)</p>

<p class=MsoNormal>Bytes = 7 × 4 = 28 bytes</p>

<p class=MsoNormal>AI = 13 / 28 = 0.464 OP/B</p>

<p class=MsoNormal>```</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>**Q2**: Calculate AI with tile size T = 8.</p>

<p class=MsoNormal>**Solution**:</p>

<p class=MsoNormal>```</p>

<p class=MsoNormal>Active threads = (T-2)³ = 6³ = 216</p>

<p class=MsoNormal>Operations = 13 × 216 = 2,808</p>

<p class=MsoNormal>Bytes = 4 × T³ = 4 × 512 = 2,048</p>

<p class=MsoNormal>AI = 2,808 / 2,048 = 1.37 OP/B</p>

<p class=MsoNormal>```</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>**Q3**: What tile size gives AI &#8805; 2.0?</p>

<p class=MsoNormal>**Solution**:</p>

<p class=MsoNormal>```</p>

<p class=MsoNormal>13(T-2)³ / 4T³ &#8805; 2</p>

<p class=MsoNormal>13(T-2)³ &#8805; 8T³</p>

<p class=MsoNormal>Solving numerically: T &#8805; 11</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Summary Table: Key Concepts</b></p>

<table class=MsoNormalTable border=0 cellspacing=3 cellpadding=0>
 <thead>
  <tr>
   <td style='padding:.75pt .75pt .75pt .75pt'>
   <p class=MsoNormal><b>Concept</b></p>
   </td>
   <td style='padding:.75pt .75pt .75pt .75pt'>
   <p class=MsoNormal><b>Key Point</b></p>
   </td>
   <td style='padding:.75pt .75pt .75pt .75pt'>
   <p class=MsoNormal><b>Optimal Value</b></p>
   </td>
  </tr>
 </thead>
 <tr>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Warp Size</p>
  </td>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>32 threads</p>
  </td>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Fixed by hardware</p>
  </td>
 </tr>
 <tr>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Context Switch</p>
  </td>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Save/restore state</p>
  </td>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Minimize frequency</p>
  </td>
 </tr>
 <tr>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Streams</p>
  </td>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Concurrent execution</p>
  </td>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Use 2-4 streams</p>
  </td>
 </tr>
 <tr>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Page-locked Memory</p>
  </td>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Fast DMA transfers</p>
  </td>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Use sparingly</p>
  </td>
 </tr>
 <tr>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Stencil</p>
  </td>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Neighbor-based computation</p>
  </td>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Tile for reuse</p>
  </td>
 </tr>
 <tr>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Shared Memory</p>
  </td>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Fast on-chip cache</p>
  </td>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>48-96 KB per block</p>
  </td>
 </tr>
 <tr>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>__syncthreads()</p>
  </td>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Block-level barrier</p>
  </td>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Minimize usage</p>
  </td>
 </tr>
 <tr>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Arithmetic Intensity</p>
  </td>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Ops per byte</p>
  </td>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal>Higher is better</p>
  </td>
 </tr>
</table>

<p class=MsoNormal>This comprehensive guide should help you understand each
concept thoroughly</p>

<p class=MsoNormal>&nbsp;</p>

<div style='border:none;border-bottom:double windowtext 2.25pt;padding:0in 0in 1.0pt 0in'>

<p class=MsoNormal style='border:none;padding:0in'>&nbsp;</p>

</div>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal><b>GPU Programming Lecture 06: Slide-by-Slide Breakdown</b></p>

<p class=MsoNormal>Below, I'll break down each slide (corresponding to the PDF
pages) from the lecture. For each slide, I'll provide:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Explanation</b>: A simple, easy-to-understand summary
     in plain language.</li>
 <li class=MsoNormal><b>MCQs</b>: 2-3 multiple-choice questions to test
     understanding, with the correct answer marked.</li>
 <li class=MsoNormal><b>Use Cases</b>: Real-world examples where the concept
     applies.</li>
 <li class=MsoNormal><b>Numericals</b>: Where applicable, a relevant numerical
     problem with step-by-step solution. If the slide doesn't directly involve
     calculations, I'll create a related example based on the content.</li>
</ul>

<p class=MsoNormal>Note: Slides are based on the provided PDF content. Some
slides are introductory or conceptual, so numericals are adapted accordingly.</p>

<p class=MsoNormal><b>Slide 1: GPU Programming Lecture 06</b></p>

<p class=MsoNormal><b>Explanation</b>: This is the title slide introducing the
lecture on GPU programming. It's the starting point for topics like CUDA and
advanced GPU concepts, setting the stage for how GPUs handle parallel tasks
efficiently.</p>

<p class=MsoNormal><b>MCQs</b>:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is the main topic of this lecture? a) CPU Programming
     b) GPU Programming c) Web Development d) Database Management <i>Correct: b</i></li>
 <li class=MsoNormal>What does &quot;Lecture 06&quot; likely indicate? a) The
     sixth lecture in a series on GPU topics b) A version number for software
     c) The number of GPUs needed d) A error code <i>Correct: a</i></li>
</ol>

<p class=MsoNormal><b>Use Cases</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Introducing a course module in computer science classes on
     parallel computing.</li>
 <li class=MsoNormal>As a cover slide in presentations for GPU workshops or
     conferences.</li>
</ul>

<p class=MsoNormal><b>Numericals</b>: Not directly applicable (title slide).
Example: If a course has 10 lectures and this is the 6th, what fraction is
completed? Solution: 6/10 = 0.6 or 60%. Step-by-step: Divide lecture number by
total (6 ÷ 10 = 0.6), multiply by 100 for percentage.</p>

<p class=MsoNormal><b>Slide 2: CUDA Runtime System</b></p>

<p class=MsoNormal><b>Explanation</b>: This slide explains how CUDA manages GPU
resources. Threads are grouped into blocks, and the system automatically
assigns blocks to GPU units (SMs) until resources run out. It keeps a list of
blocks to run and assigns new ones as old ones finish. Resources like threads
per block or shared memory are tracked to keep everything running smoothly.</p>

<p class=MsoNormal><b>MCQs</b>:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What does the CUDA runtime do when resource usage is under
     limit? a) Stops execution b) Assigns blocks to each SM automatically c)
     Deletes blocks d) Ignores new blocks <i>Correct: b</i></li>
 <li class=MsoNormal>Which is an example of SM resources mentioned? a)
     Threads/block b) Internet bandwidth c) Screen resolution d) Keyboard input
     <i>Correct: a</i></li>
 <li class=MsoNormal>What happens to a block once it's assigned to an SM? a)
     It's deleted immediately b) New blocks are assigned as previous ones
     compute c) It runs forever d) It switches to CPU <i>Correct: b</i></li>
</ol>

<p class=MsoNormal><b>Use Cases</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Optimizing AI training where multiple models run on a
     single GPU, ensuring efficient block assignment.</li>
 <li class=MsoNormal>In video game rendering, managing threads for real-time
     graphics without overloading the GPU.</li>
</ul>

<p class=MsoNormal><b>Numericals</b>: Suppose a GPU has 4 SMs, each can handle
2 blocks, and there are 10 blocks total. How many blocks are assigned
initially? Solution: 4 SMs × 2 blocks/SM = 8 blocks. Remaining: 10 - 8 = 2.
Step-by-step: Multiply SMs by blocks per SM for initial assignment, subtract
from total for leftover.</p>

<p class=MsoNormal><b>Slide 3: Recap: Warps</b></p>

<p class=MsoNormal><b>Explanation</b>: This is a quick review of warps in GPUs.
Once a block is on a GPU unit (SM), it's split into smaller groups called
warps. Threads in a warp have consecutive IDs and run together in a lockstep
way (SIMD). If one thread waits for data, the whole warp pauses. The GPU
quickly switches between warps to keep busy, and this switch is super fast.</p>

<p class=MsoNormal><b>MCQs</b>:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>How are thread IDs within a warp arranged? a) Random b)
     Consecutive and increasing c) Decreasing d) Alphabetical <i>Correct: b</i></li>
 <li class=MsoNormal>What happens if an operand isn't ready in a warp? a) The
     warp stalls b) It runs faster c) The GPU shuts down d) Only that thread
     stalls <i>Correct: a</i></li>
 <li class=MsoNormal>Why must context switching between warps be very fast? a)
     To handle stalls without slowing down the GPU b) To increase power usage
     c) To reduce thread count d) To make warps larger <i>Correct: a</i></li>
</ol>

<p class=MsoNormal><b>Use Cases</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>In scientific simulations like weather modeling, where
     many threads process data grids in parallel.</li>
 <li class=MsoNormal>Graphics processing in apps like Photoshop, handling pixel
     operations efficiently.</li>
</ul>

<p class=MsoNormal><b>Numericals</b>: A block has 128 threads, and warps are 32
threads each. How many warps per block? Solution: 128 ÷ 32 = 4 warps.
Step-by-step: Divide total threads by warp size (128 ÷ 32 = 4).</p>

<p class=MsoNormal><b>Slide 4: Context Switching in GPU</b></p>

<p class=MsoNormal><b>Explanation</b>: Context switching on GPUs is like
pausing one job to start another. It saves all the info (like thread states,
memory maps, and schedules) so the paused job can continue later. It happens
for sharing the GPU between apps, handling urgent tasks, or running jobs from
different users in the cloud.</p>

<p class=MsoNormal><b>MCQs</b>:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is a &quot;context&quot; in GPU switching? a) Just
     the program code b) All info needed to resume execution, like thread
     states and memory c) Only caches d) User input <i>Correct: b</i></li>
 <li class=MsoNormal>Why does preemption occur in GPUs? a) To suspend
     low-priority tasks for high-priority ones like graphics b) To delete tasks
     c) To increase latency d) To stop all execution <i>Correct: a</i></li>
 <li class=MsoNormal>In what scenario is multi-tenancy relevant? a) Cloud GPUs
     where multiple users share resources b) Single-user desktops only c)
     Offline mode d) CPU-only systems <i>Correct: a</i></li>
</ol>

<p class=MsoNormal><b>Use Cases</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>In cloud computing platforms like AWS, switching between
     user AI jobs.</li>
 <li class=MsoNormal>Video editing software where rendering (compute) pauses
     for UI updates (graphics).</li>
</ul>

<p class=MsoNormal><b>Numericals</b>: If switching saves 5 states (threads,
memory, etc.) and each takes 2 ms, total time? Solution: 5 × 2 = 10 ms.
Step-by-step: Multiply number of states by time per state.</p>

<p class=MsoNormal><b>Slide 5: Context Switching</b></p>

<p class=MsoNormal><b>Explanation</b>: Registers and shared memory are set
aside for a block as long as it's running. Once a block starts, it stays until
all its threads finish. Switching is quick because no need to save/restore
registers or shared memory often. The goal is to keep memory busy with many
transactions, hiding delays by having more threads or parallel instructions.</p>

<p class=MsoNormal><b>MCQs</b>:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>How long does a block stay active once started? a) Until
     all threads complete b) For 1 second c) Until switched off manually d)
     Indefinitely <i>Correct: a</i></li>
 <li class=MsoNormal>Why is context switching fast in GPUs? a) Registers and
     shared memory don't need saving/restoring often b) It uses more power c)
     It requires CPU help d) It's slow actually <i>Correct: a</i></li>
 <li class=MsoNormal>What hides latency in GPUs? a) More transactions in flight
     or increasing active threads b) Reducing threads c) Stopping memory bus d)
     Ignoring errors <i>Correct: a</i></li>
</ol>

<p class=MsoNormal><b>Use Cases</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>High-performance computing for simulations, where quick
     switches prevent idle time.</li>
 <li class=MsoNormal>Autonomous driving software, switching between sensor
     processing tasks.</li>
</ul>

<p class=MsoNormal><b>Numericals</b>: If a block has 256 threads and takes 10
ms to complete, with 4 blocks active, total time? Solution: Since parallel, ~10
ms (overlapping). Step-by-step: In parallel execution, time is max per block,
not sum.</p>

<p class=MsoNormal><b>Slide 6: CUDA Streams</b></p>

<p class=MsoNormal><b>Explanation</b>: CUDA streams are like queues of tasks on
the GPU that run in order. You create a stream, copy data asynchronously, run
kernels, and sync when needed. This allows overlapping tasks like data copies
and computations for faster performance, as shown in the timeline diagram with
multiple streams.</p>

<p class=MsoNormal><b>MCQs</b>:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is a CUDA stream? a) A sequence of device operations
     executed in order b) A type of memory c) A warp group d) A CPU thread <i>Correct:
     a</i></li>
 <li class=MsoNormal>What does cudaStreamSynchronize do? a) Waits for the
     stream to finish b) Creates a new stream c) Deletes data d) Speeds up
     execution <i>Correct: a</i></li>
 <li class=MsoNormal>In the example, how do streams help with tasks? a) By
     allowing overlapping downloads, kernels, and uploads b) By running
     everything sequentially c) By stopping timers d) By ignoring order <i>Correct:
     a</i></li>
</ol>

<p class=MsoNormal><b>Use Cases</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Machine learning training, overlapping data loading with
     model computations.</li>
 <li class=MsoNormal>Video streaming apps, processing frames while downloading
     next ones.</li>
</ul>

<p class=MsoNormal><b>Numericals</b>: If one stream takes 5 ms for memcpy + 10
ms kernel, with 2 overlapping streams, total time? Solution: ~15 ms (overlap
saves time). Step-by-step: Without overlap: 2 × (5 + 10) = 30 ms; with overlap:
max(5+10, 5+10) but staggered ~15-20 ms depending on overlap.</p>

<p class=MsoNormal><b>Slide 7: Example (CUDA Kernel for Average)</b></p>

<p class=MsoNormal><b>Explanation</b>: This slide shows a simple CUDA kernel
that calculates the average of three values from two arrays (a and b) into c.
First, allocate host memory for arrays as page-locked (pinned) so it stays in
physical memory and doesn't swap to disk. This helps fast GPU copies. Use a
link for more notes.</p>

<p class=MsoNormal><b>MCQs</b>:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What does the kernel compute? a) Average of three values
     from a and b b) Sum only c) Product d) Difference <i>Correct: a</i></li>
 <li class=MsoNormal>Why use page-locked memory? a) It never swaps to disk and
     stays in physical memory b) It's slower c) For CPU only d) To reduce size <i>Correct:
     a</i></li>
 <li class=MsoNormal>What is idx in the kernel? a) Thread index for array
     access b) Block size c) Memory type d) Constant <i>Correct: a</i></li>
</ol>

<p class=MsoNormal><b>Use Cases</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Basic data processing in scientific apps, like averaging
     sensor readings.</li>
 <li class=MsoNormal>Image filtering where pixels are averaged from neighbors.</li>
</ul>

<p class=MsoNormal><b>Numericals</b>: For N=256, idx=0: int1=0%256=0,
int2=2%256=2, float as=(a[0]+a[1]+a[2])/3. Assume a=[1,2,3], b=[4,5,6]:
as=(1+2+3)/3=2, bs=(4+5+6)/3=5, c[0]=(2+5)/2=3.5? Wait, code is c=(as+bs)/2.
Step-by-step: Calculate indices, fetch values, average.</p>

<p class=MsoNormal><b>Slide 8: Example (Page-Locked Memory and DMA)</b></p>

<p class=MsoNormal><b>Explanation</b>: Be careful with page-locked memory since
physical memory is limited. GPUs use DMA (direct memory access) to copy data
from host without CPU help, even for regular memory. This makes transfers
efficient.</p>

<p class=MsoNormal><b>MCQs</b>:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is DMA? a) Direct memory access without CPU
     intervention b) CPU-only copy c) Slow transfer d) Disk swap <i>Correct: a</i></li>
 <li class=MsoNormal>Why be careful with page-locked allocation? a) System has
     finite physical memory b) It's unlimited c) It uses virtual only d) It's
     for GPU only <i>Correct: a</i></li>
 <li class=MsoNormal>Does GPU use DMA for pageable memory? a) Yes, still uses
     DMA b) No, uses CPU c) Only sometimes d) Never <i>Correct: a</i></li>
</ol>

<p class=MsoNormal><b>Use Cases</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Large dataset transfers in big data analytics.</li>
 <li class=MsoNormal>Real-time rendering in games, copying assets quickly.</li>
</ul>

<p class=MsoNormal><b>Numericals</b>: If system has 16 GB physical memory, and
you allocate 4 GB page-locked, remaining? 16 - 4 = 12 GB. Step-by-step:
Subtract allocated from total.</p>

<p class=MsoNormal><b>Slide 9: Stencil Computation</b></p>

<p class=MsoNormal><b>Explanation</b>: A stencil is a pattern used to update
each point in a grid or array based on its neighbors. For example, in a 1D
array, a 3-point stencil averages a point with its left and right neighbors.
Stencils fit GPUs well because they're parallel, have regular memory access,
and can reuse data in shared memory.</p>

<p class=MsoNormal><b>MCQs</b>:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is a stencil? a) Pattern of neighbors to compute new
     values b) A drawing tool c) Memory type d) Thread group <i>Correct: a</i></li>
 <li class=MsoNormal>Why are stencils good for CUDA? a) Data parallelism and
     shared memory reuse b) Sequential only c) No memory access d) For CPU
     better <i>Correct: a</i></li>
 <li class=MsoNormal>In the 1D example, B[i] = ? a) Average of A[i-1], A[i],
     A[i+1] b) Sum c) Product d) Max <i>Correct: a</i></li>
</ol>

<p class=MsoNormal><b>Use Cases</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Image blurring in photo editors (e.g., Gaussian blur).</li>
 <li class=MsoNormal>Weather forecasting models updating grid points.</li>
</ul>

<p class=MsoNormal><b>Numericals</b>: For A = [1, 2, 3, 4], compute B[1] with
3-point stencil. Solution: (1+2+3)/3 = 2. Step-by-step: Sum neighbors + self,
divide by 3.</p>

<p class=MsoNormal><b>Slide 10: 2D Stencil (Image Processing)</b></p>

<p class=MsoNormal><b>Explanation</b>: In 2D (like images), a 5-point stencil
uses up, down, left, right, and center. In 3D, a 7-point adds front/back.
Visualized with diagrams showing the pattern around a central point.</p>

<p class=MsoNormal><b>MCQs</b>:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is a 5-point stencil in 2D? a) Center +
     left/right/up/down b) Only center c) Diagonal only d) 3D points <i>Correct:
     a</i></li>
 <li class=MsoNormal>For image processing, stencils are used for? a) Grids or
     matrices b) Text only c) Audio d) Networks <i>Correct: a</i></li>
 <li class=MsoNormal>A 7-point stencil is for? a) 3D, center + 6 directions b)
     1D c) 2D d) No points <i>Correct: a</i></li>
</ol>

<p class=MsoNormal><b>Use Cases</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Edge detection in computer vision (e.g., Sobel filter).</li>
 <li class=MsoNormal>3D simulations like fluid dynamics in engineering.</li>
</ul>

<p class=MsoNormal><b>Numericals</b>: In 2D grid 3x3, center (1,1)=5,
neighbors=4 each. 5-point average: (5+4+4+4+4)/5=4.2. Step-by-step: Sum all,
divide by count.</p>

<p class=MsoNormal><b>Slide 11: Implementing With Shared Memory</b></p>

<p class=MsoNormal><b>Explanation</b>: To optimize stencils, use shared memory
to cache data. Load more input than needed (block size + halo for boundaries),
compute outputs, then write back. Halo handles edge neighbors.</p>

<p class=MsoNormal><b>MCQs</b>:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is a halo in stencil implementation? a) Extra
     elements at boundaries for neighbors b) Core computation c) Output only d)
     Error <i>Correct: a</i></li>
 <li class=MsoNormal>Steps include? a) Read to shared, compute, write to global
     b) Write first c) No shared memory d) CPU copy <i>Correct: a</i></li>
 <li class=MsoNormal>For radius 1, input load per block? a) blockDim.x +
     2*radius b) blockDim.x only c) Radius only d) Zero <i>Correct: a</i></li>
</ol>

<p class=MsoNormal><b>Use Cases</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Optimizing convolutional neural networks in AI.</li>
 <li class=MsoNormal>Heat transfer simulations in physics engines.</li>
</ul>

<p class=MsoNormal><b>Numericals</b>: BlockDim.x=64, radius=1. Input elements:
64 + 2<i>1 = 66. Step-by-step: Add 2</i>radius to block size.</p>

<p class=MsoNormal><b>Slide 12: Implementing With Shared Memory (Code)</b></p>

<p class=MsoNormal><b>Explanation</b>: Code snippet for 1D stencil using shared
memory. Calculate global and shared indices, load input to shared (handling
halo), sync threads.</p>

<p class=MsoNormal><b>MCQs</b>:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is g_index? a) threadIdx.x + blockIdx.x * blockDim.x
     b) Radius only c) Shared size d) Output <i>Correct: a</i></li>
 <li class=MsoNormal>For loading, if threadIdx.x &lt; RADIUS? a) Load left halo
     b) Skip c) Compute d) Write <i>Correct: a</i></li>
 <li class=MsoNormal>What is BLOCK_SIZE? a) blockDim.x + 2*RADIUS (shared array
     size) b) Input size c) Zero d) Radius <i>Correct: a</i></li>
</ol>

<p class=MsoNormal><b>Use Cases</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Parallel array processing in data analytics.</li>
 <li class=MsoNormal>Signal processing in audio filters.</li>
</ul>

<p class=MsoNormal><b>Numericals</b>: threadIdx.x=0, blockIdx.x=1,
blockDim.x=32, RADIUS=2. g_index=0 + 1<i>32=32. s_index=0+2=2. Step-by-step:
g_index = thread + block</i>dim; s_index = thread + RADIUS.</p>

<p class=MsoNormal><b>Slide 13: __syncthreads()</b></p>

<p class=MsoNormal><b>Explanation</b>: Use __syncthreads() to ensure all
threads have loaded data to shared memory before computing. Code shows syncing
after loading halo and main data.</p>

<p class=MsoNormal><b>MCQs</b>:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What does __syncthreads() do? a) Synchronizes threads in a
     block b) Deletes data c) Computes stencil d) Switches context <i>Correct:
     a</i></li>
 <li class=MsoNormal>When is it called? a) After loading to shared memory b)
     Before loading c) Never d) In CPU <i>Correct: a</i></li>
 <li class=MsoNormal>For if (threadIdx.x &lt; RADIUS)? a) Loads halo data b)
     Writes output c) Syncs d) Stalls <i>Correct: a</i></li>
</ol>

<p class=MsoNormal><b>Use Cases</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Any shared memory kernel, like matrix multiplication.</li>
 <li class=MsoNormal>Synchronization in parallel algorithms like reduction.</li>
</ul>

<p class=MsoNormal><b>Numericals</b>: If 32 threads, RADIUS=3, how many load
halo? 3 (left) + 3 (right, but code handles both). Step-by-step: Threads &lt;
RADIUS load left, &gt; block- RADIUS load right (code variant).</p>

<p class=MsoNormal><b>Slide 14: Stencil Computation (3D Code)</b></p>

<p class=MsoNormal><b>Explanation</b>: Shows a 3D 7-point stencil formula:
output at (i,j,k) is weighted sum of center and 6 neighbors (±x, ±y, ±z) with
coefficients c0-c6. Link to more info.</p>

<p class=MsoNormal><b>MCQs</b>:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>How many points in this stencil? a) 7 (center + 6
     neighbors) b) 3 c) 5 d) 1 <i>Correct: a</i></li>
 <li class=MsoNormal>What is out[i<i>N</i>N + j*N + k]? a) Weighted sum using
     c0 to c6 b) Just center c) Average only d) Product <i>Correct: a</i></li>
 <li class=MsoNormal>Indices represent? a) 3D grid (i,j,k) b) 1D array c) 2D d)
     No grid <i>Correct: a</i></li>
</ol>

<p class=MsoNormal><b>Use Cases</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>3D medical imaging, like MRI smoothing.</li>
 <li class=MsoNormal>Seismic data processing in oil exploration.</li>
</ul>

<p class=MsoNormal><b>Numericals</b>: N=4, i=1,j=1,k=1, c0=1, c1-6=0.5, all
in=2. out=1<i>2 + 0.5</i>2<i>6=2+6=8. Step-by-step: c0</i>center +
sum(c_neighbors * neighbor_values).</p>

<p class=MsoNormal><b>Slide 15: Example Use Case (Heat Diffusion)</b></p>

<p class=MsoNormal><b>Explanation</b>: In heat simulation, update temperature
at each point as 0.25<i>center + 0.1</i>sum of 6 neighbors. Kernel runs one
thread per point, skips boundaries, reads/writes global memory.</p>

<p class=MsoNormal><b>MCQs</b>:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is this stencil for? a) Heat diffusion simulation b)
     Image coloring c) Audio d) Networking <i>Correct: a</i></li>
 <li class=MsoNormal>Coefficients: center=? a) 0.25 b) 0.1 c) 1 d) 0 <i>Correct:
     a</i></li>
 <li class=MsoNormal>What does the kernel skip? a) Boundary cells b) Center c)
     Neighbors d) All <i>Correct: a</i></li>
</ol>

<p class=MsoNormal><b>Use Cases</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Climate modeling for temperature changes.</li>
 <li class=MsoNormal>Material science for heat flow in engines.</li>
</ul>

<p class=MsoNormal><b>Numericals</b>: T[1][1][1]=10, neighbors=5 each. New
T=0.25<i>10 + 0.1</i>(5*6)=2.5 + 3=5.5. Step-by-step: Multiply center, add
neighbor sum scaled.</p>

<p class=MsoNormal><b>Slide 16: Arithmetic Intensity (AI) Calculation</b></p>

<p class=MsoNormal><b>Explanation</b>: For 3D 7-point stencil, AI is operations
per byte. Each thread: 13 ops, 7<i>4=28 bytes (floats). AI=13/28&#8776;0.46.
For tile T^3, ops=13</i>(T-2)^3, bytes=4<i>T^3, AI=13</i>(T-2)^3 / (4*T^3).
Larger T increases reuse and AI.</p>

<p class=MsoNormal><b>MCQs</b>:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal>What is AI? a) Operations / data moved (bytes) b) Threads
     / blocks c) Memory size d) Warps <i>Correct: a</i></li>
 <li class=MsoNormal>For single thread, AI=? a) 13 / (7*4) = 0.46 b) 7/13 c)
     4/13 d) 1 <i>Correct: a</i></li>
 <li class=MsoNormal>How does larger T affect AI? a) Increases due to more
     reuse b) Decreases c) No change d) Halves <i>Correct: a</i></li>
</ol>

<p class=MsoNormal><b>Use Cases</b>:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Performance tuning in HPC apps.</li>
 <li class=MsoNormal>Benchmarking GPU codes for efficiency.</li>
</ul>

<p class=MsoNormal><b>Numericals</b>: T=4. Ops=13*(4-2)^3=13<i>8=104. Bytes=4</i>64=256.
AI=104/256&#8776;0.406. Step-by-step: Compute (T-2)^3, multiply by 13; T^3 *4
for bytes; divide.</p>

<p class=MsoNormal>&nbsp;</p>

<div style='border:none;border-bottom:double windowtext 2.25pt;padding:0in 0in 1.0pt 0in'>

<p class=MsoNormal style='border:none;padding:0in'>&nbsp;</p>

</div>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>Here is a clear explanation of each slide from the attached
lecture, along with MCQs, use cases, and numerical examples for each slide.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 1: CUDA Runtime System</b></p>

<p class=MsoNormal>Explanation:<br>
The CUDA runtime assigns threads to execution resources block by block. It
automatically reduces the number of blocks per Streaming Multiprocessor (SM) if
resources are limited. The system keeps a list of blocks to execute and assigns
new blocks as older ones finish.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
Which of the following best describes how CUDA runtime manages blocks?<br>
A) Assigns all blocks to one SM<br>
B) Reduces blocks per SM if resources are limited<br>
C) Assigns blocks randomly<br>
D) Only assigns blocks when the GPU is idle<br>
Answer:&nbsp;B</p>

<p class=MsoNormal>Use Case:<br>
Running multiple kernels in parallel on a GPU, such as in scientific
simulations or deep learning training.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>Numerical Example:<br>
If an SM can handle 1024 threads and each block has 256 threads, the maximum
number of blocks per SM is&nbsp;1024256=42561024=4.<span style='font-family:
"Arial",sans-serif'>&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 2: Recap Warps</b></p>

<p class=MsoNormal>Explanation:<br>
Once a block is assigned to an SM, it is divided into warps (groups of 32
threads). Threads in a warp execute in SIMD (Single Instruction, Multiple Data)
fashion. If an operand is not ready, the warp stalls, and the GPU quickly
switches to another warp.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
What happens if an operand is not ready in a warp?<br>
A) The warp continues execution<br>
B) The warp stalls and another warp is scheduled<br>
C) The warp is terminated<br>
D) The GPU shuts down<br>
Answer:&nbsp;B</p>

<p class=MsoNormal>Use Case:<br>
Image processing, where each pixel is processed independently by a thread in a
warp.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>Numerical Example:<br>
A block with 256 threads will have&nbsp;25632=832256=8&nbsp;warps.<span
style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 3: Context Switching in GPU</b></p>

<p class=MsoNormal>Explanation:<br>
Context switching in GPUs saves the state of one task and loads another,
allowing the GPU to switch between different applications or kernels. This
includes thread/warp states, registers, memory mappings, and scheduler states.<span
style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
Which of the following is NOT part of GPU context switching?<br>
A) Thread states<br>
B) Memory mappings<br>
C) CPU cache<br>
D) Scheduler states<br>
Answer:&nbsp;C</p>

<p class=MsoNormal>Use Case:<br>
Multi-tenancy in cloud GPUs, where multiple users share the same GPU.<span
style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>Numerical Example:<br>
If a GPU switches between 3 tasks every 10 milliseconds, it performs 100
context switches per second.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 4: Why Context Switching Happens in GPUs</b></p>

<p class=MsoNormal>Explanation:<br>
Context switching happens for time-sharing between applications, preemption
(high-priority tasks), and multi-tenancy in cloud environments.<span
style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
Which is a reason for context switching in GPUs?<br>
A) To increase memory usage<br>
B) To allow time-sharing between applications<br>
C) To reduce GPU temperature<br>
D) To improve CPU performance<br>
Answer:&nbsp;B</p>

<p class=MsoNormal>Use Case:<br>
Running graphics and compute tasks simultaneously on the same GPU.<span
style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>Numerical Example:<br>
If a graphics task preempts a compute task every 50 milliseconds, the compute
task is suspended 20 times per second.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 5: CUDA Streams</b></p>

<p class=MsoNormal>Explanation:<br>
CUDA streams allow overlapping memory transfers and kernel executions. Each
stream is a sequence of operations executed in order, but different streams can
run concurrently.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
What is the main purpose of CUDA streams?<br>
A) To reduce GPU memory<br>
B) To hide memory latency<br>
C) To increase CPU usage<br>
D) To decrease kernel size<br>
Answer:&nbsp;B</p>

<p class=MsoNormal>Use Case:<br>
Real-time signal processing, where memory transfers and kernel computations
overlap.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>Numerical Example:<br>
If a memory transfer takes 10 ms and a kernel takes 10 ms, using two streams
can reduce total time from 20 ms to 10 ms.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 6: Stencil Computation</b></p>

<p class=MsoNormal>Explanation:<br>
Stencil computation updates each element in an array based on its neighbors.
For example, a 3-point stencil averages three neighboring values.<span
style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
What does a 3-point stencil compute?<br>
A) Sum of all array elements<br>
B) Average of three neighboring elements<br>
C) Maximum of three elements<br>
D) Minimum of three elements<br>
Answer:&nbsp;B</p>

<p class=MsoNormal>Use Case:<br>
Heat diffusion simulation, image blurring, and solving partial differential
equations.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>Numerical Example:<br>
For array A = , the 3-point stencil for index 2 gives&nbsp;B=A+A+A3=2+3+43=3<i>B</i>=3<i>A</i>+<i>A</i>+<i>A</i>=32+3+4=3.<span
style='font-family:"Arial",sans-serif'>&#8203;&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 7: Page-Locked Memory</b></p>

<p class=MsoNormal>Explanation:<br>
Page-locked memory is host memory that is guaranteed to stay in physical
memory, allowing faster DMA transfers to the GPU.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
Why is page-locked memory used in CUDA?<br>
A) To reduce CPU usage<br>
B) To allow faster DMA transfers<br>
C) To increase GPU memory<br>
D) To decrease kernel execution time<br>
Answer:&nbsp;B</p>

<p class=MsoNormal>Use Case:<br>
High-performance computing applications requiring fast data transfer between
CPU and GPU.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>Numerical Example:<br>
If a 1 GB array is transferred using page-locked memory, the transfer speed can
be up to 10 GB/s.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 8: Stencil Use Cases</b></p>

<p class=MsoNormal>Explanation:<br>
Stencil computations are ideal for GPUs due to data parallelism, regular memory
access, and shared memory reuse.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
Which is a benefit of stencil computations on GPUs?<br>
A) Irregular memory access<br>
B) Data parallelism<br>
C) High CPU usage<br>
D) Low memory bandwidth<br>
Answer:&nbsp;B</p>

<p class=MsoNormal>Use Case:<br>
Weather forecasting, fluid dynamics, and image processing.<span
style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>Numerical Example:<br>
A 2D stencil on a 1000x1000 grid requires 1 million threads, each processing
one grid point.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 9: Implementing Stencil with Shared Memory</b></p>

<p class=MsoNormal>Explanation:<br>
Shared memory is used to cache data for reuse by threads in the same block,
improving performance.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
What is the purpose of shared memory in stencil computation?<br>
A) To store global memory addresses<br>
B) To cache data for reuse by threads<br>
C) To increase GPU temperature<br>
D) To reduce kernel size<br>
Answer:&nbsp;B</p>

<p class=MsoNormal>Use Case:<br>
Matrix multiplication, convolution, and image filtering.<span style='font-family:
"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>Numerical Example:<br>
A block with 256 threads uses 256 * 4 bytes = 1 KB of shared memory for
caching.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal>Each slide's explanation, MCQ, use case, and numerical
example is provided above for clarity and understanding.</p>

<p class=MsoNormal>all slides</p>

<p class=MsoNormal>Answer</p>

<p class=MsoNormal><img width=12 height=12 id="Picture 2"
src="Slide-6_files/image001.jpg" alt="indico.fysik.su.se favicon"></p>

<p class=MsoNormal>Sources</p>

<p class=MsoNormal><a href="https://khushi-411.github.io/stencil/"
target="_blank"></a></p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal><a
href="https://indico.fysik.su.se/event/6537/contributions/9354/attachments/4029/4628/4.CUDA-StencilsSharedMemory-Markidis.pdf"
target="_blank"></a></p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal><a
href="https://developer.download.nvidia.com/CUDA/CUDA_Zone/papers/gpu_3dfd_rev.pdf"
target="_blank"></a></p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>Here is a detailed, easy-to-understand guide for all the
slides from GPU Programming Lecture 06, including explanations, MCQs, use
cases, and numerical examples for each major topic.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 1: CUDA Runtime System</b></p>

<p class=MsoNormal>Explanation:<br>
CUDA divides the work into blocks and assigns each block to a Streaming
Multiprocessor (SM). If there are too many blocks for the available resources,
CUDA reduces how many blocks go to each SM.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
What happens if there are not enough resources on an SM for all blocks?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) All blocks execute together</li>
 <li class=MsoNormal>B) Some blocks are delayed</li>
 <li class=MsoNormal>C) Blocks are moved to CPU</li>
 <li class=MsoNormal>D) Blocks are canceled<br>
     Answer:&nbsp;B</li>
</ul>

<p class=MsoNormal>Use Case:<br>
Running large matrix calculations or deep learning tasks.<span
style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>Numerical Example:<br>
If an SM supports 1024 threads and each block is 256 threads, at most 4 blocks
fit at once (1024/256=41024/256=4).<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 2: Warps and Their Execution</b></p>

<p class=MsoNormal>Explanation:<br>
After assigning a block to an SM, the block is split into smaller groups called
warps (usually 32 threads each). Warps execute together using the same
instructions (SIMD). If some data isn’t ready, that warp is paused, and a new
one runs.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
How many threads are typically in one CUDA warp?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) 8</li>
 <li class=MsoNormal>B) 16</li>
 <li class=MsoNormal>C) 32</li>
 <li class=MsoNormal>D) 64<br>
     Answer:&nbsp;C</li>
</ul>

<p class=MsoNormal>Use Case:<br>
Parallelizing pixel processing in image filters.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>Numerical Example:<br>
A block of 128 threads becomes 4 warps (128/32=4128/32=4).<span
style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 3: Context Switching in GPU</b></p>

<p class=MsoNormal>Explanation:<br>
Context switching means saving the state of one task and loading another so the
GPU can run different programs or kernels. This includes registers, memory, and
software states.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
Which of the following is saved during a GPU context switch?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Only the program counter</li>
 <li class=MsoNormal>B) Only cache data</li>
 <li class=MsoNormal>C) Thread states, registers, and scheduler states</li>
 <li class=MsoNormal>D) Nothing is saved<br>
     Answer:&nbsp;C</li>
</ul>

<p class=MsoNormal>Use Case:<br>
Allowing multiple users or programs to share a GPU in the cloud.<span
style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>Numerical Example:<br>
If three programs are running on the GPU and each needs context switching every
5 ms, then there are 60 context switches in a minute (3×4×60=7203×4×60=720&nbsp;if
3 programs, 4 per second, for 60 seconds).<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 4: Why Context Switching Happens</b></p>

<p class=MsoNormal>Explanation:<br>
Context switching is needed for time-sharing (sharing between apps), preemption
(handling urgent tasks first), and multi-tenancy (multiple users on shared
hardware).<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
Which is NOT a reason for GPU context switching?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Preemption</li>
 <li class=MsoNormal>B) Time-sharing</li>
 <li class=MsoNormal>C) Multi-tenancy</li>
 <li class=MsoNormal>D) Maximizing single task focus<br>
     Answer:&nbsp;D</li>
</ul>

<p class=MsoNormal>Use Case:<br>
Pausing a simulation when a software like Zoom needs to display video on the
GPU.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>Numerical Example:<br>
If a graphics app preempts compute every 50 ms, then in one second, it preempts
20 times (1000/50=201000/50=20).<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 5: Context Switching Mechanism</b></p>

<p class=MsoNormal>Explanation:<br>
Registers and shared memory for a block are allocated as long as that block is
active. Switching is made fast because only a limited amount must be
saved/restored, mostly avoided by keeping things simple and local.<span
style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
What helps make GPU context switching fast?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Not using shared memory</li>
 <li class=MsoNormal>B) Quickly saving/restoring registers only</li>
 <li class=MsoNormal>C) Never switching</li>
 <li class=MsoNormal>D) Writing to disk<br>
     Answer:&nbsp;B</li>
</ul>

<p class=MsoNormal>Use Case:<br>
Enabling fast switching between graphics and compute tasks so users don’t
notice lag.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>Numerical Example:<br>
If switching between blocks takes 1 microsecond, and there are 1,000 block
switches, it would take 1 millisecond in total (1000×1&nbsp;&#956;s=1&nbsp;ms1000×1&nbsp;<i>&#956;s</i>=1&nbsp;<i>ms</i>).<span
style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 6: CUDA Streams</b></p>

<p class=MsoNormal>Explanation:<br>
A CUDA stream is a set of operations (like memory copy or kernel launch) that
runs in order. Multiple streams can overlap work, like copying data and doing
computation, so things run faster.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
What does a CUDA stream allow?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) More CPU use</li>
 <li class=MsoNormal>B) Overlapping compute and memory copy</li>
 <li class=MsoNormal>C) Only runs one task at a time</li>
 <li class=MsoNormal>D) Stops all other streams<br>
     Answer:&nbsp;B</li>
</ul>

<p class=MsoNormal>Use Case:<br>
Speeding up real-time video analysis by handling data copies and computations
at once.<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>Numerical Example:<br>
A data copy takes 8 ms and a kernel 8 ms—done in sequence, it’s 16 ms. With
streams, both overlap and total time is about 8 ms.<span style='font-family:
"Arial",sans-serif'>&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 7: Page-Locked (Pinned) Memory</b></p>

<p class=MsoNormal>Explanation:<br>
Page-locked memory stays in physical RAM and can’t be swapped to disk, so data
transfer to the GPU is much faster using direct memory access (DMA).<span
style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
What is the main benefit of page-locked memory in CUDA?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Extra security</li>
 <li class=MsoNormal>B) Faster transfers to GPU</li>
 <li class=MsoNormal>C) Saves disk space</li>
 <li class=MsoNormal>D) Slower execution<br>
     Answer:&nbsp;B</li>
</ul>

<p class=MsoNormal>Use Case:<br>
Large, frequent data transfers in scientific applications.<span
style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>Numerical Example:<br>
If page-locked transfer rate is 6 GB/s versus normal memory’s 2 GB/s,
transferring 12 GB takes 2 seconds with page-locked and 6 seconds without.<span
style='font-family:"Arial",sans-serif'>&#8203;&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 8: Stencil Computation</b></p>

<p class=MsoNormal>Explanation:<br>
A stencil operation updates elements in a grid/array based on their neighbors.
For example, each value becomes the average of itself and the two nearest
values (a 3-point stencil in 1D).<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
What does a 3-point stencil compute?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) The sum of all</li>
 <li class=MsoNormal>B) The average of 3 nearby values</li>
 <li class=MsoNormal>C) Only the first value</li>
 <li class=MsoNormal>D) Finds the max<br>
     Answer:&nbsp;B</li>
</ul>

<p class=MsoNormal>Use Case:<br>
Simulating temperature, image smoothing, or physical phenomena.<span
style='font-family:"Arial",sans-serif'>&#8203;&#8203;</span></p>

<p class=MsoNormal>Numerical Example:<br>
For array A = , B = (A + A + A)/3 = (3+6+9)/3 = 6.<span style='font-family:
"Arial",sans-serif'>&#8203;&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 9: Why Stencils Are Good for GPUs</b></p>

<p class=MsoNormal>Explanation:<br>
Stencil computations are good for GPUs since each output can be calculated
separately, memory access is regular, and nearby threads can reuse data in
shared memory.<span style='font-family:"Arial",sans-serif'>&#8203;&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
Why are stencils efficient on GPUs?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Random access</li>
 <li class=MsoNormal>B) Data parallelism and reuse</li>
 <li class=MsoNormal>C) Needs more CPU work</li>
 <li class=MsoNormal>D) Uses only disk storage<br>
     Answer:&nbsp;B</li>
</ul>

<p class=MsoNormal>Use Case:<br>
Parallel scientific models, like heat diffusion.<span style='font-family:"Arial",sans-serif'>&#8203;&#8203;</span></p>

<p class=MsoNormal>Numerical Example:<br>
A 2D image filter (e.g., 1000x1000 pixels), where each output pixel can be
handled by its own thread, totaling a million threads.<span style='font-family:
"Arial",sans-serif'>&#8203;&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 10: 2D and 3D Stencils</b></p>

<p class=MsoNormal>Explanation:<br>
Stencils in 2D use more neighbors—like, each pixel’s new value depends on
itself plus 4 or more neighboring pixels. In 3D, even more neighbors are used
for updating values in a volume.<span style='font-family:"Arial",sans-serif'>&#8203;&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
How many neighbors in a 2D 5-point stencil?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) 1</li>
 <li class=MsoNormal>B) 2</li>
 <li class=MsoNormal>C) 4</li>
 <li class=MsoNormal>D) 5<br>
     Answer:&nbsp;D</li>
</ul>

<p class=MsoNormal>Use Case:<br>
3D physics simulations or blurring volumetric images.<span style='font-family:
"Arial",sans-serif'>&#8203;&#8203;</span></p>

<p class=MsoNormal>Numerical Example:<br>
In a 3D grid of 20x20x20 entries, using a 7-point stencil, each point (except
boundaries) reads 6 neighbors.<span style='font-family:"Arial",sans-serif'>&#8203;&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 11: Implementing with Shared Memory</b></p>

<p class=MsoNormal>Explanation:<br>
Threads in the same block can load data into shared memory, read only once from
slow global memory, and then reuse it while updating multiple output cells.<span
style='font-family:"Arial",sans-serif'>&#8203;&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
Why use shared memory for stencils?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) It slows down performance</li>
 <li class=MsoNormal>B) To reuse neighbor data quickly</li>
 <li class=MsoNormal>C) Increases memory use</li>
 <li class=MsoNormal>D) Only for CPUs<br>
     Answer:&nbsp;B</li>
</ul>

<p class=MsoNormal>Use Case:<br>
Convolution in deep learning, matrix operations.<span style='font-family:"Arial",sans-serif'>&#8203;&#8203;</span></p>

<p class=MsoNormal>Numerical Example:<br>
A block with 128 threads and a radius-1 stencil would need 130 shared memory
slots (128+2).<span style='font-family:"Arial",sans-serif'>&#8203;&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 12: Synchronization in CUDA</b></p>

<p class=MsoNormal>Explanation:<br>
Threads must sometimes wait for each other after loading shared data to avoid
race conditions (using __syncthreads()).<span style='font-family:"Arial",sans-serif'>&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
What is the purpose of __syncthreads() in CUDA?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Start all threads</li>
 <li class=MsoNormal>B) Wait until all threads reach a point</li>
 <li class=MsoNormal>C) Stop all computation</li>
 <li class=MsoNormal>D) Only used in CPU code<br>
     Answer:&nbsp;B</li>
</ul>

<p class=MsoNormal>Use Case:<br>
Making sure data is safely loaded before proceeding with computation.<span
style='font-family:"Arial",sans-serif'>&#8203;&#8203;</span></p>

<p class=MsoNormal>Numerical Example:<br>
With 128 threads per block, all must call __syncthreads() before moving to the
next step.<span style='font-family:"Arial",sans-serif'>&#8203;&#8203;</span></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=1 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 13: Arithmetic Intensity (AI)</b></p>

<p class=MsoNormal>Explanation:<br>
AI measures the number of computations per byte of data moved. Higher AI means
the program does more work on each byte and is usually faster on GPUs.<span
style='font-family:"Arial",sans-serif'>&#8203;&#8203;</span></p>

<p class=MsoNormal>MCQ:<br>
Which code typically runs faster on a GPU?</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>A) Low AI program</li>
 <li class=MsoNormal>B) High AI program</li>
 <li class=MsoNormal>C) All AI values are equal</li>
 <li class=MsoNormal>D) Zero AI<br>
     Answer:&nbsp;B</li>
</ul>

<p class=MsoNormal>Use Case:<br>
Optimizing performance in scientific codes or simulations.<span
style='font-family:"Arial",sans-serif'>&#8203;&#8203;</span></p>

<p class=MsoNormal>Numerical Example:<br>
A 3D stencil with 13 floating point op/point and 28 bytes per point: AI = 13/28
&#8776; 0.46 Op/Byte.<span style='font-family:"Arial",sans-serif'>&#8203;&#8203;</span></p>

<p class=MsoNormal>&nbsp;</p>

<div style='border:none;border-bottom:double windowtext 2.25pt;padding:0in 0in 1.0pt 0in'>

<p class=MsoNormal style='border:none;padding:0in'>&nbsp;</p>

</div>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>Of course! Here is a detailed breakdown of each slide from
the provided document, including simple explanations, multiple-choice questions
(MCQs), use cases, and numerical problems as requested.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 1: GPU Programming Lecture 06</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> This is the title slide for the
     sixth lecture on GPU programming. It introduces the main topics that will
     be covered in this session, starting with the CUDA Runtime system.</li>
 <li class=MsoNormal><b>MCQ:</b> What is the main topic of this lecture? a) CPU
     Architecture b) GPU Programming Concepts c) Web Development d) Database
     Management</li>
 <li class=MsoNormal><b>Use Case:</b> This slide sets the stage for a
     university course or a technical training session on parallel computing
     using GPUs. A student would use this to understand the lecture's scope.</li>
 <li class=MsoNormal><b>Numerical:</b> Not applicable for a title slide.</li>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 2: Recap: Warps</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> A &quot;warp&quot; is the basic
     unit of execution in an NVIDIA GPU. Think of it as a team of 32 threads
     that all work together. The most important thing is that all threads in a
     warp must execute the exact same instruction at the same time, just like a
     marching band where everyone takes the same step simultaneously. This is
     called SIMD (Single Instruction, Multiple Data).</li>
 <li class=MsoNormal><b>MCQ:</b> How many threads typically make up a warp in a
     modern NVIDIA GPU? a) 8 b) 16 c) 32 d) 64</li>
 <li class=MsoNormal><b>Use Case:</b> When you write a CUDA kernel to process a
     large image, the GPU doesn't run one thread at a time. It groups your
     threads into warps of 32. If you launch 3200 threads, the GPU will
     schedule them as 100 warps, executing them in parallel to apply a filter to
     the image much faster than a CPU.</li>
 <li class=MsoNormal><b>Numerical:</b> If you launch a CUDA kernel with a grid
     of 64 blocks, and each block has 256 threads, what is the total number of
     warps that will be created?</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal><b>Solution:</b></li>
  <ul style='margin-top:0in' type=disc>
   <li class=MsoNormal>Total threads = 64 blocks * 256 threads/block = 16,384
       threads.</li>
   <li class=MsoNormal>Total warps = Total threads / 32 threads/warp = 16,384 /
       32 = 512 warps.</li>
  </ul>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 3: Context Switching in GPU</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> Imagine you are cooking (one
     task) and the phone rings (another task). You stop cooking, remember where
     you were (save state), answer the phone (load new state), and then go back
     to cooking (restore state). A GPU does the same thing. It can be working on
     a scientific calculation (a kernel) and then switch to rendering graphics
     for a game. &quot;Context switching&quot; is the process of saving all the
     necessary information (like which line of code it was on, the values in
     its temporary storage) for the first task and loading the information for
     the second task.</li>
 <li class=MsoNormal><b>MCQ:</b> What is &quot;context switching&quot; in a
     GPU? a) The process of turning the GPU on and off. b) The process of
     saving the state of one task and loading another. c) A type of memory
     error. d) The primary method for communication between CPU and GPU.</li>
 <li class=MsoNormal><b>Use Case:</b> On your personal computer, you might be
     playing a graphically intensive game while a video encoding program is
     running in the background. The GPU needs to switch between rendering the
     game frames (a high-priority graphics task) and processing the video (a
     lower-priority compute task) many times per second so that both appear to
     run smoothly.</li>
 <li class=MsoNormal><b>Numerical:</b> Suppose a context switch requires saving
     the state of 32 registers per thread and a 4-byte program counter for each
     thread in a warp. If each register holds 4 bytes, what is the total memory
     (in bytes) required to save the state of one warp?</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal><b>Solution:</b></li>
  <ul style='margin-top:0in' type=disc>
   <li class=MsoNormal>Memory for registers = 32 threads/warp * 32
       registers/thread * 4 bytes/register = 4096 bytes.</li>
   <li class=MsoNormal>Memory for program counters = 32 threads/warp * 4
       bytes/counter = 128 bytes.</li>
   <li class=MsoNormal>Total memory = 4096 + 128 = 4224 bytes.</li>
  </ul>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 4: Why Context Switching Happens in GPUs?</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> This slide explains three main
     reasons why a GPU needs to switch between tasks:</li>
 <ol style='margin-top:0in' start=1 type=1>
  <li class=MsoNormal><b>Time-sharing:</b> Just like your computer runs your
      web browser and music player at the same time, a GPU can run multiple
      applications (like a game and a science simulation) by giving each a
      small slice of time to run.</li>
  <li class=MsoNormal><b>Preemption:</b> This is like an ambulance getting
      priority on the road. If a very important task arrives (like rendering
      the next frame for your game), the GPU can pause (preempt) a less
      important task (like a long calculation) to handle the urgent one first.</li>
  <li class=MsoNormal><b>Multi-tenancy:</b> In cloud computing (like Amazon Web
      Services or Google Cloud), many different customers rent time on the same
      physical GPU. The GPU uses context switching to run tasks from all these
      different users fairly and securely, as if they each had their own private
      GPU.</li>
 </ol>
 <li class=MsoNormal><b>MCQ:</b> Which of the following is NOT a reason for
     context switching in GPUs? a) Allowing multiple users to share a single
     GPU in the cloud. b) A high-priority graphics task interrupting a
     lower-priority compute task. c) Increasing the clock speed of the GPU cores.
     d) Sharing the GPU between a graphics application and a CUDA compute
     application.</li>
 <li class=MsoNormal><b>Use Case:</b> A cloud service provider (e.g., AWS) uses
     a single powerful GPU to serve three different customers simultaneously.
     Customer A is training a machine learning model, Customer B is running a
     financial simulation, and Customer C is rendering a 3D animation. The GPU
     scheduler rapidly switches between these three tasks, giving each user a
     fair share of the processing power.</li>
 <li class=MsoNormal><b>Numerical:</b> A GPU is scheduled to run Task A for
     50ms and Task B for 30ms in a repeating cycle. If a high-priority graphics
     task preempts the GPU at the 20ms mark, how long does Task A get to run
     before it is suspended?</li>
</ul>

<ul style='margin-top:0in' type=disc>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal><b>Solution:</b></li>
  <ul style='margin-top:0in' type=disc>
   <li class=MsoNormal>Task A starts at t=0ms.</li>
   <li class=MsoNormal>The preemption occurs at t=20ms.</li>
   <li class=MsoNormal>Therefore, Task A runs for 20ms before being suspended.</li>
  </ul>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 5: CUDA Streams</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> A CUDA Stream is like a separate
     conveyor belt or a queue of operations. You can have multiple streams
     running at the same time. Within a single stream, all operations (like
     copying data, running a kernel) happen one after another in the order you
     submitted them. However, operations in different streams can run
     concurrently, or at the same time. This is extremely useful for hiding
     delays. For example, while one stream is busy computing, another stream
     can be copying data for the next computation.</li>
 <li class=MsoNormal><b>MCQ:</b> In a single CUDA stream, how are operations
     executed? a) In parallel, in any order. b) Sequentially, in the order they
     are issued. c) Only one operation can ever be in the stream. d) Operations
     must be manually synchronized by the programmer.</li>
 <li class=MsoNormal><b>Use Case:</b> Processing a video sequence. You can
     create a pipeline with three streams:</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal><b>Stream 1:</b> Copies frame N from CPU memory to GPU
      memory.</li>
  <li class=MsoNormal><b>Stream 2:</b> Applies a filter to frame N-1 on the
      GPU.</li>
  <li class=MsoNormal><b>Stream 3:</b> Copies the processed frame N-2 from GPU
      memory back to CPU memory. By using three streams, the GPU can be doing a
      copy, a computation, and another copy all at the same time, dramatically
      speeding up the total video processing time.</li>
 </ul>
 <li class=MsoNormal><b>Numerical:</b> Assume copying data to the GPU takes
     10ms, a kernel computation takes 50ms, and copying data back takes 10ms.
     a) What is the total time if these operations are executed sequentially
     (without streams)? b) What is the ideal minimum total time if you use streams
     to overlap these operations perfectly?</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal><b>Solution:</b></li>
  <ul style='margin-top:0in' type=disc>
   <li class=MsoNormal>a) Sequential time = 10ms (copy in) + 50ms (compute) +
       10ms (copy out) = 70ms.</li>
   <li class=MsoNormal>b) Overlapped time = The longest single operation, which
       is the 50ms computation. The copies can happen concurrently with the
       computation.</li>
  </ul>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 6: Example</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> This slide introduces a simple
     programming example. The goal is to write a CUDA kernel (a function that
     runs on the GPU) that takes two vectors (arrays), <b>a</b> and <b>b</b>,
     and a single value <b>c</b>. For each position <b>i</b>, it calculates the
     average of <b>a[i]</b>, <b>b[i]</b>, and <b>c</b>.</li>
 <li class=MsoNormal><b>MCQ:</b> What is the task of the example kernel
     described on this slide? a) To find the maximum value in two vectors. b)
     To calculate the average of three values for each element. c) To multiply
     two vectors element-wise. d) To sort a vector.</li>
 <li class=MsoNormal><b>Use Case:</b> In financial modeling, you might have two
     vectors of stock prices and a constant representing a market index. You
     could use a kernel like this to compute a simple average indicator for
     each point in time, processing thousands of data points in parallel.</li>
 <li class=MsoNormal><b>Numerical:</b> Given vector <b>a = [9, 18]</b>, vector <b>b
     = [12, 6]</b>, and a scalar <b>c = 15</b>, what would be the first output
     value of the kernel (the average for index 0)?</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal><b>Solution:</b></li>
  <ul style='margin-top:0in' type=disc>
   <li class=MsoNormal>Average at index 0 = (a[0] + b[0] + c) / 3 = (9 + 12 +
       15) / 3 = 36 / 3 = 12.</li>
  </ul>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 7: Example (Page-Locked Memory)</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> This slide explains a special
     type of memory called &quot;page-locked&quot; or &quot;pinned&quot;
     memory. Normally, your operating system can move any program's data from
     your fast RAM (physical memory) to your slow hard drive (virtual memory)
     to make space. Page-locked memory is different; you tell the OS,
     &quot;Don't you dare move this data to the hard drive.&quot; The advantage
     is that the GPU can then access this memory directly and very quickly
     using a feature called DMA (Direct Memory Access), without the CPU's help.
     The downside is that it uses up your precious physical RAM, so you
     shouldn't use it for everything.</li>
 <li class=MsoNormal><b>MCQ:</b> What is a key characteristic of page-locked
     memory? a) It is always stored on the hard drive. b) It is slower for the
     GPU to access than normal memory. c) The operating system guarantees it
     will not be swapped to disk. d) It requires the CPU to intervene for every
     data transfer.</li>
 <li class=MsoNormal><b>Use Case:</b> In a real-time audio processing
     application, you need to transfer audio data from the CPU to the GPU with
     extremely low latency. By allocating the audio buffers as page-locked
     memory, you ensure the GPU can access the data instantly without any
     unexpected delays caused by the OS moving the memory around.</li>
 <li class=MsoNormal><b>Numerical:</b> A system has 8 GB (8192 MB) of physical
     RAM. A CUDA program allocates 512 MB of page-locked memory for its data
     transfers. How much physical RAM is now available for the operating system
     and other applications?</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal><b>Solution:</b></li>
  <ul style='margin-top:0in' type=disc>
   <li class=MsoNormal>Available RAM = Total RAM - Allocated Page-Locked RAM =
       8192 MB - 512 MB = 7680 MB.</li>
  </ul>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 8: Stencil Computation</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> A stencil computation is a
     pattern where you calculate the new value of each point in a grid (like an
     image or a 3D volume) based on the values of its neighbors. The
     &quot;stencil&quot; is just the name for the pattern of neighbors you look
     at. The example <b>B[i] = (A[i-1] + A[i] + A[i+1]) / 3</b> is a 1D stencil
     that creates a new array <b>B</b> where each element is the average of its
     own value and its immediate left and right neighbors in the original array
     <b>A</b>. This is essentially a simple blur filter.</li>
 <li class=MsoNormal><b>MCQ:</b> In the formula <b>B[i] = (A[i-1] + A[i] +
     A[i+1]) / 3</b>, how many points from the original array <b>A</b> are used
     to calculate a single point in <b>B</b>? a) 1 b) 2 c) 3 d) 4</li>
 <li class=MsoNormal><b>Use Case:</b> Image processing is a classic use case.
     Applying a blur, sharpen, or edge-detection filter to an image is a
     stencil computation. For each pixel, you look at its neighboring pixels
     (according to the filter's stencil, e.g., a 3x3 box) and compute the new pixel
     value.</li>
 <li class=MsoNormal><b>Numerical:</b> Given an input array <b>A = [10, 20, 30,
     40, 50]</b>, what is the value of <b>B[2]</b> after applying the 3-point
     stencil <b>B[i] = (A[i-1] + A[i] + A[i+1]) / 3</b>?</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal><b>Solution:</b></li>
  <ul style='margin-top:0in' type=disc>
   <li class=MsoNormal>To calculate B[2], we need A, A[2], and A[3].</li>
   <li class=MsoNormal>B[2] = (A + A[2] + A[3]) / 3 = (20 + 30 + 40) / 3 = 90 /
       3 = 30.</li>
  </ul>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 9: Why Stencils Matter?</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> This slide explains why stencil
     computations are a perfect match for GPUs:</li>
 <ol style='margin-top:0in' start=1 type=1>
  <li class=MsoNormal><b>Data Parallelism:</b> The calculation for each output
      point is independent of the others. You can calculate the new value for
      all 1 million pixels in an image at the same time, using 1 million
      different threads.</li>
  <li class=MsoNormal><b>Regular Memory Access:</b> Threads access memory in a
      predictable pattern (their neighbors). This allows the GPU to group
      memory requests together, making it much more efficient.</li>
  <li class=MsoNormal><b>Shared Memory Usage:</b> Threads working on a small
      neighborhood can load their shared neighboring data into a super-fast
      on-chip memory (shared memory) instead of repeatedly going to the slow
      main memory (global memory).</li>
 </ol>
 <li class=MsoNormal><b>MCQ:</b> Which of the following is a reason why stencil
     computations are well-suited for GPUs? a) They require very little memory.
     b) They exhibit high data parallelism. c) They are inherently sequential
     tasks. d) They do not use floating-point math.</li>
 <li class=MsoNormal><b>Use Case:</b> Scientific simulations like weather
     forecasting or modeling fluid dynamics (e.g., airflow over a wing). These
     simulations are often run on 3D grids where the state (pressure,
     temperature, velocity) of each point at the next time step is calculated
     from its current state and the state of its neighbors—a massive 3D stencil
     computation.</li>
 <li class=MsoNormal><b>Numerical:</b> You want to apply a 2D stencil to a 4K
     video frame (3840x2160 pixels). If you launch one thread per output pixel,
     how many threads do you need?</li>
</ul>

<ul style='margin-top:0in' type=disc>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal><b>Solution:</b></li>
  <ul style='margin-top:0in' type=disc>
   <li class=MsoNormal>Total threads = width * height = 3840 * 2160 = 8,294,400
       threads.</li>
  </ul>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 10 &amp; 11: Implementing With Shared Memory</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> These slides show the strategy
     for optimizing a stencil computation using shared memory. Instead of every
     thread in a block going to the slow global memory to get its data and its
     neighbors' data, the whole block works together. First, they all load a slightly
     larger &quot;tile&quot; of the data (including the halo of neighbor cells)
     into a small, private, and extremely fast memory space called shared
     memory. Once all the data is loaded, each thread can then perform its
     stencil calculation by reading from this super-fast shared memory. This
     dramatically reduces the number of slow global memory accesses.</li>
 <li class=MsoNormal><b>MCQ:</b> In the context of stencil computation, what is
     the primary benefit of loading data into shared memory first? a) It
     increases the total amount of memory available. b) It reduces the number
     of accesses to slow global memory. c) It simplifies the kernel code. d) It
     allows threads to run on different GPUs.</li>
 <li class=MsoNormal><b>Use Case:</b> A high-performance implementation of a
     convolutional neural network (CNN) for image recognition. The convolution
     layers are essentially stencil operations. By using shared memory to store
     the input image tile and the filter weights, the GPU can perform the
     convolutions much faster, leading to quicker image classification.</li>
 <li class=MsoNormal><b>Numerical:</b> A thread block is responsible for
     computing a 16x16 output tile. For a 3-point 1D stencil, the block needs
     to load a tile of size 16x18 into shared memory (the extra 2 columns are
     for the &quot;halo&quot; neighbors). If each data point is a 4-byte float,
     how much shared memory is required for this tile?</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal><b>Solution:</b></li>
  <ul style='margin-top:0in' type=disc>
   <li class=MsoNormal>Total elements = 16 * 18 = 288 elements.</li>
   <li class=MsoNormal>Total memory = 288 elements * 4 bytes/element = 1152
       bytes.</li>
  </ul>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 12: __syncthreads()</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> <b>__syncthreads()</b> is a
     special command in CUDA that acts as a synchronization barrier for all
     threads within a single block. It means &quot;wait here until every single
     thread in this block has reached this point.&quot; This is crucial when
     using shared memory. You must ensure that all threads have finished
     loading their part of the data into shared memory <i>before</i> any thread
     starts performing calculations that depend on that data. It's like a team
     of builders waiting for everyone to bring their bricks to the site before
     anyone starts building the wall.</li>
 <li class=MsoNormal><b>MCQ:</b> What is the purpose of the <b>__syncthreads()</b>
     function in a CUDA kernel? a) To terminate the kernel. b) To synchronize
     all threads across the entire grid. c) To act as a barrier, ensuring all
     threads in a block reach a point before any proceed. d) To copy data from
     global to shared memory.</li>
 <li class=MsoNormal><b>Use Case:</b> In any shared memory algorithm, like the
     stencil computation or a matrix multiplication. In matrix multiplication,
     each thread block loads two tiles of the input matrices into shared
     memory. <b>__syncthreads()</b> is called after each tile is loaded to
     guarantee that the entire tile is available before the threads start the
     calculation for their assigned output elements.</li>
 <li class=MsoNormal><b>Numerical:</b> A block has 256 threads. Threads 0-127
     are very fast and load their data into shared memory in 1 microsecond.
     Threads 128-255 are slower and take 5 microseconds. All threads call <b>__syncthreads()</b>
     immediately after their load operation. For how long will threads 0-127 be
     waiting at the synchronization point?</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal><b>Solution:</b></li>
  <ul style='margin-top:0in' type=disc>
   <li class=MsoNormal>Threads 0-127 will finish in 1 microsecond and then
       wait.</li>
   <li class=MsoNormal>Threads 128-255 will finish at the 5-microsecond mark.</li>
   <li class=MsoNormal>The barrier is only passed when the last thread arrives.
       Therefore, threads 0-127 will wait for 5 - 1 = 4 microseconds.</li>
  </ul>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 13 &amp; 14: Stencil Computation (Code)</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> This slide shows the actual C/C++
     code for a 3D, 7-point stencil. It calculates a new value for a point at
     coordinates <b>(i, j, k)</b> by taking a weighted average of itself and
     its 6 immediate neighbors (left, right, up, down, front, back). The <b>c0,
     c1, ... c6</b> are the weights (coefficients) for the average. The
     complex-looking expression <b>i*N*N + j*N + k</b> is just a mathematical
     trick to convert the 3D coordinates <b>(i, j, k)</b> into a single index
     for a 1D array.</li>
 <li class=MsoNormal><b>MCQ:</b> How many neighboring points are used in the 3D
     stencil computation shown in the code? a) 3 b) 6 c) 7 d) 8</li>
 <li class=MsoNormal><b>Use Case:</b> Simulating heat distribution in a 3D
     object, like a computer processor or an engine block. The temperature at
     any point inside the object is influenced by the temperatures of the
     points surrounding it. This 7-point stencil can be used in each step of the
     simulation to model how heat flows through the object over time.</li>
 <li class=MsoNormal><b>Numerical:</b> In the 3D stencil code, let the
     coefficients be <b>c0 = 0.5</b> (center) and <b>c1</b> through <b>c6 = 0.1</b>
     (each neighbor). If the value at the center point <b>in[...]</b> is 100,
     and the value at all 6 neighbors is 50, what is the resulting output value
     <b>out[...]</b>?</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal><b>Solution:</b></li>
  <ul style='margin-top:0in' type=disc>
   <li class=MsoNormal>Output = (c0 * center) + (c1 * n1) + (c2 * n2) + ... +
       (c6 * n6)</li>
   <li class=MsoNormal>Output = (0.5 * 100) + 6 * (0.1 * 50)</li>
   <li class=MsoNormal>Output = 50 + 6 * 5</li>
   <li class=MsoNormal>Output = 50 + 30 = 80.</li>
  </ul>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 15: Example Use Case</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> This slide provides a concrete
     example of a stencil computation: heat diffusion. It shows a formula that
     calculates the new temperature of a point (<b>T[i][j][k]</b>) based on its
     current temperature and the temperatures of its neighbors. The formula is
     a weighted average, which models how heat naturally spreads out from
     hotter areas to cooler areas over time.</li>
 <li class=MsoNormal><b>MCQ:</b> The example use case on this slide simulates
     which physical phenomenon? a) Electrical current flow. b) Heat diffusion. c)
     Sound wave propagation. d) Light refraction.</li>
 <li class=MsoNormal><b>Use Case:</b> An engineer designing a heat sink for a
     high-performance CPU. They can use a 3D heat diffusion simulation (which
     is a series of stencil computations) to test different heat sink designs
     virtually. The simulation would show how effectively the heat sink draws
     heat away from the processor, helping the engineer optimize the design
     before building a physical prototype.</li>
 <li class=MsoNormal><b>Numerical:</b> Using the heat diffusion formula from
     the slide: <b>T_new = 0.25 * T_old + 0.1 * (sum of 6 neighbors)</b>. If a
     point's current temperature <b>T_old</b> is 90°C, and the sum of its 6
     neighbors' temperatures is 480°C, what is its new temperature in the next
     step?</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal><b>Solution:</b></li>
  <ul style='margin-top:0in' type=disc>
   <li class=MsoNormal>T_new = 0.25 * 90 + 0.1 * 480</li>
   <li class=MsoNormal>T_new = 22.5 + 48</li>
   <li class=MsoNormal>T_new = 70.5°C.</li>
  </ul>
 </ul>
</ul>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=0 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 16: Arithmetic Intensity (AI) Calculation</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal><b>Easy Explanation:</b> Arithmetic Intensity (AI) is a
     performance metric that tells you how &quot;computationally heavy&quot; a
     task is compared to how much data it needs to move. It's calculated as: <b>AI
     = (Number of Math Operations) / (Number of Bytes Moved)</b> A high AI is
     good. It means you are doing a lot of work for every byte you fetch from
     memory, which makes your program efficient. The slide calculates the AI
     for the 3D stencil, showing it performs 13 floating-point operations for
     every 7 data points it reads (each 4 bytes). It also shows how using a
     larger tile (bigger <b>T</b>) improves AI because you reuse the data you
     loaded into shared memory more often.</li>
 <li class=MsoNormal><b>MCQ:</b> What does Arithmetic Intensity (AI) measure?
     a) The speed of the GPU's clock. b) The ratio of arithmetic operations to
     bytes of data moved. c) The total number of threads in a kernel. d) The
     amount of shared memory used by a block.</li>
 <li class=MsoNormal><b>Use Case:</b> When optimizing code, a developer can
     calculate the AI of their kernel. If the AI is very low (e.g., &lt; 1), it
     means the kernel is &quot;memory-bound&quot;—it's spending most of its
     time waiting for data from memory. The developer would then focus on optimization
     techniques like using shared memory or changing the memory access pattern
     to increase the AI and make the program run faster.</li>
 <li class=MsoNormal><b>Numerical:</b> The slide provides two formulas. Let's
     use the first, simpler one. For the 3D 7-point stencil, each thread
     performs 13 floating-point operations. Each data point is 4 bytes (a
     float). Calculate the Arithmetic Intensity (AI) in operations per byte
     (OP/B).</li>
 <ul style='margin-top:0in' type=disc>
  <li class=MsoNormal><b>Solution:</b></li>
  <ul style='margin-top:0in' type=disc>
   <li class=MsoNormal>Operations = 13</li>
   <li class=MsoNormal>Bytes moved = 7 points * 4 bytes/point = 28 bytes.</li>
   <li class=MsoNormal>AI = Operations / Bytes = 13 / 28 &#8776; 0.46 OP/B.</li>
  </ul>
 </ul>
</ul>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<div style='border:none;border-bottom:double windowtext 2.25pt;padding:0in 0in 1.0pt 0in'>

<p class=MsoNormal style='border:none;padding:0in'>&nbsp;</p>

</div>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>Thanks for sharing the lecture PDF on <b>GPU Programming
(Lecture 06)</b>. Based on the content, I’ll now provide the following for <b>each
slide</b>:</p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal><b>Simple explanation</b></li>
 <li class=MsoNormal><b>MCQs with answers</b></li>
 <li class=MsoNormal><b>Use cases</b></li>
 <li class=MsoNormal><b>Numerical examples (where applicable)</b></li>
</ol>

<p class=MsoNormal>Since this is a detailed task, I’ll start with the first few
slides and check if the format works for you before continuing with the rest.
Here's the breakdown for <b>Slides 2 to 4</b>:</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 2: CUDA Runtime System</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Simple Explanation:</b></p>

<p class=MsoNormal>The CUDA Runtime System is a layer that helps developers
interact with the GPU more easily. It manages memory, launches kernels, and
handles device synchronization without needing to write low-level code.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> What is the main role of the CUDA Runtime System?<br>
A. Managing CPU threads<br>
B. Handling low-level GPU hardware<br>
C. Simplifying GPU programming by managing memory and kernel launches<br>
D. Compiling CUDA code</p>

<p class=MsoNormal><b>Answer:</b> C</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Developers use the CUDA Runtime API to allocate memory on
     the GPU, launch kernels, and synchronize threads without needing to write
     complex driver-level code.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical:</b></p>

<p class=MsoNormal>If a kernel is launched with 256 threads per block and 10
blocks, the total number of threads is:<br>
<b>256 × 10 = 2560 threads</b></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 3: Recap - Warps</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Simple Explanation:</b></p>

<p class=MsoNormal>A <b>warp</b> is a group of 32 threads that execute the same
instruction at the same time on a GPU. It’s the basic unit of execution in
CUDA.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> How many threads are there in a warp in CUDA?<br>
A. 16<br>
B. 32<br>
C. 64<br>
D. 128</p>

<p class=MsoNormal><b>Answer:</b> B</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Warps are used to efficiently execute parallel code. If
     threads in a warp follow the same path, execution is fast. If not, it
     causes <b>warp divergence</b>, which slows things down.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical:</b></p>

<p class=MsoNormal>If a kernel launches 1024 threads, how many warps are
created?<br>
<b>1024 ÷ 32 = 32 warps</b></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 4: Context Switching in GPU</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Simple Explanation:</b></p>

<p class=MsoNormal>Context switching in GPUs means saving the current task’s
state and loading another task’s state. This allows the GPU to switch between
different programs or users.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> Which of the following is NOT a reason for GPU
context switching?<br>
A. Time-sharing between applications<br>
B. Preemption by high-priority tasks<br>
C. Multi-tenancy in cloud GPUs<br>
D. Increasing GPU clock speed</p>

<p class=MsoNormal><b>Answer:</b> D</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>In cloud environments, multiple users share the same GPU.
     Context switching allows fair usage and responsiveness.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical:</b></p>

<p class=MsoNormal>If a GPU switches between 4 tasks every 2 ms, how many
switches occur in 1 second?<br>
<b>1000 ms ÷ 2 ms = 500 switches</b></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal>Would you like me to continue with the rest of the slides in
this same format? Let me know if you'd like any changes or additions!</p>

<p class=MsoNormal>Sources</p>

<p class=MsoNormal><b>Provide your feedback on BizChat</b></p>

<p class=MsoNormal><b>You said: </b></p>

<p class=MsoNormal>yes</p>

<p class=MsoNormal><b>Copilot said: </b></p>

<p class=MsoNormal>Copilot</p>

<p class=MsoNormal>Great! Let's continue with the next set of slides from the
lecture.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 5: Context Switching (Diagram)</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Simple Explanation:</b></p>

<p class=MsoNormal>This slide likely shows a <b>visual representation</b> of
how context switching works in GPUs. It may illustrate how the GPU saves the
state of one task and loads another, showing components like registers, memory,
and scheduler.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> What is typically saved during a GPU context
switch?<br>
A. Only the memory state<br>
B. Only the thread ID<br>
C. Thread states, memory mappings, scheduler states<br>
D. Nothing is saved</p>

<p class=MsoNormal><b>Answer:</b> C</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>In a multi-user GPU server, context switching allows
     different users' programs to run without interfering with each other.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical:</b></p>

<p class=MsoNormal>If a GPU takes 5 microseconds to switch context and does
this 1000 times per second, total time spent switching is:<br>
<b>5 µs × 1000 = 5000 µs = 5 ms</b></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 6: CUDA Streams</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Simple Explanation:</b></p>

<p class=MsoNormal>CUDA Streams allow <b>overlapping</b> of operations like
memory copy and kernel execution. Each stream is a sequence of operations that
execute in order, but different streams can run in parallel.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> What is the main benefit of using CUDA Streams?<br>
A. Reduces GPU memory<br>
B. Allows overlapping of memory and compute operations<br>
C. Increases CPU speed<br>
D. Avoids using shared memory</p>

<p class=MsoNormal><b>Answer:</b> B</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>In deep learning, CUDA streams can be used to load the
     next batch of data while the current batch is being processed.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical:</b></p>

<p class=MsoNormal>If memory copy takes 2 ms and kernel execution takes 3 ms,
using streams can reduce total time from <b>2 + 3 = 5 ms</b> to just <b>3 ms</b>
(if overlapped).</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 7: CUDA Example – Page-Locked Memory</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Simple Explanation:</b></p>

<p class=MsoNormal>This example shows how to use <b>page-locked memory</b>
(also called pinned memory) for faster data transfer between CPU and GPU.
Page-locked memory stays in RAM and is not swapped to disk.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> Why is page-locked memory faster for GPU
transfers?<br>
A. It uses less RAM<br>
B. It avoids disk swapping and allows direct memory access (DMA)<br>
C. It compresses data<br>
D. It uses shared memory</p>

<p class=MsoNormal><b>Answer:</b> B</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Real-time video processing where fast memory transfer is
     critical.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical:</b></p>

<p class=MsoNormal>If normal memory transfer takes 10 ms and page-locked memory
reduces it by 40%, new time =<br>
<b>10 ms × 0.6 = 6 ms</b></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 8: Direct Memory Access (DMA)</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Simple Explanation:</b></p>

<p class=MsoNormal>DMA allows the GPU to copy data from host memory without
involving the CPU. This makes data transfer faster and frees up the CPU for
other tasks.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> What is the role of DMA in CUDA?<br>
A. It compresses data<br>
B. It allows GPU to access memory without CPU help<br>
C. It increases CPU clock speed<br>
D. It manages kernel launches</p>

<p class=MsoNormal><b>Answer:</b> B</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>In scientific simulations, DMA helps transfer large
     datasets efficiently between CPU and GPU.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical:</b></p>

<p class=MsoNormal>If CPU-assisted transfer takes 12 ms and DMA reduces it by
50%, new time =<br>
<b>12 ms × 0.5 = 6 ms</b></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal>Would you like me to continue with the next set of slides
(Stencil Computation and beyond)?</p>

<p class=MsoNormal><b>Provide your feedback on BizChat</b></p>

<p class=MsoNormal><b>You said: </b></p>

<p class=MsoNormal>yes</p>

<p class=MsoNormal><b>Copilot said: </b></p>

<p class=MsoNormal>Copilot</p>

<p class=MsoNormal>Continuing with the next slides:</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 9: Stencil Computation</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Simple Explanation:</b></p>

<p class=MsoNormal>A <b>stencil computation</b> updates each element in an
array based on its neighbors. For example, in a 1D array, a 3-point stencil
uses the current element and its left and right neighbors to compute a new
value.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> What does a 3-point stencil in 1D use to compute a
new value?<br>
A. Only the current element<br>
B. Current element and two neighbors<br>
C. All elements in the array<br>
D. Only the left neighbor</p>

<p class=MsoNormal><b>Answer:</b> B</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Used in simulations like heat diffusion, image filtering,
     and fluid dynamics.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical:</b></p>

<p class=MsoNormal>Given array A = [2, 4, 6, 8, 10], compute B[2] using 3-point
stencil:<br>
<b>B[2] = (A[1] + A[2] + A[3]) / 3 = (4 + 6 + 8) / 3 = 6</b></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 10: 2D Stencil (Image Processing)</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Simple Explanation:</b></p>

<p class=MsoNormal>In 2D, a stencil uses a grid of values (like an image). A <b>5-point
stencil</b> uses the center pixel and its four neighbors (up, down, left,
right) to compute a new value.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> What does a 5-point stencil in 2D typically
include?<br>
A. Only diagonal neighbors<br>
B. Center and 4 axis-aligned neighbors<br>
C. All 8 surrounding pixels<br>
D. Only the center pixel</p>

<p class=MsoNormal><b>Answer:</b> B</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Image blurring, edge detection, and noise reduction in
     image processing.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical:</b></p>

<p class=MsoNormal>If center = 10, left = 8, right = 12, up = 9, down = 11,
average =<br>
<b>(10 + 8 + 12 + 9 + 11) / 5 = 10</b></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 11–12: Implementing with Shared Memory</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Simple Explanation:</b></p>

<p class=MsoNormal>Shared memory is a fast memory space shared by threads in a
block. Using it in stencil computations reduces global memory access and
improves performance.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> Why is shared memory used in stencil computations?<br>
A. It is slower than global memory<br>
B. It allows threads to share data and reduce memory access time<br>
C. It increases register usage<br>
D. It avoids using the GPU</p>

<p class=MsoNormal><b>Answer:</b> B</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>In 2D/3D simulations, shared memory helps threads reuse
     neighbor data efficiently.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical:</b></p>

<p class=MsoNormal>If global memory access takes 400 cycles and shared memory
takes 40 cycles, speedup =<br>
<b>400 / 40 = 10× faster</b></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 13: __syncthreads()</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Simple Explanation:</b></p>

<p class=MsoNormal>__syncthreads() is a CUDA function that <b>synchronizes all
threads</b> in a block. It ensures all threads reach the same point before
continuing, which is important when using shared memory.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> What does __syncthreads() do?<br>
A. Launches a new kernel<br>
B. Synchronizes all threads in a block<br>
C. Allocates memory<br>
D. Ends the program</p>

<p class=MsoNormal><b>Answer:</b> B</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Used in shared memory stencil computations to avoid race
     conditions.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical:</b></p>

<p class=MsoNormal>If 256 threads in a block reach __syncthreads(), all must
wait until the slowest thread arrives. If one thread is delayed by 5 µs, all
others wait <b>5 µs</b>.</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal>Would you like me to continue with the final slides (14–16)
covering 3D stencil, use case, and arithmetic intensity?</p>

<p class=MsoNormal><b>Provide your feedback on BizChat</b></p>

<p class=MsoNormal><b>You said: </b></p>

<p class=MsoNormal>yes</p>

<p class=MsoNormal><b>Copilot said: </b></p>

<p class=MsoNormal>Copilot</p>

<p class=MsoNormal>Here’s the final set of slides from the lecture:</p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 14: 3D Stencil Computation</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Simple Explanation:</b></p>

<p class=MsoNormal>This slide shows a <b>7-point stencil</b> in 3D. It
calculates a new value for each point using the center and its 6 neighbors
(left, right, up, down, front, back). Each neighbor is multiplied by a constant
(c0 to c6).</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> How many neighbors are used in a 3D 7-point
stencil?<br>
A. 3<br>
B. 5<br>
C. 6<br>
D. 7</p>

<p class=MsoNormal><b>Answer:</b> C (6 neighbors + 1 center = 7 total points)</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Used in 3D simulations like weather modeling, fluid flow,
     and heat transfer.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical:</b></p>

<p class=MsoNormal>If all neighbors and center have value 10, and all weights
(c0 to c6) = 1, then:<br>
<b>Output = 10 × 7 = 70</b></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 15: Example Use Case – Heat Diffusion</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Simple Explanation:</b></p>

<p class=MsoNormal>This example simulates <b>heat diffusion</b> using a 7-point
stencil. Each grid point’s new temperature is a weighted average of itself and
its 6 neighbors.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> What physical phenomenon is modeled using the
7-point stencil in this example?<br>
A. Gravity<br>
B. Heat diffusion<br>
C. Sound waves<br>
D. Image compression</p>

<p class=MsoNormal><b>Answer:</b> B</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Simulating how heat spreads in a 3D object over time.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical:</b></p>

<p class=MsoNormal>If T[i][j][k] = 100 and all neighbors = 50:<br>
<b>New T = 0.25×100 + 0.1×(6×50) = 25 + 30 = 55</b></p>

<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

<p class=MsoNormal><b>Slide 16: Arithmetic Intensity (AI) Calculation</b></p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#9989;</span>
Simple Explanation:</b></p>

<p class=MsoNormal><b>Arithmetic Intensity (AI)</b> = operations / data moved.
It tells how much computation is done per byte of memory accessed. Higher AI
means better GPU performance.</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#10067;</span>
MCQ:</b></p>

<p class=MsoNormal><b>Q:</b> What does Arithmetic Intensity (AI) measure?<br>
A. Memory size<br>
B. Number of threads<br>
C. Computation per byte of memory accessed<br>
D. GPU temperature</p>

<p class=MsoNormal><b>Answer:</b> C</p>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128161;</span>
Use Case:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Helps optimize GPU programs by balancing computation and
     memory usage.</li>
</ul>

<p class=MsoNormal><b><span style='font-family:"Segoe UI Emoji",sans-serif'>&#128290;</span>
Numerical:</b></p>

<p class=MsoNormal>For a 3D stencil:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>13 operations per thread</li>
 <li class=MsoNormal>7 values loaded (4 bytes each) = 28 bytes<br>
     <b>AI = 13 / 28 &#8776; 0.46 OP/B</b></li>
</ul>

<p class=MsoNormal>If T = 10, then:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal>Output points = (T - 2)³ = 8³ = 512</li>
 <li class=MsoNormal>Total ops = 13 × 512 = 6656</li>
 <li class=MsoNormal>Total data = 4 × T³ = 4 × 1000 = 4000 bytes<br>
     <b>AI = 6656 / 4000 = 1.664 OP/B</b></li>
</ul>

<p class=MsoNormal>&nbsp;</p>

</div>

</body>

</html>
